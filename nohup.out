Parsed 7812 rows from data/sim_train_spectrum_all.
Parsed 7812 rows from data/sim_train_labels_all.
Parsed 9765 rows from data/gen_spectrum_all_00-of-16.
Parsed 9765 rows from data/gen_labels_all_00-of-16.
Parsed 9765 rows from data/gen_spectrum_all_01-of-16.
Parsed 9765 rows from data/gen_labels_all_01-of-16.
Parsed 9765 rows from data/gen_spectrum_all_02-of-16.
Parsed 9765 rows from data/gen_labels_all_02-of-16.
Parsed 9765 rows from data/gen_spectrum_all_03-of-16.
Parsed 9765 rows from data/gen_labels_all_03-of-16.
Parsed 9765 rows from data/gen_spectrum_all_04-of-16.
Parsed 9765 rows from data/gen_labels_all_04-of-16.
Parsed 9765 rows from data/gen_spectrum_all_05-of-16.
Parsed 9765 rows from data/gen_labels_all_05-of-16.
Parsed 9765 rows from data/gen_spectrum_all_06-of-16.
Parsed 9765 rows from data/gen_labels_all_06-of-16.
Parsed 9765 rows from data/gen_spectrum_all_07-of-16.
Parsed 9765 rows from data/gen_labels_all_07-of-16.
Parsed 9765 rows from data/gen_spectrum_all_08-of-16.
Parsed 9765 rows from data/gen_labels_all_08-of-16.
Parsed 9765 rows from data/gen_spectrum_all_09-of-16.
Parsed 9765 rows from data/gen_labels_all_09-of-16.
Parsed 9765 rows from data/gen_spectrum_all_10-of-16.
Parsed 9765 rows from data/gen_labels_all_10-of-16.
Parsed 9765 rows from data/gen_spectrum_all_11-of-16.
Parsed 9765 rows from data/gen_labels_all_11-of-16.
Parsed 9765 rows from data/gen_spectrum_all_12-of-16.
Parsed 9765 rows from data/gen_labels_all_12-of-16.
Parsed 9765 rows from data/gen_spectrum_all_13-of-16.
Parsed 9765 rows from data/gen_labels_all_13-of-16.
Parsed 9765 rows from data/gen_spectrum_all_14-of-16.
Parsed 9765 rows from data/gen_labels_all_14-of-16.
Parsed 9765 rows from data/gen_spectrum_all_15-of-16.
Parsed 9765 rows from data/gen_labels_all_15-of-16.
Parsed 3906 rows from data/sim_validation_spectrum_all.
Parsed 3906 rows from data/sim_validation_labels_all.
Logging training progress to tensorboard dir runs/alexnet-all-lr_0.000500-trainsize_164052-05_02_2020_01:46.
[epoch 0, batch    99] avg loss: 1.331869
[epoch 0, batch   199] avg loss: 1.169874
[epoch 0, batch   299] avg loss: 1.004697
[epoch 0, batch   399] avg loss: 0.966927
[epoch 0, batch   499] avg loss: 0.938492
[epoch 0, batch   599] avg loss: 0.912224
[epoch 0, batch   699] avg loss: 0.910049
[epoch 0, batch   799] avg loss: 0.877846
[epoch 0, batch   899] avg loss: 0.864243
[epoch 0, batch   999] avg loss: 0.869736
[epoch 0, batch  1099] avg loss: 0.853460
[epoch 0, batch  1199] avg loss: 0.841781
[epoch 0, batch  1299] avg loss: 0.842031
[epoch 0, batch  1399] avg loss: 0.831062
[epoch 0, batch  1499] avg loss: 0.807555
[epoch 0, batch  1599] avg loss: 0.810461
[epoch 0, batch  1699] avg loss: 0.816584
[epoch 0, batch  1799] avg loss: 0.794124
[epoch 0, batch  1899] avg loss: 0.787925
[epoch 0, batch  1999] avg loss: 0.787167
[epoch 0, batch  2099] avg loss: 0.784712
[epoch 0, batch  2199] avg loss: 0.788753
[epoch 0, batch  2299] avg loss: 0.757912
[epoch 0, batch  2399] avg loss: 0.763596
[epoch 0, batch  2499] avg loss: 0.776342
[epoch 1, batch    99] avg loss: 0.747623
[epoch 1, batch   199] avg loss: 0.761242
[epoch 1, batch   299] avg loss: 0.742597
[epoch 1, batch   399] avg loss: 0.733969
[epoch 1, batch   499] avg loss: 0.749119
[epoch 1, batch   599] avg loss: 0.737312
[epoch 1, batch   699] avg loss: 0.735390
[epoch 1, batch   799] avg loss: 0.742950
[epoch 1, batch   899] avg loss: 0.730404
[epoch 1, batch   999] avg loss: 0.714023
[epoch 1, batch  1099] avg loss: 0.739427
[epoch 1, batch  1199] avg loss: 0.735246
[epoch 1, batch  1299] avg loss: 0.720548
[epoch 1, batch  1399] avg loss: 0.704181
[epoch 1, batch  1499] avg loss: 0.735303
[epoch 1, batch  1599] avg loss: 0.730545
[epoch 1, batch  1699] avg loss: 0.722534
[epoch 1, batch  1799] avg loss: 0.703320
[epoch 1, batch  1899] avg loss: 0.703910
[epoch 1, batch  1999] avg loss: 0.717464
[epoch 1, batch  2099] avg loss: 0.727102
[epoch 1, batch  2199] avg loss: 0.712289
[epoch 1, batch  2299] avg loss: 0.710682
[epoch 1, batch  2399] avg loss: 0.716933
[epoch 1, batch  2499] avg loss: 0.698857
[epoch 2, batch    99] avg loss: 0.692334
[epoch 2, batch   199] avg loss: 0.710015
[epoch 2, batch   299] avg loss: 0.703611
[epoch 2, batch   399] avg loss: 0.696020
[epoch 2, batch   499] avg loss: 0.681832
[epoch 2, batch   599] avg loss: 0.694771
[epoch 2, batch   699] avg loss: 0.689059
[epoch 2, batch   799] avg loss: 0.702303
[epoch 2, batch   899] avg loss: 0.674080
[epoch 2, batch   999] avg loss: 0.689993
[epoch 2, batch  1099] avg loss: 0.678450
[epoch 2, batch  1199] avg loss: 0.657609
[epoch 2, batch  1299] avg loss: 0.680628
[epoch 2, batch  1399] avg loss: 0.673580
[epoch 2, batch  1499] avg loss: 0.667309
[epoch 2, batch  1599] avg loss: 0.680012
[epoch 2, batch  1699] avg loss: 0.685633
[epoch 2, batch  1799] avg loss: 0.682372
[epoch 2, batch  1899] avg loss: 0.680200
[epoch 2, batch  1999] avg loss: 0.694385
[epoch 2, batch  2099] avg loss: 0.666134
[epoch 2, batch  2199] avg loss: 0.657890
[epoch 2, batch  2299] avg loss: 0.665259
[epoch 2, batch  2399] avg loss: 0.664530
[epoch 2, batch  2499] avg loss: 0.659198
[epoch 3, batch    99] avg loss: 0.666538
[epoch 3, batch   199] avg loss: 0.662687
[epoch 3, batch   299] avg loss: 0.664767
[epoch 3, batch   399] avg loss: 0.664511
[epoch 3, batch   499] avg loss: 0.664994
[epoch 3, batch   599] avg loss: 0.654978
[epoch 3, batch   699] avg loss: 0.647466
[epoch 3, batch   799] avg loss: 0.654808
[epoch 3, batch   899] avg loss: 0.634322
[epoch 3, batch   999] avg loss: 0.673294
[epoch 3, batch  1099] avg loss: 0.653869
[epoch 3, batch  1199] avg loss: 0.651038
[epoch 3, batch  1299] avg loss: 0.646312
[epoch 3, batch  1399] avg loss: 0.652877
[epoch 3, batch  1499] avg loss: 0.643611
[epoch 3, batch  1599] avg loss: 0.645204
[epoch 3, batch  1699] avg loss: 0.663698
[epoch 3, batch  1799] avg loss: 0.645972
[epoch 3, batch  1899] avg loss: 0.642578
[epoch 3, batch  1999] avg loss: 0.666696
[epoch 3, batch  2099] avg loss: 0.650165
[epoch 3, batch  2199] avg loss: 0.655416
[epoch 3, batch  2299] avg loss: 0.628482
[epoch 3, batch  2399] avg loss: 0.646406
[epoch 3, batch  2499] avg loss: 0.635747
[epoch 4, batch    99] avg loss: 0.639162
[epoch 4, batch   199] avg loss: 0.623221
[epoch 4, batch   299] avg loss: 0.636975
[epoch 4, batch   399] avg loss: 0.647662
[epoch 4, batch   499] avg loss: 0.635858
[epoch 4, batch   599] avg loss: 0.637646
[epoch 4, batch   699] avg loss: 0.665134
[epoch 4, batch   799] avg loss: 0.640678
[epoch 4, batch   899] avg loss: 0.620051
[epoch 4, batch   999] avg loss: 0.632143
[epoch 4, batch  1099] avg loss: 0.638082
[epoch 4, batch  1199] avg loss: 0.621210
[epoch 4, batch  1299] avg loss: 0.629322
[epoch 4, batch  1399] avg loss: 0.637483
[epoch 4, batch  1499] avg loss: 0.634679
[epoch 4, batch  1599] avg loss: 0.635828
[epoch 4, batch  1699] avg loss: 0.631506
[epoch 4, batch  1799] avg loss: 0.611056
[epoch 4, batch  1899] avg loss: 0.609058
[epoch 4, batch  1999] avg loss: 0.595944
[epoch 4, batch  2099] avg loss: 0.636688
[epoch 4, batch  2199] avg loss: 0.632079
[epoch 4, batch  2299] avg loss: 0.609517
[epoch 4, batch  2399] avg loss: 0.635823
[epoch 4, batch  2499] avg loss: 0.629982
[epoch 5, batch    99] avg loss: 0.618973
[epoch 5, batch   199] avg loss: 0.622013
[epoch 5, batch   299] avg loss: 0.605152
[epoch 5, batch   399] avg loss: 0.607920
[epoch 5, batch   499] avg loss: 0.608644
[epoch 5, batch   599] avg loss: 0.613228
[epoch 5, batch   699] avg loss: 0.632204
[epoch 5, batch   799] avg loss: 0.611276
[epoch 5, batch   899] avg loss: 0.614124
[epoch 5, batch   999] avg loss: 0.605244
[epoch 5, batch  1099] avg loss: 0.624514
[epoch 5, batch  1199] avg loss: 0.608665
[epoch 5, batch  1299] avg loss: 0.602648
[epoch 5, batch  1399] avg loss: 0.611975
[epoch 5, batch  1499] avg loss: 0.609263
[epoch 5, batch  1599] avg loss: 0.627890
[epoch 5, batch  1699] avg loss: 0.609299
[epoch 5, batch  1799] avg loss: 0.610477
[epoch 5, batch  1899] avg loss: 0.599457
[epoch 5, batch  1999] avg loss: 0.621811
[epoch 5, batch  2099] avg loss: 0.605540
[epoch 5, batch  2199] avg loss: 0.600502
[epoch 5, batch  2299] avg loss: 0.610787
[epoch 5, batch  2399] avg loss: 0.608042
[epoch 5, batch  2499] avg loss: 0.606067
[epoch 6, batch    99] avg loss: 0.609407
[epoch 6, batch   199] avg loss: 0.611225
[epoch 6, batch   299] avg loss: 0.599302
[epoch 6, batch   399] avg loss: 0.612057
[epoch 6, batch   499] avg loss: 0.616376
[epoch 6, batch   599] avg loss: 0.612321
[epoch 6, batch   699] avg loss: 0.625823
[epoch 6, batch   799] avg loss: 0.614332
[epoch 6, batch   899] avg loss: 0.587148
[epoch 6, batch   999] avg loss: 0.605054
[epoch 6, batch  1099] avg loss: 0.608707
[epoch 6, batch  1199] avg loss: 0.592418
[epoch 6, batch  1299] avg loss: 0.611175
[epoch 6, batch  1399] avg loss: 0.582989
[epoch 6, batch  1499] avg loss: 0.600370
[epoch 6, batch  1599] avg loss: 0.600345
[epoch 6, batch  1699] avg loss: 0.602743
[epoch 6, batch  1799] avg loss: 0.588493
[epoch 6, batch  1899] avg loss: 0.612827
[epoch 6, batch  1999] avg loss: 0.609044
[epoch 6, batch  2099] avg loss: 0.586583
[epoch 6, batch  2199] avg loss: 0.617264
[epoch 6, batch  2299] avg loss: 0.593249
[epoch 6, batch  2399] avg loss: 0.626115
[epoch 6, batch  2499] avg loss: 0.598489
[epoch 7, batch    99] avg loss: 0.591158
[epoch 7, batch   199] avg loss: 0.604885
[epoch 7, batch   299] avg loss: 0.595007
[epoch 7, batch   399] avg loss: 0.595619
[epoch 7, batch   499] avg loss: 0.607340
[epoch 7, batch   599] avg loss: 0.612146
[epoch 7, batch   699] avg loss: 0.589098
[epoch 7, batch   799] avg loss: 0.592966
[epoch 7, batch   899] avg loss: 0.609872
[epoch 7, batch   999] avg loss: 0.611819
[epoch 7, batch  1099] avg loss: 0.588002
[epoch 7, batch  1199] avg loss: 0.598703
[epoch 7, batch  1299] avg loss: 0.602994
[epoch 7, batch  1399] avg loss: 0.592766
[epoch 7, batch  1499] avg loss: 0.590956
[epoch 7, batch  1599] avg loss: 0.570083
[epoch 7, batch  1699] avg loss: 0.580428
[epoch 7, batch  1799] avg loss: 0.594318
[epoch 7, batch  1899] avg loss: 0.581243
[epoch 7, batch  1999] avg loss: 0.604483
[epoch 7, batch  2099] avg loss: 0.578559
[epoch 7, batch  2199] avg loss: 0.603132
[epoch 7, batch  2299] avg loss: 0.593552
[epoch 7, batch  2399] avg loss: 0.606211
[epoch 7, batch  2499] avg loss: 0.582768
[epoch 8, batch    99] avg loss: 0.588415
[epoch 8, batch   199] avg loss: 0.587244
[epoch 8, batch   299] avg loss: 0.579165
[epoch 8, batch   399] avg loss: 0.592449
[epoch 8, batch   499] avg loss: 0.584932
[epoch 8, batch   599] avg loss: 0.584494
[epoch 8, batch   699] avg loss: 0.591629
[epoch 8, batch   799] avg loss: 0.584271
[epoch 8, batch   899] avg loss: 0.570384
[epoch 8, batch   999] avg loss: 0.583030
[epoch 8, batch  1099] avg loss: 0.581872
[epoch 8, batch  1199] avg loss: 0.585472
[epoch 8, batch  1299] avg loss: 0.576348
[epoch 8, batch  1399] avg loss: 0.586266
[epoch 8, batch  1499] avg loss: 0.601690
[epoch 8, batch  1599] avg loss: 0.576010
[epoch 8, batch  1699] avg loss: 0.583859
[epoch 8, batch  1799] avg loss: 0.589297
[epoch 8, batch  1899] avg loss: 0.584183
[epoch 8, batch  1999] avg loss: 0.575835
[epoch 8, batch  2099] avg loss: 0.594972
[epoch 8, batch  2199] avg loss: 0.592457
[epoch 8, batch  2299] avg loss: 0.583277
[epoch 8, batch  2399] avg loss: 0.580043
[epoch 8, batch  2499] avg loss: 0.592561
[epoch 9, batch    99] avg loss: 0.585064
[epoch 9, batch   199] avg loss: 0.581775
[epoch 9, batch   299] avg loss: 0.565747
[epoch 9, batch   399] avg loss: 0.582950
[epoch 9, batch   499] avg loss: 0.572521
[epoch 9, batch   599] avg loss: 0.575127
[epoch 9, batch   699] avg loss: 0.580967
[epoch 9, batch   799] avg loss: 0.567601
[epoch 9, batch   899] avg loss: 0.570567
[epoch 9, batch   999] avg loss: 0.563399
[epoch 9, batch  1099] avg loss: 0.582287
[epoch 9, batch  1199] avg loss: 0.570980
[epoch 9, batch  1299] avg loss: 0.575210
[epoch 9, batch  1399] avg loss: 0.576157
[epoch 9, batch  1499] avg loss: 0.562598
[epoch 9, batch  1599] avg loss: 0.568248
[epoch 9, batch  1699] avg loss: 0.586707
[epoch 9, batch  1799] avg loss: 0.592055
[epoch 9, batch  1899] avg loss: 0.562274
[epoch 9, batch  1999] avg loss: 0.561184
[epoch 9, batch  2099] avg loss: 0.565940
[epoch 9, batch  2199] avg loss: 0.576896
[epoch 9, batch  2299] avg loss: 0.579672
[epoch 9, batch  2399] avg loss: 0.568172
[epoch 9, batch  2499] avg loss: 0.583854
[epoch 10, batch    99] avg loss: 0.562018
[epoch 10, batch   199] avg loss: 0.567428
[epoch 10, batch   299] avg loss: 0.580779
[epoch 10, batch   399] avg loss: 0.565710
[epoch 10, batch   499] avg loss: 0.559707
[epoch 10, batch   599] avg loss: 0.564561
[epoch 10, batch   699] avg loss: 0.572165
[epoch 10, batch   799] avg loss: 0.575146
[epoch 10, batch   899] avg loss: 0.567128
[epoch 10, batch   999] avg loss: 0.560820
[epoch 10, batch  1099] avg loss: 0.566904
[epoch 10, batch  1199] avg loss: 0.590522
[epoch 10, batch  1299] avg loss: 0.576577
[epoch 10, batch  1399] avg loss: 0.582778
[epoch 10, batch  1499] avg loss: 0.564220
[epoch 10, batch  1599] avg loss: 0.559537
[epoch 10, batch  1699] avg loss: 0.571953
[epoch 10, batch  1799] avg loss: 0.563157
[epoch 10, batch  1899] avg loss: 0.584172
[epoch 10, batch  1999] avg loss: 0.564420
[epoch 10, batch  2099] avg loss: 0.566365
[epoch 10, batch  2199] avg loss: 0.586379
[epoch 10, batch  2299] avg loss: 0.573615
[epoch 10, batch  2399] avg loss: 0.564407
[epoch 10, batch  2499] avg loss: 0.568228
[epoch 11, batch    99] avg loss: 0.549261
[epoch 11, batch   199] avg loss: 0.553778
[epoch 11, batch   299] avg loss: 0.567499
[epoch 11, batch   399] avg loss: 0.559229
[epoch 11, batch   499] avg loss: 0.553610
[epoch 11, batch   599] avg loss: 0.547726
[epoch 11, batch   699] avg loss: 0.569014
[epoch 11, batch   799] avg loss: 0.560819
[epoch 11, batch   899] avg loss: 0.565696
[epoch 11, batch   999] avg loss: 0.571590
[epoch 11, batch  1099] avg loss: 0.550565
[epoch 11, batch  1199] avg loss: 0.566631
[epoch 11, batch  1299] avg loss: 0.555679
[epoch 11, batch  1399] avg loss: 0.564291
[epoch 11, batch  1499] avg loss: 0.538105
[epoch 11, batch  1599] avg loss: 0.555508
[epoch 11, batch  1699] avg loss: 0.564560
[epoch 11, batch  1799] avg loss: 0.549502
[epoch 11, batch  1899] avg loss: 0.560699
[epoch 11, batch  1999] avg loss: 0.559263
[epoch 11, batch  2099] avg loss: 0.557411
[epoch 11, batch  2199] avg loss: 0.562895
[epoch 11, batch  2299] avg loss: 0.573280
[epoch 11, batch  2399] avg loss: 0.546472
[epoch 11, batch  2499] avg loss: 0.583014
[epoch 12, batch    99] avg loss: 0.552359
[epoch 12, batch   199] avg loss: 0.553191
[epoch 12, batch   299] avg loss: 0.566066
[epoch 12, batch   399] avg loss: 0.560559
[epoch 12, batch   499] avg loss: 0.545373
[epoch 12, batch   599] avg loss: 0.559429
[epoch 12, batch   699] avg loss: 0.553954
[epoch 12, batch   799] avg loss: 0.555548
[epoch 12, batch   899] avg loss: 0.570204
[epoch 12, batch   999] avg loss: 0.565583
[epoch 12, batch  1099] avg loss: 0.553359
[epoch 12, batch  1199] avg loss: 0.545888
[epoch 12, batch  1299] avg loss: 0.563908
[epoch 12, batch  1399] avg loss: 0.572989
[epoch 12, batch  1499] avg loss: 0.557781
[epoch 12, batch  1599] avg loss: 0.566537
[epoch 12, batch  1699] avg loss: 0.556185
[epoch 12, batch  1799] avg loss: 0.556055
[epoch 12, batch  1899] avg loss: 0.547900
[epoch 12, batch  1999] avg loss: 0.561245
[epoch 12, batch  2099] avg loss: 0.572960
[epoch 12, batch  2199] avg loss: 0.547973
[epoch 12, batch  2299] avg loss: 0.544577
[epoch 12, batch  2399] avg loss: 0.562393
[epoch 12, batch  2499] avg loss: 0.550939
[epoch 13, batch    99] avg loss: 0.546420
[epoch 13, batch   199] avg loss: 0.546271
[epoch 13, batch   299] avg loss: 0.545587
[epoch 13, batch   399] avg loss: 0.557851
[epoch 13, batch   499] avg loss: 0.536975
[epoch 13, batch   599] avg loss: 0.544138
[epoch 13, batch   699] avg loss: 0.545934
[epoch 13, batch   799] avg loss: 0.559913
[epoch 13, batch   899] avg loss: 0.548083
[epoch 13, batch   999] avg loss: 0.534184
[epoch 13, batch  1099] avg loss: 0.530898
[epoch 13, batch  1199] avg loss: 0.544443
[epoch 13, batch  1299] avg loss: 0.566435
[epoch 13, batch  1399] avg loss: 0.538417
[epoch 13, batch  1499] avg loss: 0.559939
[epoch 13, batch  1599] avg loss: 0.558638
[epoch 13, batch  1699] avg loss: 0.537781
[epoch 13, batch  1799] avg loss: 0.538142
[epoch 13, batch  1899] avg loss: 0.555045
[epoch 13, batch  1999] avg loss: 0.547097
[epoch 13, batch  2099] avg loss: 0.540462
[epoch 13, batch  2199] avg loss: 0.550161
[epoch 13, batch  2299] avg loss: 0.544172
[epoch 13, batch  2399] avg loss: 0.546107
[epoch 13, batch  2499] avg loss: 0.569219
[epoch 14, batch    99] avg loss: 0.540277
[epoch 14, batch   199] avg loss: 0.536442
[epoch 14, batch   299] avg loss: 0.538311
[epoch 14, batch   399] avg loss: 0.546121
[epoch 14, batch   499] avg loss: 0.548029
[epoch 14, batch   599] avg loss: 0.550882
[epoch 14, batch   699] avg loss: 0.551844
[epoch 14, batch   799] avg loss: 0.544499
[epoch 14, batch   899] avg loss: 0.528819
[epoch 14, batch   999] avg loss: 0.535581
[epoch 14, batch  1099] avg loss: 0.552486
[epoch 14, batch  1199] avg loss: 0.535213
[epoch 14, batch  1299] avg loss: 0.550698
[epoch 14, batch  1399] avg loss: 0.538403
[epoch 14, batch  1499] avg loss: 0.530177
[epoch 14, batch  1599] avg loss: 0.532619
[epoch 14, batch  1699] avg loss: 0.530572
[epoch 14, batch  1799] avg loss: 0.550658
[epoch 14, batch  1899] avg loss: 0.544825
[epoch 14, batch  1999] avg loss: 0.543306
[epoch 14, batch  2099] avg loss: 0.537296
[epoch 14, batch  2199] avg loss: 0.531504
[epoch 14, batch  2299] avg loss: 0.552448
[epoch 14, batch  2399] avg loss: 0.546466
[epoch 14, batch  2499] avg loss: 0.536516
[epoch 15, batch    99] avg loss: 0.536494
[epoch 15, batch   199] avg loss: 0.527894
[epoch 15, batch   299] avg loss: 0.531731
[epoch 15, batch   399] avg loss: 0.532764
[epoch 15, batch   499] avg loss: 0.551958
[epoch 15, batch   599] avg loss: 0.539875
[epoch 15, batch   699] avg loss: 0.536449
[epoch 15, batch   799] avg loss: 0.543689
[epoch 15, batch   899] avg loss: 0.545550
[epoch 15, batch   999] avg loss: 0.528216
[epoch 15, batch  1099] avg loss: 0.532280
[epoch 15, batch  1199] avg loss: 0.540150
[epoch 15, batch  1299] avg loss: 0.537665
[epoch 15, batch  1399] avg loss: 0.531800
[epoch 15, batch  1499] avg loss: 0.530147
[epoch 15, batch  1599] avg loss: 0.526274
[epoch 15, batch  1699] avg loss: 0.547591
[epoch 15, batch  1799] avg loss: 0.536950
[epoch 15, batch  1899] avg loss: 0.543997
[epoch 15, batch  1999] avg loss: 0.548914
[epoch 15, batch  2099] avg loss: 0.535878
[epoch 15, batch  2199] avg loss: 0.543822
[epoch 15, batch  2299] avg loss: 0.545852
[epoch 15, batch  2399] avg loss: 0.530107
[epoch 15, batch  2499] avg loss: 0.544042
[epoch 16, batch    99] avg loss: 0.537884
[epoch 16, batch   199] avg loss: 0.519959
[epoch 16, batch   299] avg loss: 0.532103
[epoch 16, batch   399] avg loss: 0.539399
[epoch 16, batch   499] avg loss: 0.529770
[epoch 16, batch   599] avg loss: 0.560085
[epoch 16, batch   699] avg loss: 0.531465
[epoch 16, batch   799] avg loss: 0.531279
[epoch 16, batch   899] avg loss: 0.563422
[epoch 16, batch   999] avg loss: 0.539700
[epoch 16, batch  1099] avg loss: 0.536141
[epoch 16, batch  1199] avg loss: 0.541664
[epoch 16, batch  1299] avg loss: 0.521271
[epoch 16, batch  1399] avg loss: 0.530319
[epoch 16, batch  1499] avg loss: 0.536242
[epoch 16, batch  1599] avg loss: 0.550682
[epoch 16, batch  1699] avg loss: 0.534306
[epoch 16, batch  1799] avg loss: 0.547170
[epoch 16, batch  1899] avg loss: 0.536486
[epoch 16, batch  1999] avg loss: 0.532778
[epoch 16, batch  2099] avg loss: 0.522388
[epoch 16, batch  2199] avg loss: 0.525456
[epoch 16, batch  2299] avg loss: 0.548033
[epoch 16, batch  2399] avg loss: 0.537261
[epoch 16, batch  2499] avg loss: 0.546985
[epoch 17, batch    99] avg loss: 0.521047
[epoch 17, batch   199] avg loss: 0.539095
[epoch 17, batch   299] avg loss: 0.526063
[epoch 17, batch   399] avg loss: 0.535015
[epoch 17, batch   499] avg loss: 0.533496
[epoch 17, batch   599] avg loss: 0.508116
[epoch 17, batch   699] avg loss: 0.528463
[epoch 17, batch   799] avg loss: 0.518808
[epoch 17, batch   899] avg loss: 0.527719
[epoch 17, batch   999] avg loss: 0.542495
[epoch 17, batch  1099] avg loss: 0.513466
[epoch 17, batch  1199] avg loss: 0.536363
[epoch 17, batch  1299] avg loss: 0.518391
[epoch 17, batch  1399] avg loss: 0.523974
[epoch 17, batch  1499] avg loss: 0.509446
[epoch 17, batch  1599] avg loss: 0.529037
[epoch 17, batch  1699] avg loss: 0.541978
[epoch 17, batch  1799] avg loss: 0.533689
[epoch 17, batch  1899] avg loss: 0.537075
[epoch 17, batch  1999] avg loss: 0.528717
[epoch 17, batch  2099] avg loss: 0.521291
[epoch 17, batch  2199] avg loss: 0.516575
[epoch 17, batch  2299] avg loss: 0.524973
[epoch 17, batch  2399] avg loss: 0.526093
[epoch 17, batch  2499] avg loss: 0.530193
[epoch 18, batch    99] avg loss: 0.526215
[epoch 18, batch   199] avg loss: 0.511048
[epoch 18, batch   299] avg loss: 0.528105
[epoch 18, batch   399] avg loss: 0.547127
[epoch 18, batch   499] avg loss: 0.538773
[epoch 18, batch   599] avg loss: 0.520518
[epoch 18, batch   699] avg loss: 0.526934
[epoch 18, batch   799] avg loss: 0.529402
[epoch 18, batch   899] avg loss: 0.513837
[epoch 18, batch   999] avg loss: 0.555481
[epoch 18, batch  1099] avg loss: 0.540291
[epoch 18, batch  1199] avg loss: 0.515334
[epoch 18, batch  1299] avg loss: 0.542480
[epoch 18, batch  1399] avg loss: 0.533116
[epoch 18, batch  1499] avg loss: 0.518833
[epoch 18, batch  1599] avg loss: 0.523668
[epoch 18, batch  1699] avg loss: 0.547929
[epoch 18, batch  1799] avg loss: 0.513977
[epoch 18, batch  1899] avg loss: 0.522599
[epoch 18, batch  1999] avg loss: 0.526570
[epoch 18, batch  2099] avg loss: 0.506394
[epoch 18, batch  2199] avg loss: 0.516402
[epoch 18, batch  2299] avg loss: 0.514880
[epoch 18, batch  2399] avg loss: 0.532644
[epoch 18, batch  2499] avg loss: 0.523791
[epoch 19, batch    99] avg loss: 0.532857
[epoch 19, batch   199] avg loss: 0.521527
[epoch 19, batch   299] avg loss: 0.532031
[epoch 19, batch   399] avg loss: 0.518233
[epoch 19, batch   499] avg loss: 0.522346
[epoch 19, batch   599] avg loss: 0.526155
[epoch 19, batch   699] avg loss: 0.530937
[epoch 19, batch   799] avg loss: 0.531060
[epoch 19, batch   899] avg loss: 0.518405
[epoch 19, batch   999] avg loss: 0.518020
[epoch 19, batch  1099] avg loss: 0.521437
[epoch 19, batch  1199] avg loss: 0.512307
[epoch 19, batch  1299] avg loss: 0.525056
[epoch 19, batch  1399] avg loss: 0.515183
[epoch 19, batch  1499] avg loss: 0.513477
[epoch 19, batch  1599] avg loss: 0.528343
[epoch 19, batch  1699] avg loss: 0.501656
[epoch 19, batch  1799] avg loss: 0.519129
[epoch 19, batch  1899] avg loss: 0.512168
[epoch 19, batch  1999] avg loss: 0.527412
[epoch 19, batch  2099] avg loss: 0.509869
[epoch 19, batch  2199] avg loss: 0.546696
[epoch 19, batch  2299] avg loss: 0.508037
[epoch 19, batch  2399] avg loss: 0.526077
[epoch 19, batch  2499] avg loss: 0.527371
[epoch 20, batch    99] avg loss: 0.507004
[epoch 20, batch   199] avg loss: 0.509736
[epoch 20, batch   299] avg loss: 0.518782
[epoch 20, batch   399] avg loss: 0.520921
[epoch 20, batch   499] avg loss: 0.525116
[epoch 20, batch   599] avg loss: 0.513916
[epoch 20, batch   699] avg loss: 0.508175
[epoch 20, batch   799] avg loss: 0.511523
[epoch 20, batch   899] avg loss: 0.534577
[epoch 20, batch   999] avg loss: 0.544213
[epoch 20, batch  1099] avg loss: 0.513391
[epoch 20, batch  1199] avg loss: 0.510584
[epoch 20, batch  1299] avg loss: 0.511457
[epoch 20, batch  1399] avg loss: 0.532171
[epoch 20, batch  1499] avg loss: 0.508993
[epoch 20, batch  1599] avg loss: 0.518284
[epoch 20, batch  1699] avg loss: 0.504237
[epoch 20, batch  1799] avg loss: 0.510573
[epoch 20, batch  1899] avg loss: 0.512923
[epoch 20, batch  1999] avg loss: 0.514876
[epoch 20, batch  2099] avg loss: 0.511102
[epoch 20, batch  2199] avg loss: 0.518961
[epoch 20, batch  2299] avg loss: 0.496857
[epoch 20, batch  2399] avg loss: 0.513463
[epoch 20, batch  2499] avg loss: 0.506577
[epoch 21, batch    99] avg loss: 0.526599
[epoch 21, batch   199] avg loss: 0.517793
[epoch 21, batch   299] avg loss: 0.533357
[epoch 21, batch   399] avg loss: 0.502999
[epoch 21, batch   499] avg loss: 0.523239
[epoch 21, batch   599] avg loss: 0.511191
[epoch 21, batch   699] avg loss: 0.511667
[epoch 21, batch   799] avg loss: 0.508556
[epoch 21, batch   899] avg loss: 0.520888
[epoch 21, batch   999] avg loss: 0.527050
[epoch 21, batch  1099] avg loss: 0.509138
[epoch 21, batch  1199] avg loss: 0.510255
[epoch 21, batch  1299] avg loss: 0.523133
[epoch 21, batch  1399] avg loss: 0.528509
[epoch 21, batch  1499] avg loss: 0.506117
[epoch 21, batch  1599] avg loss: 0.523557
[epoch 21, batch  1699] avg loss: 0.497330
[epoch 21, batch  1799] avg loss: 0.514494
[epoch 21, batch  1899] avg loss: 0.526155
[epoch 21, batch  1999] avg loss: 0.499899
[epoch 21, batch  2099] avg loss: 0.500364
[epoch 21, batch  2199] avg loss: 0.525941
[epoch 21, batch  2299] avg loss: 0.514368
[epoch 21, batch  2399] avg loss: 0.498864
[epoch 21, batch  2499] avg loss: 0.513450
[epoch 22, batch    99] avg loss: 0.532696
[epoch 22, batch   199] avg loss: 0.517040
[epoch 22, batch   299] avg loss: 0.495836
[epoch 22, batch   399] avg loss: 0.497373
[epoch 22, batch   499] avg loss: 0.500811
[epoch 22, batch   599] avg loss: 0.509210
[epoch 22, batch   699] avg loss: 0.518556
[epoch 22, batch   799] avg loss: 0.503837
[epoch 22, batch   899] avg loss: 0.526681
[epoch 22, batch   999] avg loss: 0.515000
[epoch 22, batch  1099] avg loss: 0.511724
[epoch 22, batch  1199] avg loss: 0.523868
[epoch 22, batch  1299] avg loss: 0.490409
[epoch 22, batch  1399] avg loss: 0.502720
[epoch 22, batch  1499] avg loss: 0.510329
[epoch 22, batch  1599] avg loss: 0.500989
[epoch 22, batch  1699] avg loss: 0.501572
[epoch 22, batch  1799] avg loss: 0.511643
[epoch 22, batch  1899] avg loss: 0.499890
[epoch 22, batch  1999] avg loss: 0.514041
[epoch 22, batch  2099] avg loss: 0.515273
[epoch 22, batch  2199] avg loss: 0.522926
[epoch 22, batch  2299] avg loss: 0.508180
[epoch 22, batch  2399] avg loss: 0.524818
[epoch 22, batch  2499] avg loss: 0.489922
[epoch 23, batch    99] avg loss: 0.508167
[epoch 23, batch   199] avg loss: 0.496101
[epoch 23, batch   299] avg loss: 0.529340
[epoch 23, batch   399] avg loss: 0.516824
[epoch 23, batch   499] avg loss: 0.500498
[epoch 23, batch   599] avg loss: 0.503062
[epoch 23, batch   699] avg loss: 0.502608
[epoch 23, batch   799] avg loss: 0.502741
[epoch 23, batch   899] avg loss: 0.506117
[epoch 23, batch   999] avg loss: 0.510322
[epoch 23, batch  1099] avg loss: 0.510196
[epoch 23, batch  1199] avg loss: 0.492687
[epoch 23, batch  1299] avg loss: 0.497031
[epoch 23, batch  1399] avg loss: 0.533244
[epoch 23, batch  1499] avg loss: 0.505188
[epoch 23, batch  1599] avg loss: 0.506335
[epoch 23, batch  1699] avg loss: 0.507305
[epoch 23, batch  1799] avg loss: 0.500925
[epoch 23, batch  1899] avg loss: 0.508851
[epoch 23, batch  1999] avg loss: 0.498056
[epoch 23, batch  2099] avg loss: 0.523880
[epoch 23, batch  2199] avg loss: 0.491019
[epoch 23, batch  2299] avg loss: 0.497830
[epoch 23, batch  2399] avg loss: 0.486408
[epoch 23, batch  2499] avg loss: 0.494132
[epoch 24, batch    99] avg loss: 0.501672
[epoch 24, batch   199] avg loss: 0.518717
[epoch 24, batch   299] avg loss: 0.492942
[epoch 24, batch   399] avg loss: 0.508156
[epoch 24, batch   499] avg loss: 0.502953
[epoch 24, batch   599] avg loss: 0.495351
[epoch 24, batch   699] avg loss: 0.524249
[epoch 24, batch   799] avg loss: 0.531837
[epoch 24, batch   899] avg loss: 0.503042
[epoch 24, batch   999] avg loss: 0.490453
[epoch 24, batch  1099] avg loss: 0.497925
[epoch 24, batch  1199] avg loss: 0.500034
[epoch 24, batch  1299] avg loss: 0.511176
[epoch 24, batch  1399] avg loss: 0.511234
[epoch 24, batch  1499] avg loss: 0.515422
[epoch 24, batch  1599] avg loss: 0.514726
[epoch 24, batch  1699] avg loss: 0.520771
[epoch 24, batch  1799] avg loss: 0.489916
[epoch 24, batch  1899] avg loss: 0.499796
[epoch 24, batch  1999] avg loss: 0.497535
[epoch 24, batch  2099] avg loss: 0.517701
[epoch 24, batch  2199] avg loss: 0.523413
[epoch 24, batch  2299] avg loss: 0.495862
[epoch 24, batch  2399] avg loss: 0.507965
[epoch 24, batch  2499] avg loss: 0.492574
[epoch 25, batch    99] avg loss: 0.514015
[epoch 25, batch   199] avg loss: 0.498687
[epoch 25, batch   299] avg loss: 0.501744
[epoch 25, batch   399] avg loss: 0.492018
[epoch 25, batch   499] avg loss: 0.496222
[epoch 25, batch   599] avg loss: 0.502449
[epoch 25, batch   699] avg loss: 0.498912
[epoch 25, batch   799] avg loss: 0.514206
[epoch 25, batch   899] avg loss: 0.496264
[epoch 25, batch   999] avg loss: 0.512372
[epoch 25, batch  1099] avg loss: 0.496563
[epoch 25, batch  1199] avg loss: 0.505342
[epoch 25, batch  1299] avg loss: 0.508429
[epoch 25, batch  1399] avg loss: 0.496978
[epoch 25, batch  1499] avg loss: 0.487694
[epoch 25, batch  1599] avg loss: 0.503398
[epoch 25, batch  1699] avg loss: 0.511875
[epoch 25, batch  1799] avg loss: 0.492518
[epoch 25, batch  1899] avg loss: 0.500097
[epoch 25, batch  1999] avg loss: 0.506845
[epoch 25, batch  2099] avg loss: 0.506063
[epoch 25, batch  2199] avg loss: 0.506397
[epoch 25, batch  2299] avg loss: 0.503786
[epoch 25, batch  2399] avg loss: 0.494949
[epoch 25, batch  2499] avg loss: 0.512418
[epoch 26, batch    99] avg loss: 0.522678
[epoch 26, batch   199] avg loss: 0.497839
[epoch 26, batch   299] avg loss: 0.509428
[epoch 26, batch   399] avg loss: 0.495002
[epoch 26, batch   499] avg loss: 0.499083
[epoch 26, batch   599] avg loss: 0.504315
[epoch 26, batch   699] avg loss: 0.487771
[epoch 26, batch   799] avg loss: 0.499103
[epoch 26, batch   899] avg loss: 0.480787
[epoch 26, batch   999] avg loss: 0.516641
[epoch 26, batch  1099] avg loss: 0.504522
[epoch 26, batch  1199] avg loss: 0.498170
[epoch 26, batch  1299] avg loss: 0.513770
[epoch 26, batch  1399] avg loss: 0.497940
[epoch 26, batch  1499] avg loss: 0.497223
[epoch 26, batch  1599] avg loss: 0.493214
[epoch 26, batch  1699] avg loss: 0.507058
[epoch 26, batch  1799] avg loss: 0.484403
[epoch 26, batch  1899] avg loss: 0.485544
[epoch 26, batch  1999] avg loss: 0.505869
[epoch 26, batch  2099] avg loss: 0.512727
[epoch 26, batch  2199] avg loss: 0.511358
[epoch 26, batch  2299] avg loss: 0.485810
[epoch 26, batch  2399] avg loss: 0.485537
[epoch 26, batch  2499] avg loss: 0.503238
[epoch 27, batch    99] avg loss: 0.491516
[epoch 27, batch   199] avg loss: 0.497140
[epoch 27, batch   299] avg loss: 0.503951
[epoch 27, batch   399] avg loss: 0.487483
[epoch 27, batch   499] avg loss: 0.487149
[epoch 27, batch   599] avg loss: 0.501838
[epoch 27, batch   699] avg loss: 0.512463
[epoch 27, batch   799] avg loss: 0.497803
[epoch 27, batch   899] avg loss: 0.511018
[epoch 27, batch   999] avg loss: 0.500238
[epoch 27, batch  1099] avg loss: 0.497532
[epoch 27, batch  1199] avg loss: 0.486606
[epoch 27, batch  1299] avg loss: 0.509304
[epoch 27, batch  1399] avg loss: 0.496945
[epoch 27, batch  1499] avg loss: 0.490630
[epoch 27, batch  1599] avg loss: 0.489369
[epoch 27, batch  1699] avg loss: 0.492986
[epoch 27, batch  1799] avg loss: 0.483186
[epoch 27, batch  1899] avg loss: 0.500390
[epoch 27, batch  1999] avg loss: 0.502587
[epoch 27, batch  2099] avg loss: 0.504043
[epoch 27, batch  2199] avg loss: 0.492146
[epoch 27, batch  2299] avg loss: 0.479901
[epoch 27, batch  2399] avg loss: 0.494889
[epoch 27, batch  2499] avg loss: 0.491349
[epoch 28, batch    99] avg loss: 0.489589
[epoch 28, batch   199] avg loss: 0.476942
[epoch 28, batch   299] avg loss: 0.512115
[epoch 28, batch   399] avg loss: 0.495117
[epoch 28, batch   499] avg loss: 0.493840
[epoch 28, batch   599] avg loss: 0.510640
[epoch 28, batch   699] avg loss: 0.495221
[epoch 28, batch   799] avg loss: 0.494672
[epoch 28, batch   899] avg loss: 0.498535
[epoch 28, batch   999] avg loss: 0.476444
[epoch 28, batch  1099] avg loss: 0.491243
[epoch 28, batch  1199] avg loss: 0.495451
[epoch 28, batch  1299] avg loss: 0.510245
[epoch 28, batch  1399] avg loss: 0.499763
[epoch 28, batch  1499] avg loss: 0.483588
[epoch 28, batch  1599] avg loss: 0.492454
[epoch 28, batch  1699] avg loss: 0.499309
[epoch 28, batch  1799] avg loss: 0.494915
[epoch 28, batch  1899] avg loss: 0.506568
[epoch 28, batch  1999] avg loss: 0.503370
[epoch 28, batch  2099] avg loss: 0.477699
[epoch 28, batch  2199] avg loss: 0.481019
[epoch 28, batch  2299] avg loss: 0.497314
[epoch 28, batch  2399] avg loss: 0.487051
[epoch 28, batch  2499] avg loss: 0.494443
[epoch 29, batch    99] avg loss: 0.494397
[epoch 29, batch   199] avg loss: 0.481179
[epoch 29, batch   299] avg loss: 0.486640
[epoch 29, batch   399] avg loss: 0.489893
[epoch 29, batch   499] avg loss: 0.483049
[epoch 29, batch   599] avg loss: 0.488612
[epoch 29, batch   699] avg loss: 0.493950
[epoch 29, batch   799] avg loss: 0.485786
[epoch 29, batch   899] avg loss: 0.495739
[epoch 29, batch   999] avg loss: 0.491333
[epoch 29, batch  1099] avg loss: 0.472188
[epoch 29, batch  1199] avg loss: 0.485224
[epoch 29, batch  1299] avg loss: 0.475259
[epoch 29, batch  1399] avg loss: 0.485798
[epoch 29, batch  1499] avg loss: 0.503579
[epoch 29, batch  1599] avg loss: 0.489057
[epoch 29, batch  1699] avg loss: 0.480590
[epoch 29, batch  1799] avg loss: 0.504436
[epoch 29, batch  1899] avg loss: 0.489909
[epoch 29, batch  1999] avg loss: 0.477984
[epoch 29, batch  2099] avg loss: 0.496492
[epoch 29, batch  2199] avg loss: 0.479587
[epoch 29, batch  2299] avg loss: 0.487280
[epoch 29, batch  2399] avg loss: 0.490461
[epoch 29, batch  2499] avg loss: 0.494570
[epoch 30, batch    99] avg loss: 0.486307
[epoch 30, batch   199] avg loss: 0.523928
[epoch 30, batch   299] avg loss: 0.495910
[epoch 30, batch   399] avg loss: 0.492379
[epoch 30, batch   499] avg loss: 0.473100
[epoch 30, batch   599] avg loss: 0.495351
[epoch 30, batch   699] avg loss: 0.489086
[epoch 30, batch   799] avg loss: 0.472943
[epoch 30, batch   899] avg loss: 0.503604
[epoch 30, batch   999] avg loss: 0.472567
[epoch 30, batch  1099] avg loss: 0.493578
[epoch 30, batch  1199] avg loss: 0.476167
[epoch 30, batch  1299] avg loss: 0.497283
[epoch 30, batch  1399] avg loss: 0.492252
[epoch 30, batch  1499] avg loss: 0.500839
[epoch 30, batch  1599] avg loss: 0.473199
[epoch 30, batch  1699] avg loss: 0.483845
[epoch 30, batch  1799] avg loss: 0.516679
[epoch 30, batch  1899] avg loss: 0.498589
[epoch 30, batch  1999] avg loss: 0.465486
[epoch 30, batch  2099] avg loss: 0.502608
[epoch 30, batch  2199] avg loss: 0.501372
[epoch 30, batch  2299] avg loss: 0.474250
[epoch 30, batch  2399] avg loss: 0.485151
[epoch 30, batch  2499] avg loss: 0.492498
[epoch 31, batch    99] avg loss: 0.464261
[epoch 31, batch   199] avg loss: 0.488386
[epoch 31, batch   299] avg loss: 0.496277
[epoch 31, batch   399] avg loss: 0.496330
[epoch 31, batch   499] avg loss: 0.478877
[epoch 31, batch   599] avg loss: 0.490904
[epoch 31, batch   699] avg loss: 0.483622
[epoch 31, batch   799] avg loss: 0.480531
[epoch 31, batch   899] avg loss: 0.480599
[epoch 31, batch   999] avg loss: 0.482219
[epoch 31, batch  1099] avg loss: 0.481678
[epoch 31, batch  1199] avg loss: 0.489619
[epoch 31, batch  1299] avg loss: 0.466699
[epoch 31, batch  1399] avg loss: 0.481447
[epoch 31, batch  1499] avg loss: 0.504566
[epoch 31, batch  1599] avg loss: 0.469291
[epoch 31, batch  1699] avg loss: 0.483585
[epoch 31, batch  1799] avg loss: 0.475110
[epoch 31, batch  1899] avg loss: 0.493122
[epoch 31, batch  1999] avg loss: 0.481005
[epoch 31, batch  2099] avg loss: 0.487295
[epoch 31, batch  2199] avg loss: 0.481908
[epoch 31, batch  2299] avg loss: 0.472608
[epoch 31, batch  2399] avg loss: 0.476034
[epoch 31, batch  2499] avg loss: 0.486785
[epoch 32, batch    99] avg loss: 0.484918
[epoch 32, batch   199] avg loss: 0.487027
[epoch 32, batch   299] avg loss: 0.479370
[epoch 32, batch   399] avg loss: 0.483874
[epoch 32, batch   499] avg loss: 0.470012
[epoch 32, batch   599] avg loss: 0.483504
[epoch 32, batch   699] avg loss: 0.479408
[epoch 32, batch   799] avg loss: 0.484042
[epoch 32, batch   899] avg loss: 0.494102
[epoch 32, batch   999] avg loss: 0.486021
[epoch 32, batch  1099] avg loss: 0.491414
[epoch 32, batch  1199] avg loss: 0.474579
[epoch 32, batch  1299] avg loss: 0.474139
[epoch 32, batch  1399] avg loss: 0.467283
[epoch 32, batch  1499] avg loss: 0.475769
[epoch 32, batch  1599] avg loss: 0.486789
[epoch 32, batch  1699] avg loss: 0.470285
[epoch 32, batch  1799] avg loss: 0.483912
[epoch 32, batch  1899] avg loss: 0.483839
[epoch 32, batch  1999] avg loss: 0.485345
[epoch 32, batch  2099] avg loss: 0.483323
[epoch 32, batch  2199] avg loss: 0.485428
[epoch 32, batch  2299] avg loss: 0.493506
[epoch 32, batch  2399] avg loss: 0.481038
[epoch 32, batch  2499] avg loss: 0.478919
[epoch 33, batch    99] avg loss: 0.474697
[epoch 33, batch   199] avg loss: 0.487572
[epoch 33, batch   299] avg loss: 0.472156
[epoch 33, batch   399] avg loss: 0.495194
[epoch 33, batch   499] avg loss: 0.477619
[epoch 33, batch   599] avg loss: 0.478816
[epoch 33, batch   699] avg loss: 0.469898
[epoch 33, batch   799] avg loss: 0.501979
[epoch 33, batch   899] avg loss: 0.488628
[epoch 33, batch   999] avg loss: 0.490780
[epoch 33, batch  1099] avg loss: 0.486810
[epoch 33, batch  1199] avg loss: 0.468966
[epoch 33, batch  1299] avg loss: 0.479154
[epoch 33, batch  1399] avg loss: 0.493385
[epoch 33, batch  1499] avg loss: 0.465438
[epoch 33, batch  1599] avg loss: 0.465874
[epoch 33, batch  1699] avg loss: 0.493134
[epoch 33, batch  1799] avg loss: 0.501114
[epoch 33, batch  1899] avg loss: 0.482170
[epoch 33, batch  1999] avg loss: 0.468763
[epoch 33, batch  2099] avg loss: 0.471048
[epoch 33, batch  2199] avg loss: 0.484718
[epoch 33, batch  2299] avg loss: 0.458780
[epoch 33, batch  2399] avg loss: 0.480793
[epoch 33, batch  2499] avg loss: 0.468037
[epoch 34, batch    99] avg loss: 0.476616
[epoch 34, batch   199] avg loss: 0.472693
[epoch 34, batch   299] avg loss: 0.478794
[epoch 34, batch   399] avg loss: 0.481731
[epoch 34, batch   499] avg loss: 0.513592
[epoch 34, batch   599] avg loss: 0.465987
[epoch 34, batch   699] avg loss: 0.476176
[epoch 34, batch   799] avg loss: 0.472094
[epoch 34, batch   899] avg loss: 0.482910
[epoch 34, batch   999] avg loss: 0.470368
[epoch 34, batch  1099] avg loss: 0.468603
[epoch 34, batch  1199] avg loss: 0.486636
[epoch 34, batch  1299] avg loss: 0.482249
[epoch 34, batch  1399] avg loss: 0.477753
[epoch 34, batch  1499] avg loss: 0.463137
[epoch 34, batch  1599] avg loss: 0.483547
[epoch 34, batch  1699] avg loss: 0.485901
[epoch 34, batch  1799] avg loss: 0.496029
[epoch 34, batch  1899] avg loss: 0.470475
[epoch 34, batch  1999] avg loss: 0.483487
[epoch 34, batch  2099] avg loss: 0.468712
[epoch 34, batch  2199] avg loss: 0.472118
[epoch 34, batch  2299] avg loss: 0.464969
[epoch 34, batch  2399] avg loss: 0.462285
[epoch 34, batch  2499] avg loss: 0.491407
[epoch 35, batch    99] avg loss: 0.472799
[epoch 35, batch   199] avg loss: 0.468670
[epoch 35, batch   299] avg loss: 0.484554
[epoch 35, batch   399] avg loss: 0.466281
[epoch 35, batch   499] avg loss: 0.479247
[epoch 35, batch   599] avg loss: 0.460524
[epoch 35, batch   699] avg loss: 0.459735
[epoch 35, batch   799] avg loss: 0.488877
[epoch 35, batch   899] avg loss: 0.471673
[epoch 35, batch   999] avg loss: 0.468684
[epoch 35, batch  1099] avg loss: 0.474789
[epoch 35, batch  1199] avg loss: 0.471340
[epoch 35, batch  1299] avg loss: 0.469663
[epoch 35, batch  1399] avg loss: 0.478448
[epoch 35, batch  1499] avg loss: 0.470968
[epoch 35, batch  1599] avg loss: 0.484128
[epoch 35, batch  1699] avg loss: 0.473253
[epoch 35, batch  1799] avg loss: 0.474635
[epoch 35, batch  1899] avg loss: 0.480897
[epoch 35, batch  1999] avg loss: 0.476519
[epoch 35, batch  2099] avg loss: 0.495228
[epoch 35, batch  2199] avg loss: 0.475782
[epoch 35, batch  2299] avg loss: 0.480871
[epoch 35, batch  2399] avg loss: 0.495705
[epoch 35, batch  2499] avg loss: 0.473093
[epoch 36, batch    99] avg loss: 0.478886
[epoch 36, batch   199] avg loss: 0.454907
[epoch 36, batch   299] avg loss: 0.482242
[epoch 36, batch   399] avg loss: 0.465269
[epoch 36, batch   499] avg loss: 0.479964
[epoch 36, batch   599] avg loss: 0.475244
[epoch 36, batch   699] avg loss: 0.460401
[epoch 36, batch   799] avg loss: 0.467573
[epoch 36, batch   899] avg loss: 0.473801
[epoch 36, batch   999] avg loss: 0.449365
[epoch 36, batch  1099] avg loss: 0.517918
[epoch 36, batch  1199] avg loss: 0.474529
[epoch 36, batch  1299] avg loss: 0.510300
[epoch 36, batch  1399] avg loss: 0.485248
[epoch 36, batch  1499] avg loss: 0.467404
[epoch 36, batch  1599] avg loss: 0.477923
[epoch 36, batch  1699] avg loss: 0.464713
[epoch 36, batch  1799] avg loss: 0.474989
[epoch 36, batch  1899] avg loss: 0.479733
[epoch 36, batch  1999] avg loss: 0.461602
[epoch 36, batch  2099] avg loss: 0.461017
[epoch 36, batch  2199] avg loss: 0.460402
[epoch 36, batch  2299] avg loss: 0.496709
[epoch 36, batch  2399] avg loss: 0.479030
[epoch 36, batch  2499] avg loss: 0.465806
[epoch 37, batch    99] avg loss: 0.475058
[epoch 37, batch   199] avg loss: 0.466367
[epoch 37, batch   299] avg loss: 0.467317
[epoch 37, batch   399] avg loss: 0.481529
[epoch 37, batch   499] avg loss: 0.468383
[epoch 37, batch   599] avg loss: 0.469622
[epoch 37, batch   699] avg loss: 0.459644
[epoch 37, batch   799] avg loss: 0.470971
[epoch 37, batch   899] avg loss: 0.468014
[epoch 37, batch   999] avg loss: 0.476052
[epoch 37, batch  1099] avg loss: 0.480768
[epoch 37, batch  1199] avg loss: 0.478034
[epoch 37, batch  1299] avg loss: 0.450824
[epoch 37, batch  1399] avg loss: 0.464821
[epoch 37, batch  1499] avg loss: 0.462816
[epoch 37, batch  1599] avg loss: 0.473214
[epoch 37, batch  1699] avg loss: 0.473776
[epoch 37, batch  1799] avg loss: 0.474129
[epoch 37, batch  1899] avg loss: 0.463549
[epoch 37, batch  1999] avg loss: 0.475695
[epoch 37, batch  2099] avg loss: 0.463037
[epoch 37, batch  2199] avg loss: 0.470200
[epoch 37, batch  2299] avg loss: 0.468540
[epoch 37, batch  2399] avg loss: 0.454770
[epoch 37, batch  2499] avg loss: 0.479638
[epoch 38, batch    99] avg loss: 0.467701
[epoch 38, batch   199] avg loss: 0.479644
[epoch 38, batch   299] avg loss: 0.470445
[epoch 38, batch   399] avg loss: 0.471454
[epoch 38, batch   499] avg loss: 0.479286
[epoch 38, batch   599] avg loss: 0.474760
[epoch 38, batch   699] avg loss: 0.477316
[epoch 38, batch   799] avg loss: 0.471781
[epoch 38, batch   899] avg loss: 0.457980
[epoch 38, batch   999] avg loss: 0.480468
[epoch 38, batch  1099] avg loss: 0.478547
[epoch 38, batch  1199] avg loss: 0.468523
[epoch 38, batch  1299] avg loss: 0.459614
[epoch 38, batch  1399] avg loss: 0.468933
[epoch 38, batch  1499] avg loss: 0.493532
[epoch 38, batch  1599] avg loss: 0.479343
[epoch 38, batch  1699] avg loss: 0.473374
[epoch 38, batch  1799] avg loss: 0.471267
[epoch 38, batch  1899] avg loss: 0.457409
[epoch 38, batch  1999] avg loss: 0.458237
[epoch 38, batch  2099] avg loss: 0.461961
[epoch 38, batch  2199] avg loss: 0.458944
[epoch 38, batch  2299] avg loss: 0.494824
[epoch 38, batch  2399] avg loss: 0.456998
[epoch 38, batch  2499] avg loss: 0.459556
[epoch 39, batch    99] avg loss: 0.482550
[epoch 39, batch   199] avg loss: 0.474563
[epoch 39, batch   299] avg loss: 0.475434
[epoch 39, batch   399] avg loss: 0.478136
[epoch 39, batch   499] avg loss: 0.488155
[epoch 39, batch   599] avg loss: 0.465964
[epoch 39, batch   699] avg loss: 0.470301
[epoch 39, batch   799] avg loss: 0.475858
[epoch 39, batch   899] avg loss: 0.456467
[epoch 39, batch   999] avg loss: 0.458004
[epoch 39, batch  1099] avg loss: 0.464231
[epoch 39, batch  1199] avg loss: 0.465320
[epoch 39, batch  1299] avg loss: 0.459249
[epoch 39, batch  1399] avg loss: 0.485859
[epoch 39, batch  1499] avg loss: 0.469495
[epoch 39, batch  1599] avg loss: 0.456695
[epoch 39, batch  1699] avg loss: 0.461301
[epoch 39, batch  1799] avg loss: 0.465581
[epoch 39, batch  1899] avg loss: 0.454568
[epoch 39, batch  1999] avg loss: 0.475373
[epoch 39, batch  2099] avg loss: 0.464062
[epoch 39, batch  2199] avg loss: 0.449431
[epoch 39, batch  2299] avg loss: 0.485465
[epoch 39, batch  2399] avg loss: 0.471039
[epoch 39, batch  2499] avg loss: 0.455168
Model saved to model/20200502-022047.pth.
accuracy/TriangPrismIsosc : 0.6966666666666667
n_examples/TriangPrismIsosc : 1500.0
accuracy/parallelepiped : 0.4066666666666667
n_examples/parallelepiped : 1500.0
accuracy/sphere : 0.9901960784313726
n_examples/sphere : 306.0
accuracy/wire : 0.9216666666666666
n_examples/wire : 600.0
accuracy/avg_geom : 0.6428571428571429
loss/validation_geom : 0.7657858815122371
accuracy/Au : 0.7987711213517665
n_examples/Au : 1302.0
accuracy/SiN : 0.026113671274961597
n_examples/SiN : 1302.0
accuracy/SiO2 : 0.18509984639016897
n_examples/SiO2 : 1302.0
accuracy/avg_mat : 0.33666154633896567
loss/validation_mat : 1.2514832650949816
MSE/ShortestDim : 1.0251859858712202
MAE/ShortestDim : 0.5508467040334185
MSE/MiddleDim : 7.66841665658045
MAE/MiddleDim : 1.5610413747265959
MSE/LongDim : 153.98072834425076
MAE/LongDim : 7.64743586216471
MSE/log Area/Vol : 20.128467909446204
MAE/log Area/Vol : 3.51790690885955
loss/validation_dim : 182.80279889614866
loss/validation : 184.82006804275588
Metrics saved to model/20200502-022047_metrics.csv.
Parsed 7812 rows from data/sim_train_spectrum_all.
Parsed 7812 rows from data/sim_train_labels_all.
Parsed 9765 rows from data/gen_spectrum_all_00-of-16.
Parsed 9765 rows from data/gen_labels_all_00-of-16.
Parsed 9765 rows from data/gen_spectrum_all_01-of-16.
Parsed 9765 rows from data/gen_labels_all_01-of-16.
Parsed 9765 rows from data/gen_spectrum_all_02-of-16.
Parsed 9765 rows from data/gen_labels_all_02-of-16.
Parsed 9765 rows from data/gen_spectrum_all_03-of-16.
Parsed 9765 rows from data/gen_labels_all_03-of-16.
Parsed 9765 rows from data/gen_spectrum_all_04-of-16.
Parsed 9765 rows from data/gen_labels_all_04-of-16.
Parsed 9765 rows from data/gen_spectrum_all_05-of-16.
Parsed 9765 rows from data/gen_labels_all_05-of-16.
Parsed 9765 rows from data/gen_spectrum_all_06-of-16.
Parsed 9765 rows from data/gen_labels_all_06-of-16.
Parsed 9765 rows from data/gen_spectrum_all_07-of-16.
Parsed 9765 rows from data/gen_labels_all_07-of-16.
Parsed 9765 rows from data/gen_spectrum_all_08-of-16.
Parsed 9765 rows from data/gen_labels_all_08-of-16.
Parsed 9765 rows from data/gen_spectrum_all_09-of-16.
Parsed 9765 rows from data/gen_labels_all_09-of-16.
Parsed 9765 rows from data/gen_spectrum_all_10-of-16.
Parsed 9765 rows from data/gen_labels_all_10-of-16.
Parsed 9765 rows from data/gen_spectrum_all_11-of-16.
Parsed 9765 rows from data/gen_labels_all_11-of-16.
Parsed 9765 rows from data/gen_spectrum_all_12-of-16.
Parsed 9765 rows from data/gen_labels_all_12-of-16.
Parsed 9765 rows from data/gen_spectrum_all_13-of-16.
Parsed 9765 rows from data/gen_labels_all_13-of-16.
Parsed 9765 rows from data/gen_spectrum_all_14-of-16.
Parsed 9765 rows from data/gen_labels_all_14-of-16.
Parsed 9765 rows from data/gen_spectrum_all_15-of-16.
Parsed 9765 rows from data/gen_labels_all_15-of-16.
Parsed 3906 rows from data/sim_validation_spectrum_all.
Parsed 3906 rows from data/sim_validation_labels_all.
Logging training progress to tensorboard dir runs/alexnet-all-lr_0.001000-trainsize_164052-05_02_2020_02:21.
[epoch 0, batch    99] avg loss: 1.278986
[epoch 0, batch   199] avg loss: 1.055836
[epoch 0, batch   299] avg loss: 0.938687
[epoch 0, batch   399] avg loss: 0.916649
[epoch 0, batch   499] avg loss: 0.887063
[epoch 0, batch   599] avg loss: 0.860289
[epoch 0, batch   699] avg loss: 0.821494
[epoch 0, batch   799] avg loss: 0.823195
[epoch 0, batch   899] avg loss: 0.812752
[epoch 0, batch   999] avg loss: 0.792546
[epoch 0, batch  1099] avg loss: 0.819489
[epoch 0, batch  1199] avg loss: 0.796590
[epoch 0, batch  1299] avg loss: 0.778497
[epoch 0, batch  1399] avg loss: 0.782024
[epoch 0, batch  1499] avg loss: 0.773028
[epoch 0, batch  1599] avg loss: 0.765188
[epoch 0, batch  1699] avg loss: 0.794882
[epoch 0, batch  1799] avg loss: 0.761812
[epoch 0, batch  1899] avg loss: 0.760757
[epoch 0, batch  1999] avg loss: 0.752567
[epoch 0, batch  2099] avg loss: 0.755791
[epoch 0, batch  2199] avg loss: 0.754882
[epoch 0, batch  2299] avg loss: 0.729529
[epoch 0, batch  2399] avg loss: 0.725233
[epoch 0, batch  2499] avg loss: 0.749322
[epoch 1, batch    99] avg loss: 0.718198
[epoch 1, batch   199] avg loss: 0.724788
[epoch 1, batch   299] avg loss: 0.727843
[epoch 1, batch   399] avg loss: 0.739125
[epoch 1, batch   499] avg loss: 0.718084
[epoch 1, batch   599] avg loss: 0.718813
[epoch 1, batch   699] avg loss: 0.697289
[epoch 1, batch   799] avg loss: 0.707951
[epoch 1, batch   899] avg loss: 0.715078
[epoch 1, batch   999] avg loss: 0.691615
[epoch 1, batch  1099] avg loss: 0.692160
[epoch 1, batch  1199] avg loss: 0.699048
[epoch 1, batch  1299] avg loss: 0.708197
[epoch 1, batch  1399] avg loss: 0.710227
[epoch 1, batch  1499] avg loss: 0.708761
[epoch 1, batch  1599] avg loss: 0.698173
[epoch 1, batch  1699] avg loss: 0.680545
[epoch 1, batch  1799] avg loss: 0.690392
[epoch 1, batch  1899] avg loss: 0.676104
[epoch 1, batch  1999] avg loss: 0.668861
[epoch 1, batch  2099] avg loss: 0.684427
[epoch 1, batch  2199] avg loss: 0.700550
[epoch 1, batch  2299] avg loss: 0.680503
[epoch 1, batch  2399] avg loss: 0.678005
[epoch 1, batch  2499] avg loss: 0.713776
[epoch 2, batch    99] avg loss: 0.683703
[epoch 2, batch   199] avg loss: 0.671968
[epoch 2, batch   299] avg loss: 0.703041
[epoch 2, batch   399] avg loss: 0.668957
[epoch 2, batch   499] avg loss: 0.682654
[epoch 2, batch   599] avg loss: 0.695972
[epoch 2, batch   699] avg loss: 0.681793
[epoch 2, batch   799] avg loss: 0.679392
[epoch 2, batch   899] avg loss: 0.653535
[epoch 2, batch   999] avg loss: 0.660887
[epoch 2, batch  1099] avg loss: 0.664138
[epoch 2, batch  1199] avg loss: 0.661800
[epoch 2, batch  1299] avg loss: 0.659844
[epoch 2, batch  1399] avg loss: 0.651870
[epoch 2, batch  1499] avg loss: 0.665699
[epoch 2, batch  1599] avg loss: 0.663516
[epoch 2, batch  1699] avg loss: 0.655459
[epoch 2, batch  1799] avg loss: 0.663480
[epoch 2, batch  1899] avg loss: 0.651380
[epoch 2, batch  1999] avg loss: 0.665930
[epoch 2, batch  2099] avg loss: 0.658781
[epoch 2, batch  2199] avg loss: 0.649229
[epoch 2, batch  2299] avg loss: 0.630689
[epoch 2, batch  2399] avg loss: 0.656204
[epoch 2, batch  2499] avg loss: 0.648619
[epoch 3, batch    99] avg loss: 0.655386
[epoch 3, batch   199] avg loss: 0.636432
[epoch 3, batch   299] avg loss: 0.639556
[epoch 3, batch   399] avg loss: 0.666603
[epoch 3, batch   499] avg loss: 0.653647
[epoch 3, batch   599] avg loss: 0.644580
[epoch 3, batch   699] avg loss: 0.682040
[epoch 3, batch   799] avg loss: 0.644274
[epoch 3, batch   899] avg loss: 0.639272
[epoch 3, batch   999] avg loss: 0.627923
[epoch 3, batch  1099] avg loss: 0.660038
[epoch 3, batch  1199] avg loss: 0.624305
[epoch 3, batch  1299] avg loss: 0.639610
[epoch 3, batch  1399] avg loss: 0.626304
[epoch 3, batch  1499] avg loss: 0.637891
[epoch 3, batch  1599] avg loss: 0.629591
[epoch 3, batch  1699] avg loss: 0.637695
[epoch 3, batch  1799] avg loss: 0.650238
[epoch 3, batch  1899] avg loss: 0.624863
[epoch 3, batch  1999] avg loss: 0.649415
[epoch 3, batch  2099] avg loss: 0.636571
[epoch 3, batch  2199] avg loss: 0.622972
[epoch 3, batch  2299] avg loss: 0.634245
[epoch 3, batch  2399] avg loss: 0.626331
[epoch 3, batch  2499] avg loss: 0.640543
[epoch 4, batch    99] avg loss: 0.626456
[epoch 4, batch   199] avg loss: 0.637396
[epoch 4, batch   299] avg loss: 0.688583
[epoch 4, batch   399] avg loss: 0.667814
[epoch 4, batch   499] avg loss: 0.662774
[epoch 4, batch   599] avg loss: 0.629478
[epoch 4, batch   699] avg loss: 0.649578
[epoch 4, batch   799] avg loss: 0.650366
[epoch 4, batch   899] avg loss: 0.629672
[epoch 4, batch   999] avg loss: 0.625977
[epoch 4, batch  1099] avg loss: 0.647708
[epoch 4, batch  1199] avg loss: 0.638002
[epoch 4, batch  1299] avg loss: 0.627121
[epoch 4, batch  1399] avg loss: 0.633385
[epoch 4, batch  1499] avg loss: 0.615200
[epoch 4, batch  1599] avg loss: 0.607599
[epoch 4, batch  1699] avg loss: 0.630901
[epoch 4, batch  1799] avg loss: 0.613990
[epoch 4, batch  1899] avg loss: 0.656688
[epoch 4, batch  1999] avg loss: 0.614548
[epoch 4, batch  2099] avg loss: 0.622913
[epoch 4, batch  2199] avg loss: 0.641629
[epoch 4, batch  2299] avg loss: 0.647429
[epoch 4, batch  2399] avg loss: 0.636257
[epoch 4, batch  2499] avg loss: 0.621689
[epoch 5, batch    99] avg loss: 0.621858
[epoch 5, batch   199] avg loss: 0.635223
[epoch 5, batch   299] avg loss: 0.707910
[epoch 5, batch   399] avg loss: 0.643885
[epoch 5, batch   499] avg loss: 0.640462
[epoch 5, batch   599] avg loss: 0.612724
[epoch 5, batch   699] avg loss: 0.629915
[epoch 5, batch   799] avg loss: 0.619133
[epoch 5, batch   899] avg loss: 0.616955
[epoch 5, batch   999] avg loss: 0.616747
[epoch 5, batch  1099] avg loss: 0.632030
[epoch 5, batch  1199] avg loss: 0.621311
[epoch 5, batch  1299] avg loss: 0.620872
[epoch 5, batch  1399] avg loss: 0.606125
[epoch 5, batch  1499] avg loss: 0.604568
[epoch 5, batch  1599] avg loss: 0.599682
[epoch 5, batch  1699] avg loss: 0.618501
[epoch 5, batch  1799] avg loss: 0.635029
[epoch 5, batch  1899] avg loss: 0.628035
[epoch 5, batch  1999] avg loss: 0.624336
[epoch 5, batch  2099] avg loss: 0.610537
[epoch 5, batch  2199] avg loss: 0.621857
[epoch 5, batch  2299] avg loss: 0.627359
[epoch 5, batch  2399] avg loss: 0.614625
[epoch 5, batch  2499] avg loss: 0.635753
[epoch 6, batch    99] avg loss: 0.618446
[epoch 6, batch   199] avg loss: 0.613997
[epoch 6, batch   299] avg loss: 0.624497
[epoch 6, batch   399] avg loss: 0.640014
[epoch 6, batch   499] avg loss: 0.626828
[epoch 6, batch   599] avg loss: 0.626707
[epoch 6, batch   699] avg loss: 0.619306
[epoch 6, batch   799] avg loss: 0.600504
[epoch 6, batch   899] avg loss: 0.606714
[epoch 6, batch   999] avg loss: 0.631820
[epoch 6, batch  1099] avg loss: 0.620071
[epoch 6, batch  1199] avg loss: 0.606091
[epoch 6, batch  1299] avg loss: 0.617979
[epoch 6, batch  1399] avg loss: 0.616058
[epoch 6, batch  1499] avg loss: 0.645487
[epoch 6, batch  1599] avg loss: 0.605321
[epoch 6, batch  1699] avg loss: 0.607978
[epoch 6, batch  1799] avg loss: 0.609678
[epoch 6, batch  1899] avg loss: 0.618579
[epoch 6, batch  1999] avg loss: 0.606497
[epoch 6, batch  2099] avg loss: 0.605715
[epoch 6, batch  2199] avg loss: 0.617432
[epoch 6, batch  2299] avg loss: 0.613377
[epoch 6, batch  2399] avg loss: 0.635136
[epoch 6, batch  2499] avg loss: 0.613470
[epoch 7, batch    99] avg loss: 0.605650
[epoch 7, batch   199] avg loss: 0.609803
[epoch 7, batch   299] avg loss: 0.624780
[epoch 7, batch   399] avg loss: 0.601907
[epoch 7, batch   499] avg loss: 0.610726
[epoch 7, batch   599] avg loss: 0.602265
[epoch 7, batch   699] avg loss: 0.639272
[epoch 7, batch   799] avg loss: 0.603896
[epoch 7, batch   899] avg loss: 0.593192
[epoch 7, batch   999] avg loss: 0.593923
[epoch 7, batch  1099] avg loss: 0.593180
[epoch 7, batch  1199] avg loss: 0.623822
[epoch 7, batch  1299] avg loss: 0.595123
[epoch 7, batch  1399] avg loss: 0.597283
[epoch 7, batch  1499] avg loss: 0.610740
[epoch 7, batch  1599] avg loss: 0.598266
[epoch 7, batch  1699] avg loss: 0.595878
[epoch 7, batch  1799] avg loss: 0.642281
[epoch 7, batch  1899] avg loss: 0.612079
[epoch 7, batch  1999] avg loss: 0.591591
[epoch 7, batch  2099] avg loss: 0.596939
[epoch 7, batch  2199] avg loss: 0.634336
[epoch 7, batch  2299] avg loss: 0.647399
[epoch 7, batch  2399] avg loss: 0.615767
[epoch 7, batch  2499] avg loss: 0.605640
[epoch 8, batch    99] avg loss: 0.598069
[epoch 8, batch   199] avg loss: 0.623952
[epoch 8, batch   299] avg loss: 0.577589
[epoch 8, batch   399] avg loss: 0.601858
[epoch 8, batch   499] avg loss: 0.620404
[epoch 8, batch   599] avg loss: 0.623819
[epoch 8, batch   699] avg loss: 0.581867
[epoch 8, batch   799] avg loss: 0.593112
[epoch 8, batch   899] avg loss: 0.591564
[epoch 8, batch   999] avg loss: 0.606478
[epoch 8, batch  1099] avg loss: 0.608615
[epoch 8, batch  1199] avg loss: 0.602801
[epoch 8, batch  1299] avg loss: 0.605721
[epoch 8, batch  1399] avg loss: 0.656385
[epoch 8, batch  1499] avg loss: 0.600902
[epoch 8, batch  1599] avg loss: 0.632986
[epoch 8, batch  1699] avg loss: 0.632868
[epoch 8, batch  1799] avg loss: 0.607713
[epoch 8, batch  1899] avg loss: 0.591472
[epoch 8, batch  1999] avg loss: 0.586164
[epoch 8, batch  2099] avg loss: 0.607927
[epoch 8, batch  2199] avg loss: 0.598017
[epoch 8, batch  2299] avg loss: 0.614868
[epoch 8, batch  2399] avg loss: 0.615860
[epoch 8, batch  2499] avg loss: 0.592929
[epoch 9, batch    99] avg loss: 0.584280
[epoch 9, batch   199] avg loss: 0.588976
[epoch 9, batch   299] avg loss: 0.601189
[epoch 9, batch   399] avg loss: 0.581238
[epoch 9, batch   499] avg loss: 0.585944
[epoch 9, batch   599] avg loss: 0.574803
[epoch 9, batch   699] avg loss: 0.591426
[epoch 9, batch   799] avg loss: 0.598463
[epoch 9, batch   899] avg loss: 0.618513
[epoch 9, batch   999] avg loss: 0.591587
[epoch 9, batch  1099] avg loss: 0.592885
[epoch 9, batch  1199] avg loss: 0.584420
[epoch 9, batch  1299] avg loss: 0.604978
[epoch 9, batch  1399] avg loss: 0.585683
[epoch 9, batch  1499] avg loss: 0.589891
[epoch 9, batch  1599] avg loss: 0.590982
[epoch 9, batch  1699] avg loss: 0.595889
[epoch 9, batch  1799] avg loss: 0.586168
[epoch 9, batch  1899] avg loss: 0.597028
[epoch 9, batch  1999] avg loss: 0.599392
[epoch 9, batch  2099] avg loss: 0.585024
[epoch 9, batch  2199] avg loss: 0.587354
[epoch 9, batch  2299] avg loss: 0.590139
[epoch 9, batch  2399] avg loss: 0.589506
[epoch 9, batch  2499] avg loss: 0.593644
[epoch 10, batch    99] avg loss: 0.595837
[epoch 10, batch   199] avg loss: 0.614884
[epoch 10, batch   299] avg loss: 0.588521
[epoch 10, batch   399] avg loss: 0.613683
[epoch 10, batch   499] avg loss: 0.586193
[epoch 10, batch   599] avg loss: 0.593390
[epoch 10, batch   699] avg loss: 0.587190
[epoch 10, batch   799] avg loss: 0.593197
[epoch 10, batch   899] avg loss: 0.592549
[epoch 10, batch   999] avg loss: 0.623958
[epoch 10, batch  1099] avg loss: 0.598240
[epoch 10, batch  1199] avg loss: 0.574971
[epoch 10, batch  1299] avg loss: 0.575486
[epoch 10, batch  1399] avg loss: 0.594279
[epoch 10, batch  1499] avg loss: 0.585067
[epoch 10, batch  1599] avg loss: 0.582404
[epoch 10, batch  1699] avg loss: 0.601063
[epoch 10, batch  1799] avg loss: 0.587234
[epoch 10, batch  1899] avg loss: 0.590397
[epoch 10, batch  1999] avg loss: 0.624547
[epoch 10, batch  2099] avg loss: 0.614033
[epoch 10, batch  2199] avg loss: 0.598947
[epoch 10, batch  2299] avg loss: 0.578112
[epoch 10, batch  2399] avg loss: 0.586076
[epoch 10, batch  2499] avg loss: 0.599348
[epoch 11, batch    99] avg loss: 0.584074
[epoch 11, batch   199] avg loss: 0.568386
[epoch 11, batch   299] avg loss: 0.571049
[epoch 11, batch   399] avg loss: 0.574274
[epoch 11, batch   499] avg loss: 0.583477
[epoch 11, batch   599] avg loss: 0.590741
[epoch 11, batch   699] avg loss: 0.574973
[epoch 11, batch   799] avg loss: 0.583081
[epoch 11, batch   899] avg loss: 0.574283
[epoch 11, batch   999] avg loss: 0.602068
[epoch 11, batch  1099] avg loss: 0.578976
[epoch 11, batch  1199] avg loss: 0.587048
[epoch 11, batch  1299] avg loss: 0.588245
[epoch 11, batch  1399] avg loss: 0.580797
[epoch 11, batch  1499] avg loss: 0.569621
[epoch 11, batch  1599] avg loss: 0.579583
[epoch 11, batch  1699] avg loss: 0.581473
[epoch 11, batch  1799] avg loss: 0.586949
[epoch 11, batch  1899] avg loss: 0.579325
[epoch 11, batch  1999] avg loss: 0.580398
[epoch 11, batch  2099] avg loss: 0.579045
[epoch 11, batch  2199] avg loss: 0.578470
[epoch 11, batch  2299] avg loss: 0.580006
[epoch 11, batch  2399] avg loss: 0.582610
[epoch 11, batch  2499] avg loss: 0.562758
[epoch 12, batch    99] avg loss: 0.582045
[epoch 12, batch   199] avg loss: 0.553797
[epoch 12, batch   299] avg loss: 0.569833
[epoch 12, batch   399] avg loss: 0.574124
[epoch 12, batch   499] avg loss: 0.590725
[epoch 12, batch   599] avg loss: 0.599655
[epoch 12, batch   699] avg loss: 0.576470
[epoch 12, batch   799] avg loss: 0.583737
[epoch 12, batch   899] avg loss: 0.602298
[epoch 12, batch   999] avg loss: 0.566835
[epoch 12, batch  1099] avg loss: 0.579025
[epoch 12, batch  1199] avg loss: 0.567293
[epoch 12, batch  1299] avg loss: 0.575420
[epoch 12, batch  1399] avg loss: 0.572953
[epoch 12, batch  1499] avg loss: 0.567093
[epoch 12, batch  1599] avg loss: 0.582051
[epoch 12, batch  1699] avg loss: 0.580754
[epoch 12, batch  1799] avg loss: 0.577269
[epoch 12, batch  1899] avg loss: 0.573928
[epoch 12, batch  1999] avg loss: 0.569974
[epoch 12, batch  2099] avg loss: 0.600330
[epoch 12, batch  2199] avg loss: 0.579085
[epoch 12, batch  2299] avg loss: 0.573153
[epoch 12, batch  2399] avg loss: 0.562315
[epoch 12, batch  2499] avg loss: 0.586261
[epoch 13, batch    99] avg loss: 0.586231
[epoch 13, batch   199] avg loss: 0.587182
[epoch 13, batch   299] avg loss: 0.557014
[epoch 13, batch   399] avg loss: 0.557123
[epoch 13, batch   499] avg loss: 0.566515
[epoch 13, batch   599] avg loss: 0.573676
[epoch 13, batch   699] avg loss: 0.564698
[epoch 13, batch   799] avg loss: 0.581148
[epoch 13, batch   899] avg loss: 0.553558
[epoch 13, batch   999] avg loss: 0.588286
[epoch 13, batch  1099] avg loss: 0.557573
[epoch 13, batch  1199] avg loss: 0.574202
[epoch 13, batch  1299] avg loss: 0.562329
[epoch 13, batch  1399] avg loss: 0.555957
[epoch 13, batch  1499] avg loss: 0.548617
[epoch 13, batch  1599] avg loss: 0.564849
[epoch 13, batch  1699] avg loss: 0.563207
[epoch 13, batch  1799] avg loss: 0.567741
[epoch 13, batch  1899] avg loss: 0.613626
[epoch 13, batch  1999] avg loss: 0.589623
[epoch 13, batch  2099] avg loss: 0.567907
[epoch 13, batch  2199] avg loss: 0.565753
[epoch 13, batch  2299] avg loss: 0.589366
[epoch 13, batch  2399] avg loss: 0.564659
[epoch 13, batch  2499] avg loss: 0.578818
[epoch 14, batch    99] avg loss: 0.576114
[epoch 14, batch   199] avg loss: 0.615676
[epoch 14, batch   299] avg loss: 0.570530
[epoch 14, batch   399] avg loss: 0.566260
[epoch 14, batch   499] avg loss: 0.552534
[epoch 14, batch   599] avg loss: 0.567357
[epoch 14, batch   699] avg loss: 0.579868
[epoch 14, batch   799] avg loss: 0.556578
[epoch 14, batch   899] avg loss: 0.554468
[epoch 14, batch   999] avg loss: 0.594594
[epoch 14, batch  1099] avg loss: 0.558738
[epoch 14, batch  1199] avg loss: 0.590185
[epoch 14, batch  1299] avg loss: 0.582956
[epoch 14, batch  1399] avg loss: 0.573451
[epoch 14, batch  1499] avg loss: 0.554628
[epoch 14, batch  1599] avg loss: 0.590272
[epoch 14, batch  1699] avg loss: 0.578486
[epoch 14, batch  1799] avg loss: 0.558343
[epoch 14, batch  1899] avg loss: 0.562968
[epoch 14, batch  1999] avg loss: 0.585651
[epoch 14, batch  2099] avg loss: 0.562779
[epoch 14, batch  2199] avg loss: 0.553467
[epoch 14, batch  2299] avg loss: 0.562070
[epoch 14, batch  2399] avg loss: 0.573087
[epoch 14, batch  2499] avg loss: 0.558931
[epoch 15, batch    99] avg loss: 0.557337
[epoch 15, batch   199] avg loss: 0.552102
[epoch 15, batch   299] avg loss: 0.545469
[epoch 15, batch   399] avg loss: 0.552882
[epoch 15, batch   499] avg loss: 0.551229
[epoch 15, batch   599] avg loss: 0.574099
[epoch 15, batch   699] avg loss: 0.574713
[epoch 15, batch   799] avg loss: 0.564555
[epoch 15, batch   899] avg loss: 0.571796
[epoch 15, batch   999] avg loss: 0.557313
[epoch 15, batch  1099] avg loss: 0.563458
[epoch 15, batch  1199] avg loss: 0.551168
[epoch 15, batch  1299] avg loss: 0.561771
[epoch 15, batch  1399] avg loss: 0.568974
[epoch 15, batch  1499] avg loss: 0.548935
[epoch 15, batch  1599] avg loss: 0.561422
[epoch 15, batch  1699] avg loss: 0.558194
[epoch 15, batch  1799] avg loss: 0.565706
[epoch 15, batch  1899] avg loss: 0.565908
[epoch 15, batch  1999] avg loss: 0.536614
[epoch 15, batch  2099] avg loss: 0.561292
[epoch 15, batch  2199] avg loss: 0.566940
[epoch 15, batch  2299] avg loss: 0.575805
[epoch 15, batch  2399] avg loss: 0.595525
[epoch 15, batch  2499] avg loss: 0.581683
[epoch 16, batch    99] avg loss: 0.556080
[epoch 16, batch   199] avg loss: 0.561464
[epoch 16, batch   299] avg loss: 0.542352
[epoch 16, batch   399] avg loss: 0.569529
[epoch 16, batch   499] avg loss: 0.545728
[epoch 16, batch   599] avg loss: 0.590339
[epoch 16, batch   699] avg loss: 0.546385
[epoch 16, batch   799] avg loss: 0.567851
[epoch 16, batch   899] avg loss: 0.556498
[epoch 16, batch   999] avg loss: 0.560667
[epoch 16, batch  1099] avg loss: 0.567907
[epoch 16, batch  1199] avg loss: 0.551176
[epoch 16, batch  1299] avg loss: 0.579415
[epoch 16, batch  1399] avg loss: 0.549049
[epoch 16, batch  1499] avg loss: 0.550536
[epoch 16, batch  1599] avg loss: 0.548626
[epoch 16, batch  1699] avg loss: 0.566012
[epoch 16, batch  1799] avg loss: 0.548631
[epoch 16, batch  1899] avg loss: 0.549151
[epoch 16, batch  1999] avg loss: 0.550348
[epoch 16, batch  2099] avg loss: 0.571587
[epoch 16, batch  2199] avg loss: 0.560835
[epoch 16, batch  2299] avg loss: 0.556579
[epoch 16, batch  2399] avg loss: 0.582485
[epoch 16, batch  2499] avg loss: 0.572516
[epoch 17, batch    99] avg loss: 0.561967
[epoch 17, batch   199] avg loss: 0.563924
[epoch 17, batch   299] avg loss: 0.545580
[epoch 17, batch   399] avg loss: 0.594548
[epoch 17, batch   499] avg loss: 0.536027
[epoch 17, batch   599] avg loss: 0.537853
[epoch 17, batch   699] avg loss: 0.538094
[epoch 17, batch   799] avg loss: 0.547503
[epoch 17, batch   899] avg loss: 0.575293
[epoch 17, batch   999] avg loss: 0.560957
[epoch 17, batch  1099] avg loss: 0.554854
[epoch 17, batch  1199] avg loss: 0.558747
[epoch 17, batch  1299] avg loss: 0.569534
[epoch 17, batch  1399] avg loss: 0.547531
[epoch 17, batch  1499] avg loss: 0.558916
[epoch 17, batch  1599] avg loss: 0.550249
[epoch 17, batch  1699] avg loss: 0.565774
[epoch 17, batch  1799] avg loss: 0.567098
[epoch 17, batch  1899] avg loss: 0.571672
[epoch 17, batch  1999] avg loss: 0.557144
[epoch 17, batch  2099] avg loss: 0.549994
[epoch 17, batch  2199] avg loss: 0.547315
[epoch 17, batch  2299] avg loss: 0.541305
[epoch 17, batch  2399] avg loss: 0.559385
[epoch 17, batch  2499] avg loss: 0.547457
[epoch 18, batch    99] avg loss: 0.542123
[epoch 18, batch   199] avg loss: 0.565775
[epoch 18, batch   299] avg loss: 0.561254
[epoch 18, batch   399] avg loss: 0.538791
[epoch 18, batch   499] avg loss: 0.532220
[epoch 18, batch   599] avg loss: 0.535606
[epoch 18, batch   699] avg loss: 0.555919
[epoch 18, batch   799] avg loss: 0.556829
[epoch 18, batch   899] avg loss: 0.548397
[epoch 18, batch   999] avg loss: 0.561933
[epoch 18, batch  1099] avg loss: 0.538908
[epoch 18, batch  1199] avg loss: 0.566446
[epoch 18, batch  1299] avg loss: 0.541523
[epoch 18, batch  1399] avg loss: 0.574666
[epoch 18, batch  1499] avg loss: 0.568229
[epoch 18, batch  1599] avg loss: 0.548407
[epoch 18, batch  1699] avg loss: 0.553253
[epoch 18, batch  1799] avg loss: 0.536608
[epoch 18, batch  1899] avg loss: 0.528544
[epoch 18, batch  1999] avg loss: 0.541560
[epoch 18, batch  2099] avg loss: 0.546156
[epoch 18, batch  2199] avg loss: 0.541867
[epoch 18, batch  2299] avg loss: 0.540816
[epoch 18, batch  2399] avg loss: 0.538730
[epoch 18, batch  2499] avg loss: 0.538522
[epoch 19, batch    99] avg loss: 0.577199
[epoch 19, batch   199] avg loss: 0.551837
[epoch 19, batch   299] avg loss: 0.561143
[epoch 19, batch   399] avg loss: 0.557490
[epoch 19, batch   499] avg loss: 0.541580
[epoch 19, batch   599] avg loss: 0.531731
[epoch 19, batch   699] avg loss: 0.525232
[epoch 19, batch   799] avg loss: 0.539879
[epoch 19, batch   899] avg loss: 0.537162
[epoch 19, batch   999] avg loss: 0.594221
[epoch 19, batch  1099] avg loss: 0.568818
[epoch 19, batch  1199] avg loss: 0.550929
[epoch 19, batch  1299] avg loss: 0.540367
[epoch 19, batch  1399] avg loss: 0.563918
[epoch 19, batch  1499] avg loss: 0.545031
[epoch 19, batch  1599] avg loss: 0.552185
[epoch 19, batch  1699] avg loss: 0.537651
[epoch 19, batch  1799] avg loss: 0.548817
[epoch 19, batch  1899] avg loss: 0.541629
[epoch 19, batch  1999] avg loss: 0.540011
[epoch 19, batch  2099] avg loss: 0.557432
[epoch 19, batch  2199] avg loss: 0.555756
[epoch 19, batch  2299] avg loss: 0.532201
[epoch 19, batch  2399] avg loss: 0.532088
[epoch 19, batch  2499] avg loss: 0.549265
Model saved to model/20200502-023922.pth.
accuracy/TriangPrismIsosc : 0.624
n_examples/TriangPrismIsosc : 1500.0
accuracy/parallelepiped : 0.31533333333333335
n_examples/parallelepiped : 1500.0
accuracy/sphere : 0.9934640522875817
n_examples/sphere : 306.0
accuracy/wire : 0.8933333333333333
n_examples/wire : 600.0
accuracy/avg_geom : 0.5757808499743984
loss/validation_geom : 0.8697045311461457
accuracy/Au : 0.22350230414746544
n_examples/Au : 1302.0
accuracy/SiN : 0.052995391705069124
n_examples/SiN : 1302.0
accuracy/SiO2 : 0.6152073732718893
n_examples/SiO2 : 1302.0
accuracy/avg_mat : 0.29723502304147464
loss/validation_mat : 1.251468832896907
MSE/ShortestDim : 5.350798827467731
MAE/ShortestDim : 0.9364613357319077
MSE/MiddleDim : 7.115044725128949
MAE/MiddleDim : 1.7359116792495717
MSE/LongDim : 141.99322305235933
MAE/LongDim : 6.963396821947386
MSE/log Area/Vol : 15.901036469068396
MAE/log Area/Vol : 3.4602152281886664
loss/validation_dim : 170.3601030740244
loss/validation : 172.48127643806745
Metrics saved to model/20200502-023922_metrics.csv.
Parsed 7812 rows from data/sim_train_spectrum_all.
Parsed 7812 rows from data/sim_train_labels_all.
Parsed 9765 rows from data/gen_spectrum_all_00-of-16.
Parsed 9765 rows from data/gen_labels_all_00-of-16.
Parsed 9765 rows from data/gen_spectrum_all_01-of-16.
Parsed 9765 rows from data/gen_labels_all_01-of-16.
Parsed 9765 rows from data/gen_spectrum_all_02-of-16.
Parsed 9765 rows from data/gen_labels_all_02-of-16.
Parsed 9765 rows from data/gen_spectrum_all_03-of-16.
Parsed 9765 rows from data/gen_labels_all_03-of-16.
Parsed 9765 rows from data/gen_spectrum_all_04-of-16.
Parsed 9765 rows from data/gen_labels_all_04-of-16.
Parsed 9765 rows from data/gen_spectrum_all_05-of-16.
Parsed 9765 rows from data/gen_labels_all_05-of-16.
Parsed 9765 rows from data/gen_spectrum_all_06-of-16.
Parsed 9765 rows from data/gen_labels_all_06-of-16.
Parsed 9765 rows from data/gen_spectrum_all_07-of-16.
Parsed 9765 rows from data/gen_labels_all_07-of-16.
Parsed 9765 rows from data/gen_spectrum_all_08-of-16.
Parsed 9765 rows from data/gen_labels_all_08-of-16.
Parsed 9765 rows from data/gen_spectrum_all_09-of-16.
Parsed 9765 rows from data/gen_labels_all_09-of-16.
Parsed 9765 rows from data/gen_spectrum_all_10-of-16.
Parsed 9765 rows from data/gen_labels_all_10-of-16.
Parsed 9765 rows from data/gen_spectrum_all_11-of-16.
Parsed 9765 rows from data/gen_labels_all_11-of-16.
Parsed 9765 rows from data/gen_spectrum_all_12-of-16.
Parsed 9765 rows from data/gen_labels_all_12-of-16.
Parsed 9765 rows from data/gen_spectrum_all_13-of-16.
Parsed 9765 rows from data/gen_labels_all_13-of-16.
Parsed 9765 rows from data/gen_spectrum_all_14-of-16.
Parsed 9765 rows from data/gen_labels_all_14-of-16.
Parsed 9765 rows from data/gen_spectrum_all_15-of-16.
Parsed 9765 rows from data/gen_labels_all_15-of-16.
Parsed 3906 rows from data/sim_validation_spectrum_all.
Parsed 3906 rows from data/sim_validation_labels_all.
Logging training progress to tensorboard dir runs/alexnet-all-lr_0.000500-trainsize_164052-05_02_2020_02:40-joint.
[epoch 0, batch    99] avg loss: 1.330810
[epoch 0, batch   199] avg loss: 1.118242
[epoch 0, batch   299] avg loss: 0.967151
[epoch 0, batch   399] avg loss: 0.932346
[epoch 0, batch   499] avg loss: 0.878491
[epoch 0, batch   599] avg loss: 0.881524
[epoch 0, batch   699] avg loss: 0.861968
[epoch 0, batch   799] avg loss: 0.857245
[epoch 0, batch   899] avg loss: 0.831828
[epoch 0, batch   999] avg loss: 0.810770
[epoch 0, batch  1099] avg loss: 0.815149
[epoch 0, batch  1199] avg loss: 0.817962
[epoch 0, batch  1299] avg loss: 0.810123
[epoch 0, batch  1399] avg loss: 0.786414
[epoch 0, batch  1499] avg loss: 0.781881
[epoch 0, batch  1599] avg loss: 0.809512
[epoch 0, batch  1699] avg loss: 0.768987
[epoch 0, batch  1799] avg loss: 0.752555
[epoch 0, batch  1899] avg loss: 0.751609
[epoch 0, batch  1999] avg loss: 0.769182
[epoch 0, batch  2099] avg loss: 0.747469
[epoch 0, batch  2199] avg loss: 0.749880
[epoch 0, batch  2299] avg loss: 0.751324
[epoch 0, batch  2399] avg loss: 0.754432
[epoch 0, batch  2499] avg loss: 0.740241
[epoch 1, batch    99] avg loss: 0.744558
[epoch 1, batch   199] avg loss: 0.733981
[epoch 1, batch   299] avg loss: 0.720839
[epoch 1, batch   399] avg loss: 0.715606
[epoch 1, batch   499] avg loss: 0.709927
[epoch 1, batch   599] avg loss: 0.720418
[epoch 1, batch   699] avg loss: 0.728760
[epoch 1, batch   799] avg loss: 0.726173
[epoch 1, batch   899] avg loss: 0.713937
[epoch 1, batch   999] avg loss: 0.707374
[epoch 1, batch  1099] avg loss: 0.693095
[epoch 1, batch  1199] avg loss: 0.715860
[epoch 1, batch  1299] avg loss: 0.699115
[epoch 1, batch  1399] avg loss: 0.699443
[epoch 1, batch  1499] avg loss: 0.695558
[epoch 1, batch  1599] avg loss: 0.696355
[epoch 1, batch  1699] avg loss: 0.692173
[epoch 1, batch  1799] avg loss: 0.699081
[epoch 1, batch  1899] avg loss: 0.701078
[epoch 1, batch  1999] avg loss: 0.692040
[epoch 1, batch  2099] avg loss: 0.683988
[epoch 1, batch  2199] avg loss: 0.686620
[epoch 1, batch  2299] avg loss: 0.697015
[epoch 1, batch  2399] avg loss: 0.676795
[epoch 1, batch  2499] avg loss: 0.694180
[epoch 2, batch    99] avg loss: 0.660561
[epoch 2, batch   199] avg loss: 0.689795
[epoch 2, batch   299] avg loss: 0.685117
[epoch 2, batch   399] avg loss: 0.678111
[epoch 2, batch   499] avg loss: 0.685396
[epoch 2, batch   599] avg loss: 0.674843
[epoch 2, batch   699] avg loss: 0.677871
[epoch 2, batch   799] avg loss: 0.667858
[epoch 2, batch   899] avg loss: 0.675740
[epoch 2, batch   999] avg loss: 0.677915
[epoch 2, batch  1099] avg loss: 0.668107
[epoch 2, batch  1199] avg loss: 0.695217
[epoch 2, batch  1299] avg loss: 0.657516
[epoch 2, batch  1399] avg loss: 0.663221
[epoch 2, batch  1499] avg loss: 0.677332
[epoch 2, batch  1599] avg loss: 0.664487
[epoch 2, batch  1699] avg loss: 0.668331
[epoch 2, batch  1799] avg loss: 0.660835
[epoch 2, batch  1899] avg loss: 0.655025
[epoch 2, batch  1999] avg loss: 0.661059
[epoch 2, batch  2099] avg loss: 0.675058
[epoch 2, batch  2199] avg loss: 0.648110
[epoch 2, batch  2299] avg loss: 0.671808
[epoch 2, batch  2399] avg loss: 0.673557
[epoch 2, batch  2499] avg loss: 0.665323
[epoch 3, batch    99] avg loss: 0.652472
[epoch 3, batch   199] avg loss: 0.676334
[epoch 3, batch   299] avg loss: 0.639272
[epoch 3, batch   399] avg loss: 0.668732
[epoch 3, batch   499] avg loss: 0.662418
[epoch 3, batch   599] avg loss: 0.655552
[epoch 3, batch   699] avg loss: 0.662288
[epoch 3, batch   799] avg loss: 0.654009
[epoch 3, batch   899] avg loss: 0.659297
[epoch 3, batch   999] avg loss: 0.652391
[epoch 3, batch  1099] avg loss: 0.651240
[epoch 3, batch  1199] avg loss: 0.652329
[epoch 3, batch  1299] avg loss: 0.644280
[epoch 3, batch  1399] avg loss: 0.655933
[epoch 3, batch  1499] avg loss: 0.669500
[epoch 3, batch  1599] avg loss: 0.651106
[epoch 3, batch  1699] avg loss: 0.645443
[epoch 3, batch  1799] avg loss: 0.656066
[epoch 3, batch  1899] avg loss: 0.650129
[epoch 3, batch  1999] avg loss: 0.646063
[epoch 3, batch  2099] avg loss: 0.657263
[epoch 3, batch  2199] avg loss: 0.646704
[epoch 3, batch  2299] avg loss: 0.652140
[epoch 3, batch  2399] avg loss: 0.643384
[epoch 3, batch  2499] avg loss: 0.644233
[epoch 4, batch    99] avg loss: 0.630774
[epoch 4, batch   199] avg loss: 0.644072
[epoch 4, batch   299] avg loss: 0.635945
[epoch 4, batch   399] avg loss: 0.639457
[epoch 4, batch   499] avg loss: 0.652198
[epoch 4, batch   599] avg loss: 0.645560
[epoch 4, batch   699] avg loss: 0.633039
[epoch 4, batch   799] avg loss: 0.640855
[epoch 4, batch   899] avg loss: 0.629995
[epoch 4, batch   999] avg loss: 0.658825
[epoch 4, batch  1099] avg loss: 0.627307
[epoch 4, batch  1199] avg loss: 0.640625
[epoch 4, batch  1299] avg loss: 0.623711
[epoch 4, batch  1399] avg loss: 0.625841
[epoch 4, batch  1499] avg loss: 0.647636
[epoch 4, batch  1599] avg loss: 0.635243
[epoch 4, batch  1699] avg loss: 0.635848
[epoch 4, batch  1799] avg loss: 0.623045
[epoch 4, batch  1899] avg loss: 0.647654
[epoch 4, batch  1999] avg loss: 0.634068
[epoch 4, batch  2099] avg loss: 0.633974
[epoch 4, batch  2199] avg loss: 0.629151
[epoch 4, batch  2299] avg loss: 0.645685
[epoch 4, batch  2399] avg loss: 0.636525
[epoch 4, batch  2499] avg loss: 0.630967
[epoch 5, batch    99] avg loss: 0.627236
[epoch 5, batch   199] avg loss: 0.633706
[epoch 5, batch   299] avg loss: 0.636528
[epoch 5, batch   399] avg loss: 0.628625
[epoch 5, batch   499] avg loss: 0.629430
[epoch 5, batch   599] avg loss: 0.618485
[epoch 5, batch   699] avg loss: 0.620578
[epoch 5, batch   799] avg loss: 0.637246
[epoch 5, batch   899] avg loss: 0.616800
[epoch 5, batch   999] avg loss: 0.618353
[epoch 5, batch  1099] avg loss: 0.612866
[epoch 5, batch  1199] avg loss: 0.605507
[epoch 5, batch  1299] avg loss: 0.611194
[epoch 5, batch  1399] avg loss: 0.609628
[epoch 5, batch  1499] avg loss: 0.605675
[epoch 5, batch  1599] avg loss: 0.621406
[epoch 5, batch  1699] avg loss: 0.614194
[epoch 5, batch  1799] avg loss: 0.634607
[epoch 5, batch  1899] avg loss: 0.618170
[epoch 5, batch  1999] avg loss: 0.613386
[epoch 5, batch  2099] avg loss: 0.614295
[epoch 5, batch  2199] avg loss: 0.613563
[epoch 5, batch  2299] avg loss: 0.603625
[epoch 5, batch  2399] avg loss: 0.603484
[epoch 5, batch  2499] avg loss: 0.612547
[epoch 6, batch    99] avg loss: 0.618593
[epoch 6, batch   199] avg loss: 0.602768
[epoch 6, batch   299] avg loss: 0.602983
[epoch 6, batch   399] avg loss: 0.600927
[epoch 6, batch   499] avg loss: 0.631856
[epoch 6, batch   599] avg loss: 0.606241
[epoch 6, batch   699] avg loss: 0.618855
[epoch 6, batch   799] avg loss: 0.609819
[epoch 6, batch   899] avg loss: 0.604757
[epoch 6, batch   999] avg loss: 0.594302
[epoch 6, batch  1099] avg loss: 0.588769
[epoch 6, batch  1199] avg loss: 0.583687
[epoch 6, batch  1299] avg loss: 0.623318
[epoch 6, batch  1399] avg loss: 0.601833
[epoch 6, batch  1499] avg loss: 0.613594
[epoch 6, batch  1599] avg loss: 0.607337
[epoch 6, batch  1699] avg loss: 0.598270
[epoch 6, batch  1799] avg loss: 0.600097
[epoch 6, batch  1899] avg loss: 0.621108
[epoch 6, batch  1999] avg loss: 0.608957
[epoch 6, batch  2099] avg loss: 0.614609
[epoch 6, batch  2199] avg loss: 0.609241
[epoch 6, batch  2299] avg loss: 0.597920
[epoch 6, batch  2399] avg loss: 0.589657
[epoch 6, batch  2499] avg loss: 0.602375
[epoch 7, batch    99] avg loss: 0.591496
[epoch 7, batch   199] avg loss: 0.606180
[epoch 7, batch   299] avg loss: 0.611126
[epoch 7, batch   399] avg loss: 0.600804
[epoch 7, batch   499] avg loss: 0.588940
[epoch 7, batch   599] avg loss: 0.594637
[epoch 7, batch   699] avg loss: 0.592258
[epoch 7, batch   799] avg loss: 0.615686
[epoch 7, batch   899] avg loss: 0.593144
[epoch 7, batch   999] avg loss: 0.601914
[epoch 7, batch  1099] avg loss: 0.584838
[epoch 7, batch  1199] avg loss: 0.597337
[epoch 7, batch  1299] avg loss: 0.597530
[epoch 7, batch  1399] avg loss: 0.589206
[epoch 7, batch  1499] avg loss: 0.592685
[epoch 7, batch  1599] avg loss: 0.607471
[epoch 7, batch  1699] avg loss: 0.593924
[epoch 7, batch  1799] avg loss: 0.601165
[epoch 7, batch  1899] avg loss: 0.589860
[epoch 7, batch  1999] avg loss: 0.584705
[epoch 7, batch  2099] avg loss: 0.585800
[epoch 7, batch  2199] avg loss: 0.579620
[epoch 7, batch  2299] avg loss: 0.584360
[epoch 7, batch  2399] avg loss: 0.595672
[epoch 7, batch  2499] avg loss: 0.591295
[epoch 8, batch    99] avg loss: 0.574708
[epoch 8, batch   199] avg loss: 0.602324
[epoch 8, batch   299] avg loss: 0.596006
[epoch 8, batch   399] avg loss: 0.564208
[epoch 8, batch   499] avg loss: 0.577803
[epoch 8, batch   599] avg loss: 0.587169
[epoch 8, batch   699] avg loss: 0.583704
[epoch 8, batch   799] avg loss: 0.596227
[epoch 8, batch   899] avg loss: 0.597041
[epoch 8, batch   999] avg loss: 0.609491
[epoch 8, batch  1099] avg loss: 0.574734
[epoch 8, batch  1199] avg loss: 0.567909
[epoch 8, batch  1299] avg loss: 0.600270
[epoch 8, batch  1399] avg loss: 0.591142
[epoch 8, batch  1499] avg loss: 0.584830
[epoch 8, batch  1599] avg loss: 0.583255
[epoch 8, batch  1699] avg loss: 0.594393
[epoch 8, batch  1799] avg loss: 0.591355
[epoch 8, batch  1899] avg loss: 0.569564
[epoch 8, batch  1999] avg loss: 0.579676
[epoch 8, batch  2099] avg loss: 0.582065
[epoch 8, batch  2199] avg loss: 0.598959
[epoch 8, batch  2299] avg loss: 0.585361
[epoch 8, batch  2399] avg loss: 0.557602
[epoch 8, batch  2499] avg loss: 0.564911
[epoch 9, batch    99] avg loss: 0.574001
[epoch 9, batch   199] avg loss: 0.595557
[epoch 9, batch   299] avg loss: 0.572172
[epoch 9, batch   399] avg loss: 0.568028
[epoch 9, batch   499] avg loss: 0.573509
[epoch 9, batch   599] avg loss: 0.570173
[epoch 9, batch   699] avg loss: 0.590447
[epoch 9, batch   799] avg loss: 0.569066
[epoch 9, batch   899] avg loss: 0.579757
[epoch 9, batch   999] avg loss: 0.578600
[epoch 9, batch  1099] avg loss: 0.592402
[epoch 9, batch  1199] avg loss: 0.571454
[epoch 9, batch  1299] avg loss: 0.574239
[epoch 9, batch  1399] avg loss: 0.567853
[epoch 9, batch  1499] avg loss: 0.577632
[epoch 9, batch  1599] avg loss: 0.586377
[epoch 9, batch  1699] avg loss: 0.597623
[epoch 9, batch  1799] avg loss: 0.576672
[epoch 9, batch  1899] avg loss: 0.565836
[epoch 9, batch  1999] avg loss: 0.588856
[epoch 9, batch  2099] avg loss: 0.564384
[epoch 9, batch  2199] avg loss: 0.580129
[epoch 9, batch  2299] avg loss: 0.555498
[epoch 9, batch  2399] avg loss: 0.581392
[epoch 9, batch  2499] avg loss: 0.583788
[epoch 10, batch    99] avg loss: 0.582228
[epoch 10, batch   199] avg loss: 0.570776
[epoch 10, batch   299] avg loss: 0.576096
[epoch 10, batch   399] avg loss: 0.562721
[epoch 10, batch   499] avg loss: 0.570890
[epoch 10, batch   599] avg loss: 0.632930
[epoch 10, batch   699] avg loss: 0.567662
[epoch 10, batch   799] avg loss: 0.557122
[epoch 10, batch   899] avg loss: 0.581990
[epoch 10, batch   999] avg loss: 0.554683
[epoch 10, batch  1099] avg loss: 0.564104
[epoch 10, batch  1199] avg loss: 0.570102
[epoch 10, batch  1299] avg loss: 0.588717
[epoch 10, batch  1399] avg loss: 0.575537
[epoch 10, batch  1499] avg loss: 0.564283
[epoch 10, batch  1599] avg loss: 0.579368
[epoch 10, batch  1699] avg loss: 0.572618
[epoch 10, batch  1799] avg loss: 0.573979
[epoch 10, batch  1899] avg loss: 0.579093
[epoch 10, batch  1999] avg loss: 0.552007
[epoch 10, batch  2099] avg loss: 0.569550
[epoch 10, batch  2199] avg loss: 0.565778
[epoch 10, batch  2299] avg loss: 0.572220
[epoch 10, batch  2399] avg loss: 0.570595
[epoch 10, batch  2499] avg loss: 0.555064
[epoch 11, batch    99] avg loss: 0.559545
[epoch 11, batch   199] avg loss: 0.565836
[epoch 11, batch   299] avg loss: 0.569115
[epoch 11, batch   399] avg loss: 0.558065
[epoch 11, batch   499] avg loss: 0.546674
[epoch 11, batch   599] avg loss: 0.560661
[epoch 11, batch   699] avg loss: 0.578054
[epoch 11, batch   799] avg loss: 0.568461
[epoch 11, batch   899] avg loss: 0.563092
[epoch 11, batch   999] avg loss: 0.569906
[epoch 11, batch  1099] avg loss: 0.558487
[epoch 11, batch  1199] avg loss: 0.555965
[epoch 11, batch  1299] avg loss: 0.549440
[epoch 11, batch  1399] avg loss: 0.569255
[epoch 11, batch  1499] avg loss: 0.561345
[epoch 11, batch  1599] avg loss: 0.560886
[epoch 11, batch  1699] avg loss: 0.562185
[epoch 11, batch  1799] avg loss: 0.550799
[epoch 11, batch  1899] avg loss: 0.561049
[epoch 11, batch  1999] avg loss: 0.571134
[epoch 11, batch  2099] avg loss: 0.561410
[epoch 11, batch  2199] avg loss: 0.555338
[epoch 11, batch  2299] avg loss: 0.559675
[epoch 11, batch  2399] avg loss: 0.566174
[epoch 11, batch  2499] avg loss: 0.543965
[epoch 12, batch    99] avg loss: 0.554655
[epoch 12, batch   199] avg loss: 0.559351
[epoch 12, batch   299] avg loss: 0.541553
[epoch 12, batch   399] avg loss: 0.550797
[epoch 12, batch   499] avg loss: 0.555159
[epoch 12, batch   599] avg loss: 0.553982
[epoch 12, batch   699] avg loss: 0.548865
[epoch 12, batch   799] avg loss: 0.545847
[epoch 12, batch   899] avg loss: 0.560159
[epoch 12, batch   999] avg loss: 0.563085
[epoch 12, batch  1099] avg loss: 0.571503
[epoch 12, batch  1199] avg loss: 0.551562
[epoch 12, batch  1299] avg loss: 0.560572
[epoch 12, batch  1399] avg loss: 0.559008
[epoch 12, batch  1499] avg loss: 0.573641
[epoch 12, batch  1599] avg loss: 0.551779
[epoch 12, batch  1699] avg loss: 0.560015
[epoch 12, batch  1799] avg loss: 0.554151
[epoch 12, batch  1899] avg loss: 0.551896
[epoch 12, batch  1999] avg loss: 0.566577
[epoch 12, batch  2099] avg loss: 0.550620
[epoch 12, batch  2199] avg loss: 0.548684
[epoch 12, batch  2299] avg loss: 0.568355
[epoch 12, batch  2399] avg loss: 0.539785
[epoch 12, batch  2499] avg loss: 0.554358
[epoch 13, batch    99] avg loss: 0.563270
[epoch 13, batch   199] avg loss: 0.554457
[epoch 13, batch   299] avg loss: 0.545764
[epoch 13, batch   399] avg loss: 0.557396
[epoch 13, batch   499] avg loss: 0.536795
[epoch 13, batch   599] avg loss: 0.553664
[epoch 13, batch   699] avg loss: 0.542510
[epoch 13, batch   799] avg loss: 0.562594
[epoch 13, batch   899] avg loss: 0.549146
[epoch 13, batch   999] avg loss: 0.540326
[epoch 13, batch  1099] avg loss: 0.556977
[epoch 13, batch  1199] avg loss: 0.546143
[epoch 13, batch  1299] avg loss: 0.547379
[epoch 13, batch  1399] avg loss: 0.547908
[epoch 13, batch  1499] avg loss: 0.556405
[epoch 13, batch  1599] avg loss: 0.547095
[epoch 13, batch  1699] avg loss: 0.539388
[epoch 13, batch  1799] avg loss: 0.560284
[epoch 13, batch  1899] avg loss: 0.541334
[epoch 13, batch  1999] avg loss: 0.560890
[epoch 13, batch  2099] avg loss: 0.548565
[epoch 13, batch  2199] avg loss: 0.551333
[epoch 13, batch  2299] avg loss: 0.552404
[epoch 13, batch  2399] avg loss: 0.567655
[epoch 13, batch  2499] avg loss: 0.533692
[epoch 14, batch    99] avg loss: 0.548994
[epoch 14, batch   199] avg loss: 0.520485
[epoch 14, batch   299] avg loss: 0.575837
[epoch 14, batch   399] avg loss: 0.547087
[epoch 14, batch   499] avg loss: 0.549608
[epoch 14, batch   599] avg loss: 0.532006
[epoch 14, batch   699] avg loss: 0.540296
[epoch 14, batch   799] avg loss: 0.535979
[epoch 14, batch   899] avg loss: 0.551120
[epoch 14, batch   999] avg loss: 0.537378
[epoch 14, batch  1099] avg loss: 0.547993
[epoch 14, batch  1199] avg loss: 0.554681
[epoch 14, batch  1299] avg loss: 0.549716
[epoch 14, batch  1399] avg loss: 0.547552
[epoch 14, batch  1499] avg loss: 0.535712
[epoch 14, batch  1599] avg loss: 0.542019
[epoch 14, batch  1699] avg loss: 0.545259
[epoch 14, batch  1799] avg loss: 0.531457
[epoch 14, batch  1899] avg loss: 0.533060
[epoch 14, batch  1999] avg loss: 0.544873
[epoch 14, batch  2099] avg loss: 0.539919
[epoch 14, batch  2199] avg loss: 0.552665
[epoch 14, batch  2299] avg loss: 0.537380
[epoch 14, batch  2399] avg loss: 0.556117
[epoch 14, batch  2499] avg loss: 0.529767
[epoch 15, batch    99] avg loss: 0.541392
[epoch 15, batch   199] avg loss: 0.533718
[epoch 15, batch   299] avg loss: 0.558306
[epoch 15, batch   399] avg loss: 0.534831
[epoch 15, batch   499] avg loss: 0.535154
[epoch 15, batch   599] avg loss: 0.543746
[epoch 15, batch   699] avg loss: 0.550238
[epoch 15, batch   799] avg loss: 0.537773
[epoch 15, batch   899] avg loss: 0.530559
[epoch 15, batch   999] avg loss: 0.543283
[epoch 15, batch  1099] avg loss: 0.518781
[epoch 15, batch  1199] avg loss: 0.538608
[epoch 15, batch  1299] avg loss: 0.533485
[epoch 15, batch  1399] avg loss: 0.527755
[epoch 15, batch  1499] avg loss: 0.532438
[epoch 15, batch  1599] avg loss: 0.536302
[epoch 15, batch  1699] avg loss: 0.526122
[epoch 15, batch  1799] avg loss: 0.522960
[epoch 15, batch  1899] avg loss: 0.545135
[epoch 15, batch  1999] avg loss: 0.547982
[epoch 15, batch  2099] avg loss: 0.543697
[epoch 15, batch  2199] avg loss: 0.523200
[epoch 15, batch  2299] avg loss: 0.536655
[epoch 15, batch  2399] avg loss: 0.531138
[epoch 15, batch  2499] avg loss: 0.532205
[epoch 16, batch    99] avg loss: 0.540360
[epoch 16, batch   199] avg loss: 0.534939
[epoch 16, batch   299] avg loss: 0.531691
[epoch 16, batch   399] avg loss: 0.535155
[epoch 16, batch   499] avg loss: 0.533925
[epoch 16, batch   599] avg loss: 0.524671
[epoch 16, batch   699] avg loss: 0.552420
[epoch 16, batch   799] avg loss: 0.526411
[epoch 16, batch   899] avg loss: 0.539242
[epoch 16, batch   999] avg loss: 0.526259
[epoch 16, batch  1099] avg loss: 0.521345
[epoch 16, batch  1199] avg loss: 0.532995
[epoch 16, batch  1299] avg loss: 0.539658
[epoch 16, batch  1399] avg loss: 0.547343
[epoch 16, batch  1499] avg loss: 0.539519
[epoch 16, batch  1599] avg loss: 0.547328
[epoch 16, batch  1699] avg loss: 0.536475
[epoch 16, batch  1799] avg loss: 0.527493
[epoch 16, batch  1899] avg loss: 0.521353
[epoch 16, batch  1999] avg loss: 0.525691
[epoch 16, batch  2099] avg loss: 0.512409
[epoch 16, batch  2199] avg loss: 0.545416
[epoch 16, batch  2299] avg loss: 0.517260
[epoch 16, batch  2399] avg loss: 0.523224
[epoch 16, batch  2499] avg loss: 0.540347
[epoch 17, batch    99] avg loss: 0.534291
[epoch 17, batch   199] avg loss: 0.527284
[epoch 17, batch   299] avg loss: 0.543964
[epoch 17, batch   399] avg loss: 0.526389
[epoch 17, batch   499] avg loss: 0.508932
[epoch 17, batch   599] avg loss: 0.515888
[epoch 17, batch   699] avg loss: 0.522901
[epoch 17, batch   799] avg loss: 0.547963
[epoch 17, batch   899] avg loss: 0.534094
[epoch 17, batch   999] avg loss: 0.537873
[epoch 17, batch  1099] avg loss: 0.525956
[epoch 17, batch  1199] avg loss: 0.526041
[epoch 17, batch  1299] avg loss: 0.515880
[epoch 17, batch  1399] avg loss: 0.575100
[epoch 17, batch  1499] avg loss: 0.522139
[epoch 17, batch  1599] avg loss: 0.516154
[epoch 17, batch  1699] avg loss: 0.528317
[epoch 17, batch  1799] avg loss: 0.533286
[epoch 17, batch  1899] avg loss: 0.531400
[epoch 17, batch  1999] avg loss: 0.526668
[epoch 17, batch  2099] avg loss: 0.523293
[epoch 17, batch  2199] avg loss: 0.526881
[epoch 17, batch  2299] avg loss: 0.530151
[epoch 17, batch  2399] avg loss: 0.519122
[epoch 17, batch  2499] avg loss: 0.531899
[epoch 18, batch    99] avg loss: 0.512519
[epoch 18, batch   199] avg loss: 0.521894
[epoch 18, batch   299] avg loss: 0.537384
[epoch 18, batch   399] avg loss: 0.529903
[epoch 18, batch   499] avg loss: 0.499555
[epoch 18, batch   599] avg loss: 0.521833
[epoch 18, batch   699] avg loss: 0.528613
[epoch 18, batch   799] avg loss: 0.530960
[epoch 18, batch   899] avg loss: 0.531671
[epoch 18, batch   999] avg loss: 0.524501
[epoch 18, batch  1099] avg loss: 0.554672
[epoch 18, batch  1199] avg loss: 0.538262
[epoch 18, batch  1299] avg loss: 0.521331
[epoch 18, batch  1399] avg loss: 0.518485
[epoch 18, batch  1499] avg loss: 0.523326
[epoch 18, batch  1599] avg loss: 0.512398
[epoch 18, batch  1699] avg loss: 0.533935
[epoch 18, batch  1799] avg loss: 0.520405
[epoch 18, batch  1899] avg loss: 0.517736
[epoch 18, batch  1999] avg loss: 0.517795
[epoch 18, batch  2099] avg loss: 0.537494
[epoch 18, batch  2199] avg loss: 0.542820
[epoch 18, batch  2299] avg loss: 0.516805
[epoch 18, batch  2399] avg loss: 0.527706
[epoch 18, batch  2499] avg loss: 0.523392
[epoch 19, batch    99] avg loss: 0.502309
[epoch 19, batch   199] avg loss: 0.520082
[epoch 19, batch   299] avg loss: 0.512723
[epoch 19, batch   399] avg loss: 0.529457
[epoch 19, batch   499] avg loss: 0.505452
[epoch 19, batch   599] avg loss: 0.514164
[epoch 19, batch   699] avg loss: 0.518229
[epoch 19, batch   799] avg loss: 0.518872
[epoch 19, batch   899] avg loss: 0.510606
[epoch 19, batch   999] avg loss: 0.516561
[epoch 19, batch  1099] avg loss: 0.516521
[epoch 19, batch  1199] avg loss: 0.496761
[epoch 19, batch  1299] avg loss: 0.528102
[epoch 19, batch  1399] avg loss: 0.528443
[epoch 19, batch  1499] avg loss: 0.520960
[epoch 19, batch  1599] avg loss: 0.517079
[epoch 19, batch  1699] avg loss: 0.519965
[epoch 19, batch  1799] avg loss: 0.510686
[epoch 19, batch  1899] avg loss: 0.531198
[epoch 19, batch  1999] avg loss: 0.521665
[epoch 19, batch  2099] avg loss: 0.526241
[epoch 19, batch  2199] avg loss: 0.514831
[epoch 19, batch  2299] avg loss: 0.523996
[epoch 19, batch  2399] avg loss: 0.505408
[epoch 19, batch  2499] avg loss: 0.531829
Model saved to model/20200502-025649.pth.
accuracy/TriangPrismIsosc : 0.6453333333333333
n_examples/TriangPrismIsosc : 1500.0
accuracy/parallelepiped : 0.356
n_examples/parallelepiped : 1500.0
accuracy/sphere : 0.9771241830065359
n_examples/sphere : 306.0
accuracy/wire : 0.895
n_examples/wire : 600.0
accuracy/avg_geom : 0.5985663082437276
loss/validation_geom : 0.832507895434507
accuracy/Au : 0.14669738863287252
n_examples/Au : 1302.0
accuracy/SiN : 0.36482334869431643
n_examples/SiN : 1302.0
accuracy/SiO2 : 0.4377880184331797
n_examples/SiO2 : 1302.0
accuracy/avg_mat : 0.31643625192012287
loss/validation_mat : 1.1906008302280369
MSE/ShortestDim : 1.0492375931081195
MAE/ShortestDim : 0.5870311550158936
MSE/MiddleDim : 5.727896461151712
MAE/MiddleDim : 1.5474331769128977
MSE/LongDim : 147.45895055252285
MAE/LongDim : 7.240729459175622
MSE/log Area/Vol : 9.1681031941513
MAE/log Area/Vol : 2.7650296371897487
loss/validation_dim : 163.40418780093398
loss/validation : 165.42729652659654
Metrics saved to model/20200502-025649_metrics.csv.
Parsed 2604 rows from data/sim_train_spectrum_Au.
Parsed 2604 rows from data/sim_train_labels_Au.
Parsed 9765 rows from data/gen_spectrum_Au_00-of-16.
Parsed 9765 rows from data/gen_labels_Au_00-of-16.
Parsed 9765 rows from data/gen_spectrum_Au_01-of-16.
Parsed 9765 rows from data/gen_labels_Au_01-of-16.
Parsed 9765 rows from data/gen_spectrum_Au_02-of-16.
Parsed 9765 rows from data/gen_labels_Au_02-of-16.
Parsed 9765 rows from data/gen_spectrum_Au_03-of-16.
Parsed 9765 rows from data/gen_labels_Au_03-of-16.
Parsed 9765 rows from data/gen_spectrum_Au_04-of-16.
Parsed 9765 rows from data/gen_labels_Au_04-of-16.
Parsed 9765 rows from data/gen_spectrum_Au_05-of-16.
Parsed 9765 rows from data/gen_labels_Au_05-of-16.
Parsed 9765 rows from data/gen_spectrum_Au_06-of-16.
Parsed 9765 rows from data/gen_labels_Au_06-of-16.
Parsed 9765 rows from data/gen_spectrum_Au_07-of-16.
Parsed 9765 rows from data/gen_labels_Au_07-of-16.
Parsed 9765 rows from data/gen_spectrum_Au_08-of-16.
Parsed 9765 rows from data/gen_labels_Au_08-of-16.
Parsed 9765 rows from data/gen_spectrum_Au_09-of-16.
Parsed 9765 rows from data/gen_labels_Au_09-of-16.
Parsed 9765 rows from data/gen_spectrum_Au_10-of-16.
Parsed 9765 rows from data/gen_labels_Au_10-of-16.
Parsed 9765 rows from data/gen_spectrum_Au_11-of-16.
Parsed 9765 rows from data/gen_labels_Au_11-of-16.
Parsed 9765 rows from data/gen_spectrum_Au_12-of-16.
Parsed 9765 rows from data/gen_labels_Au_12-of-16.
Parsed 9765 rows from data/gen_spectrum_Au_13-of-16.
Parsed 9765 rows from data/gen_labels_Au_13-of-16.
Parsed 9765 rows from data/gen_spectrum_Au_14-of-16.
Parsed 9765 rows from data/gen_labels_Au_14-of-16.
Parsed 9765 rows from data/gen_spectrum_Au_15-of-16.
Parsed 9765 rows from data/gen_labels_Au_15-of-16.
Parsed 1302 rows from data/sim_validation_spectrum_Au.
Parsed 1302 rows from data/sim_validation_labels_Au.
Logging training progress to tensorboard dir runs/alexnet-Au-lr_0.000500-trainsize_158844-05_02_2020_02:57-joint.
[epoch 0, batch    99] avg loss: 1.305323
[epoch 0, batch   199] avg loss: 1.061715
[epoch 0, batch   299] avg loss: 0.980005
[epoch 0, batch   399] avg loss: 0.956729
[epoch 0, batch   499] avg loss: 0.928371
[epoch 0, batch   599] avg loss: 0.912268
[epoch 0, batch   699] avg loss: 0.881833
[epoch 0, batch   799] avg loss: 0.893554
[epoch 0, batch   899] avg loss: 0.902090
[epoch 0, batch   999] avg loss: 0.865411
[epoch 0, batch  1099] avg loss: 0.868727
[epoch 0, batch  1199] avg loss: 0.880536
[epoch 0, batch  1299] avg loss: 0.863446
[epoch 0, batch  1399] avg loss: 0.857409
[epoch 0, batch  1499] avg loss: 0.847703
[epoch 0, batch  1599] avg loss: 0.882449
[epoch 0, batch  1699] avg loss: 0.846632
[epoch 0, batch  1799] avg loss: 0.809197
[epoch 0, batch  1899] avg loss: 0.816157
[epoch 0, batch  1999] avg loss: 0.810261
[epoch 0, batch  2099] avg loss: 0.827907
[epoch 0, batch  2199] avg loss: 0.828504
[epoch 0, batch  2299] avg loss: 0.819127
[epoch 0, batch  2399] avg loss: 0.808037
[epoch 1, batch    99] avg loss: 0.806336
[epoch 1, batch   199] avg loss: 0.829499
[epoch 1, batch   299] avg loss: 0.804820
[epoch 1, batch   399] avg loss: 0.751975
[epoch 1, batch   499] avg loss: 0.771147
[epoch 1, batch   599] avg loss: 0.780365
[epoch 1, batch   699] avg loss: 0.838889
[epoch 1, batch   799] avg loss: 0.788918
[epoch 1, batch   899] avg loss: 0.786150
[epoch 1, batch   999] avg loss: 0.796750
[epoch 1, batch  1099] avg loss: 0.766544
[epoch 1, batch  1199] avg loss: 0.774627
[epoch 1, batch  1299] avg loss: 0.790240
[epoch 1, batch  1399] avg loss: 0.766098
[epoch 1, batch  1499] avg loss: 0.767161
[epoch 1, batch  1599] avg loss: 0.785857
[epoch 1, batch  1699] avg loss: 0.765355
[epoch 1, batch  1799] avg loss: 0.754517
[epoch 1, batch  1899] avg loss: 0.734983
[epoch 1, batch  1999] avg loss: 0.752541
[epoch 1, batch  2099] avg loss: 0.720699
[epoch 1, batch  2199] avg loss: 0.767343
[epoch 1, batch  2299] avg loss: 0.747699
[epoch 1, batch  2399] avg loss: 0.758028
[epoch 2, batch    99] avg loss: 0.729812
[epoch 2, batch   199] avg loss: 0.745133
[epoch 2, batch   299] avg loss: 0.723319
[epoch 2, batch   399] avg loss: 0.741972
[epoch 2, batch   499] avg loss: 0.733167
[epoch 2, batch   599] avg loss: 0.706723
[epoch 2, batch   699] avg loss: 0.758847
[epoch 2, batch   799] avg loss: 0.743376
[epoch 2, batch   899] avg loss: 0.723483
[epoch 2, batch   999] avg loss: 0.728451
[epoch 2, batch  1099] avg loss: 0.708988
[epoch 2, batch  1199] avg loss: 0.721599
[epoch 2, batch  1299] avg loss: 0.749362
[epoch 2, batch  1399] avg loss: 0.715264
[epoch 2, batch  1499] avg loss: 0.707114
[epoch 2, batch  1599] avg loss: 0.690081
[epoch 2, batch  1699] avg loss: 0.697673
[epoch 2, batch  1799] avg loss: 0.693618
[epoch 2, batch  1899] avg loss: 0.707826
[epoch 2, batch  1999] avg loss: 0.713664
[epoch 2, batch  2099] avg loss: 0.695843
[epoch 2, batch  2199] avg loss: 0.688224
[epoch 2, batch  2299] avg loss: 0.696644
[epoch 2, batch  2399] avg loss: 0.698368
[epoch 3, batch    99] avg loss: 0.718209
[epoch 3, batch   199] avg loss: 0.689194
[epoch 3, batch   299] avg loss: 0.704987
[epoch 3, batch   399] avg loss: 0.662234
[epoch 3, batch   499] avg loss: 0.662390
[epoch 3, batch   599] avg loss: 0.687213
[epoch 3, batch   699] avg loss: 0.689546
[epoch 3, batch   799] avg loss: 0.673619
[epoch 3, batch   899] avg loss: 0.661925
[epoch 3, batch   999] avg loss: 0.678362
[epoch 3, batch  1099] avg loss: 0.712275
[epoch 3, batch  1199] avg loss: 0.673173
[epoch 3, batch  1299] avg loss: 0.663649
[epoch 3, batch  1399] avg loss: 0.661270
[epoch 3, batch  1499] avg loss: 0.666034
[epoch 3, batch  1599] avg loss: 0.706709
[epoch 3, batch  1699] avg loss: 0.665112
[epoch 3, batch  1799] avg loss: 0.638049
[epoch 3, batch  1899] avg loss: 0.657199
[epoch 3, batch  1999] avg loss: 0.698308
[epoch 3, batch  2099] avg loss: 0.660188
[epoch 3, batch  2199] avg loss: 0.660144
[epoch 3, batch  2299] avg loss: 0.666668
[epoch 3, batch  2399] avg loss: 0.652187
[epoch 4, batch    99] avg loss: 0.629561
[epoch 4, batch   199] avg loss: 0.642737
[epoch 4, batch   299] avg loss: 0.657301
[epoch 4, batch   399] avg loss: 0.637138
[epoch 4, batch   499] avg loss: 0.652956
[epoch 4, batch   599] avg loss: 0.628515
[epoch 4, batch   699] avg loss: 0.643823
[epoch 4, batch   799] avg loss: 0.637263
[epoch 4, batch   899] avg loss: 0.627657
[epoch 4, batch   999] avg loss: 0.634285
[epoch 4, batch  1099] avg loss: 0.680281
[epoch 4, batch  1199] avg loss: 0.639393
[epoch 4, batch  1299] avg loss: 0.627263
[epoch 4, batch  1399] avg loss: 0.633251
[epoch 4, batch  1499] avg loss: 0.644082
[epoch 4, batch  1599] avg loss: 0.661149
[epoch 4, batch  1699] avg loss: 0.647348
[epoch 4, batch  1799] avg loss: 0.634150
[epoch 4, batch  1899] avg loss: 0.640698
[epoch 4, batch  1999] avg loss: 0.610852
[epoch 4, batch  2099] avg loss: 0.610960
[epoch 4, batch  2199] avg loss: 0.600966
[epoch 4, batch  2299] avg loss: 0.621991
[epoch 4, batch  2399] avg loss: 0.616914
[epoch 5, batch    99] avg loss: 0.658852
[epoch 5, batch   199] avg loss: 0.633496
[epoch 5, batch   299] avg loss: 0.672183
[epoch 5, batch   399] avg loss: 0.613322
[epoch 5, batch   499] avg loss: 0.621545
[epoch 5, batch   599] avg loss: 0.633510
[epoch 5, batch   699] avg loss: 0.613750
[epoch 5, batch   799] avg loss: 0.639438
[epoch 5, batch   899] avg loss: 0.607307
[epoch 5, batch   999] avg loss: 0.613165
[epoch 5, batch  1099] avg loss: 0.611410
[epoch 5, batch  1199] avg loss: 0.624164
[epoch 5, batch  1299] avg loss: 0.624851
[epoch 5, batch  1399] avg loss: 0.607825
[epoch 5, batch  1499] avg loss: 0.605921
[epoch 5, batch  1599] avg loss: 0.641443
[epoch 5, batch  1699] avg loss: 0.600031
[epoch 5, batch  1799] avg loss: 0.588643
[epoch 5, batch  1899] avg loss: 0.600946
[epoch 5, batch  1999] avg loss: 0.610136
[epoch 5, batch  2099] avg loss: 0.592804
[epoch 5, batch  2199] avg loss: 0.639121
[epoch 5, batch  2299] avg loss: 0.580728
[epoch 5, batch  2399] avg loss: 0.599210
[epoch 6, batch    99] avg loss: 0.626019
[epoch 6, batch   199] avg loss: 0.585197
[epoch 6, batch   299] avg loss: 0.588967
[epoch 6, batch   399] avg loss: 0.585756
[epoch 6, batch   499] avg loss: 0.608092
[epoch 6, batch   599] avg loss: 0.606968
[epoch 6, batch   699] avg loss: 0.618038
[epoch 6, batch   799] avg loss: 0.608206
[epoch 6, batch   899] avg loss: 0.597699
[epoch 6, batch   999] avg loss: 0.578901
[epoch 6, batch  1099] avg loss: 0.608208
[epoch 6, batch  1199] avg loss: 0.589674
[epoch 6, batch  1299] avg loss: 0.561209
[epoch 6, batch  1399] avg loss: 0.591692
[epoch 6, batch  1499] avg loss: 0.607153
[epoch 6, batch  1599] avg loss: 0.581519
[epoch 6, batch  1699] avg loss: 0.577892
[epoch 6, batch  1799] avg loss: 0.592583
[epoch 6, batch  1899] avg loss: 0.641536
[epoch 6, batch  1999] avg loss: 0.594513
[epoch 6, batch  2099] avg loss: 0.576857
[epoch 6, batch  2199] avg loss: 0.596322
[epoch 6, batch  2299] avg loss: 0.575789
[epoch 6, batch  2399] avg loss: 0.592543
[epoch 7, batch    99] avg loss: 0.572326
[epoch 7, batch   199] avg loss: 0.560163
[epoch 7, batch   299] avg loss: 0.602181
[epoch 7, batch   399] avg loss: 0.597874
[epoch 7, batch   499] avg loss: 0.575455
[epoch 7, batch   599] avg loss: 0.581540
[epoch 7, batch   699] avg loss: 0.545542
[epoch 7, batch   799] avg loss: 0.605881
[epoch 7, batch   899] avg loss: 0.583167
[epoch 7, batch   999] avg loss: 0.581204
[epoch 7, batch  1099] avg loss: 0.585224
[epoch 7, batch  1199] avg loss: 0.555653
[epoch 7, batch  1299] avg loss: 0.574530
[epoch 7, batch  1399] avg loss: 0.584957
[epoch 7, batch  1499] avg loss: 0.565651
[epoch 7, batch  1599] avg loss: 0.587751
[epoch 7, batch  1699] avg loss: 0.569658
[epoch 7, batch  1799] avg loss: 0.579322
[epoch 7, batch  1899] avg loss: 0.559105
[epoch 7, batch  1999] avg loss: 0.578376
[epoch 7, batch  2099] avg loss: 0.589459
[epoch 7, batch  2199] avg loss: 0.551211
[epoch 7, batch  2299] avg loss: 0.545618
[epoch 7, batch  2399] avg loss: 0.563564
[epoch 8, batch    99] avg loss: 0.594122
[epoch 8, batch   199] avg loss: 0.560240
[epoch 8, batch   299] avg loss: 0.593376
[epoch 8, batch   399] avg loss: 0.572767
[epoch 8, batch   499] avg loss: 0.596173
[epoch 8, batch   599] avg loss: 0.566009
[epoch 8, batch   699] avg loss: 0.552474
[epoch 8, batch   799] avg loss: 0.571821
[epoch 8, batch   899] avg loss: 0.574956
[epoch 8, batch   999] avg loss: 0.556739
[epoch 8, batch  1099] avg loss: 0.579060
[epoch 8, batch  1199] avg loss: 0.562287
[epoch 8, batch  1299] avg loss: 0.577366
[epoch 8, batch  1399] avg loss: 0.570966
[epoch 8, batch  1499] avg loss: 0.559974
[epoch 8, batch  1599] avg loss: 0.558757
[epoch 8, batch  1699] avg loss: 0.564481
[epoch 8, batch  1799] avg loss: 0.559139
[epoch 8, batch  1899] avg loss: 0.551175
[epoch 8, batch  1999] avg loss: 0.532318
[epoch 8, batch  2099] avg loss: 0.551419
[epoch 8, batch  2199] avg loss: 0.577260
[epoch 8, batch  2299] avg loss: 0.539263
[epoch 8, batch  2399] avg loss: 0.571649
[epoch 9, batch    99] avg loss: 0.562223
[epoch 9, batch   199] avg loss: 0.575604
[epoch 9, batch   299] avg loss: 0.556581
[epoch 9, batch   399] avg loss: 0.551731
[epoch 9, batch   499] avg loss: 0.574520
[epoch 9, batch   599] avg loss: 0.573745
[epoch 9, batch   699] avg loss: 0.564088
[epoch 9, batch   799] avg loss: 0.551344
[epoch 9, batch   899] avg loss: 0.565929
[epoch 9, batch   999] avg loss: 0.544133
[epoch 9, batch  1099] avg loss: 0.565780
[epoch 9, batch  1199] avg loss: 0.549040
[epoch 9, batch  1299] avg loss: 0.532092
[epoch 9, batch  1399] avg loss: 0.546405
[epoch 9, batch  1499] avg loss: 0.548010
[epoch 9, batch  1599] avg loss: 0.571343
[epoch 9, batch  1699] avg loss: 0.593719
[epoch 9, batch  1799] avg loss: 0.552978
[epoch 9, batch  1899] avg loss: 0.535567
[epoch 9, batch  1999] avg loss: 0.558329
[epoch 9, batch  2099] avg loss: 0.534908
[epoch 9, batch  2199] avg loss: 0.544997
[epoch 9, batch  2299] avg loss: 0.543347
[epoch 9, batch  2399] avg loss: 0.542357
[epoch 10, batch    99] avg loss: 0.555793
[epoch 10, batch   199] avg loss: 0.558662
[epoch 10, batch   299] avg loss: 0.554233
[epoch 10, batch   399] avg loss: 0.564386
[epoch 10, batch   499] avg loss: 0.529888
[epoch 10, batch   599] avg loss: 0.555821
[epoch 10, batch   699] avg loss: 0.544496
[epoch 10, batch   799] avg loss: 0.518617
[epoch 10, batch   899] avg loss: 0.563330
[epoch 10, batch   999] avg loss: 0.577486
[epoch 10, batch  1099] avg loss: 0.555140
[epoch 10, batch  1199] avg loss: 0.536349
[epoch 10, batch  1299] avg loss: 0.542018
[epoch 10, batch  1399] avg loss: 0.524823
[epoch 10, batch  1499] avg loss: 0.522573
[epoch 10, batch  1599] avg loss: 0.560638
[epoch 10, batch  1699] avg loss: 0.539022
[epoch 10, batch  1799] avg loss: 0.563449
[epoch 10, batch  1899] avg loss: 0.532636
[epoch 10, batch  1999] avg loss: 0.533979
[epoch 10, batch  2099] avg loss: 0.532329
[epoch 10, batch  2199] avg loss: 0.560048
[epoch 10, batch  2299] avg loss: 0.528619
[epoch 10, batch  2399] avg loss: 0.518787
[epoch 11, batch    99] avg loss: 0.555989
[epoch 11, batch   199] avg loss: 0.514215
[epoch 11, batch   299] avg loss: 0.544091
[epoch 11, batch   399] avg loss: 0.543167
[epoch 11, batch   499] avg loss: 0.519738
[epoch 11, batch   599] avg loss: 0.537855
[epoch 11, batch   699] avg loss: 0.533153
[epoch 11, batch   799] avg loss: 0.524003
[epoch 11, batch   899] avg loss: 0.549784
[epoch 11, batch   999] avg loss: 0.518234
[epoch 11, batch  1099] avg loss: 0.536886
[epoch 11, batch  1199] avg loss: 0.511664
[epoch 11, batch  1299] avg loss: 0.512565
[epoch 11, batch  1399] avg loss: 0.527710
[epoch 11, batch  1499] avg loss: 0.513700
[epoch 11, batch  1599] avg loss: 0.532283
[epoch 11, batch  1699] avg loss: 0.521671
[epoch 11, batch  1799] avg loss: 0.572897
[epoch 11, batch  1899] avg loss: 0.531999
[epoch 11, batch  1999] avg loss: 0.529930
[epoch 11, batch  2099] avg loss: 0.501168
[epoch 11, batch  2199] avg loss: 0.540194
[epoch 11, batch  2299] avg loss: 0.549755
[epoch 11, batch  2399] avg loss: 0.530297
[epoch 12, batch    99] avg loss: 0.510641
[epoch 12, batch   199] avg loss: 0.521700
[epoch 12, batch   299] avg loss: 0.537070
[epoch 12, batch   399] avg loss: 0.530028
[epoch 12, batch   499] avg loss: 0.548237
[epoch 12, batch   599] avg loss: 0.511185
[epoch 12, batch   699] avg loss: 0.531625
[epoch 12, batch   799] avg loss: 0.537881
[epoch 12, batch   899] avg loss: 0.510467
[epoch 12, batch   999] avg loss: 0.532359
[epoch 12, batch  1099] avg loss: 0.531489
[epoch 12, batch  1199] avg loss: 0.530273
[epoch 12, batch  1299] avg loss: 0.533231
[epoch 12, batch  1399] avg loss: 0.505565
[epoch 12, batch  1499] avg loss: 0.514099
[epoch 12, batch  1599] avg loss: 0.517932
[epoch 12, batch  1699] avg loss: 0.486458
[epoch 12, batch  1799] avg loss: 0.527717
[epoch 12, batch  1899] avg loss: 0.501350
[epoch 12, batch  1999] avg loss: 0.528863
[epoch 12, batch  2099] avg loss: 0.510201
[epoch 12, batch  2199] avg loss: 0.510538
[epoch 12, batch  2299] avg loss: 0.511585
[epoch 12, batch  2399] avg loss: 0.555401
[epoch 13, batch    99] avg loss: 0.496898
[epoch 13, batch   199] avg loss: 0.512288
[epoch 13, batch   299] avg loss: 0.528490
[epoch 13, batch   399] avg loss: 0.505788
[epoch 13, batch   499] avg loss: 0.537405
[epoch 13, batch   599] avg loss: 0.543759
[epoch 13, batch   699] avg loss: 0.503631
[epoch 13, batch   799] avg loss: 0.512409
[epoch 13, batch   899] avg loss: 0.509259
[epoch 13, batch   999] avg loss: 0.510663
[epoch 13, batch  1099] avg loss: 0.524033
[epoch 13, batch  1199] avg loss: 0.520533
[epoch 13, batch  1299] avg loss: 0.523972
[epoch 13, batch  1399] avg loss: 0.523981
[epoch 13, batch  1499] avg loss: 0.507835
[epoch 13, batch  1599] avg loss: 0.539954
[epoch 13, batch  1699] avg loss: 0.499391
[epoch 13, batch  1799] avg loss: 0.499101
[epoch 13, batch  1899] avg loss: 0.509880
[epoch 13, batch  1999] avg loss: 0.523205
[epoch 13, batch  2099] avg loss: 0.515755
[epoch 13, batch  2199] avg loss: 0.540575
[epoch 13, batch  2299] avg loss: 0.547240
[epoch 13, batch  2399] avg loss: 0.497131
[epoch 14, batch    99] avg loss: 0.505440
[epoch 14, batch   199] avg loss: 0.493076
[epoch 14, batch   299] avg loss: 0.556027
[epoch 14, batch   399] avg loss: 0.524424
[epoch 14, batch   499] avg loss: 0.513639
[epoch 14, batch   599] avg loss: 0.505289
[epoch 14, batch   699] avg loss: 0.536312
[epoch 14, batch   799] avg loss: 0.525590
[epoch 14, batch   899] avg loss: 0.486339
[epoch 14, batch   999] avg loss: 0.526272
[epoch 14, batch  1099] avg loss: 0.501634
[epoch 14, batch  1199] avg loss: 0.511171
[epoch 14, batch  1299] avg loss: 0.496328
[epoch 14, batch  1399] avg loss: 0.502444
[epoch 14, batch  1499] avg loss: 0.504418
[epoch 14, batch  1599] avg loss: 0.496464
[epoch 14, batch  1699] avg loss: 0.500668
[epoch 14, batch  1799] avg loss: 0.501460
[epoch 14, batch  1899] avg loss: 0.546739
[epoch 14, batch  1999] avg loss: 0.524991
[epoch 14, batch  2099] avg loss: 0.496594
[epoch 14, batch  2199] avg loss: 0.530208
[epoch 14, batch  2299] avg loss: 0.493325
[epoch 14, batch  2399] avg loss: 0.508825
[epoch 15, batch    99] avg loss: 0.490904
[epoch 15, batch   199] avg loss: 0.501828
[epoch 15, batch   299] avg loss: 0.525971
[epoch 15, batch   399] avg loss: 0.506364
[epoch 15, batch   499] avg loss: 0.504630
[epoch 15, batch   599] avg loss: 0.481747
[epoch 15, batch   699] avg loss: 0.511819
[epoch 15, batch   799] avg loss: 0.510461
[epoch 15, batch   899] avg loss: 0.506260
[epoch 15, batch   999] avg loss: 0.506093
[epoch 15, batch  1099] avg loss: 0.524212
[epoch 15, batch  1199] avg loss: 0.554679
[epoch 15, batch  1299] avg loss: 0.482264
[epoch 15, batch  1399] avg loss: 0.515592
[epoch 15, batch  1499] avg loss: 0.539510
[epoch 15, batch  1599] avg loss: 0.484673
[epoch 15, batch  1699] avg loss: 0.520525
[epoch 15, batch  1799] avg loss: 0.501652
[epoch 15, batch  1899] avg loss: 0.501189
[epoch 15, batch  1999] avg loss: 0.490552
[epoch 15, batch  2099] avg loss: 0.494421
[epoch 15, batch  2199] avg loss: 0.502085
[epoch 15, batch  2299] avg loss: 0.521737
[epoch 15, batch  2399] avg loss: 0.506302
[epoch 16, batch    99] avg loss: 0.501159
[epoch 16, batch   199] avg loss: 0.504958
[epoch 16, batch   299] avg loss: 0.486792
[epoch 16, batch   399] avg loss: 0.496043
[epoch 16, batch   499] avg loss: 0.493848
[epoch 16, batch   599] avg loss: 0.486553
[epoch 16, batch   699] avg loss: 0.501638
[epoch 16, batch   799] avg loss: 0.509183
[epoch 16, batch   899] avg loss: 0.495253
[epoch 16, batch   999] avg loss: 0.518644
[epoch 16, batch  1099] avg loss: 0.478222
[epoch 16, batch  1199] avg loss: 0.515690
[epoch 16, batch  1299] avg loss: 0.492176
[epoch 16, batch  1399] avg loss: 0.493900
[epoch 16, batch  1499] avg loss: 0.488439
[epoch 16, batch  1599] avg loss: 0.556818
[epoch 16, batch  1699] avg loss: 0.476739
[epoch 16, batch  1799] avg loss: 0.526010
[epoch 16, batch  1899] avg loss: 0.520449
[epoch 16, batch  1999] avg loss: 0.504710
[epoch 16, batch  2099] avg loss: 0.493102
[epoch 16, batch  2199] avg loss: 0.479175
[epoch 16, batch  2299] avg loss: 0.469734
[epoch 16, batch  2399] avg loss: 0.506434
[epoch 17, batch    99] avg loss: 0.513530
[epoch 17, batch   199] avg loss: 0.496040
[epoch 17, batch   299] avg loss: 0.504114
[epoch 17, batch   399] avg loss: 0.554036
[epoch 17, batch   499] avg loss: 0.490333
[epoch 17, batch   599] avg loss: 0.487097
[epoch 17, batch   699] avg loss: 0.473974
[epoch 17, batch   799] avg loss: 0.474023
[epoch 17, batch   899] avg loss: 0.509799
[epoch 17, batch   999] avg loss: 0.498877
[epoch 17, batch  1099] avg loss: 0.493680
[epoch 17, batch  1199] avg loss: 0.480707
[epoch 17, batch  1299] avg loss: 0.522630
[epoch 17, batch  1399] avg loss: 0.480400
[epoch 17, batch  1499] avg loss: 0.476289
[epoch 17, batch  1599] avg loss: 0.472682
[epoch 17, batch  1699] avg loss: 0.489985
[epoch 17, batch  1799] avg loss: 0.464511
[epoch 17, batch  1899] avg loss: 0.485221
[epoch 17, batch  1999] avg loss: 0.477014
[epoch 17, batch  2099] avg loss: 0.485793
[epoch 17, batch  2199] avg loss: 0.475143
[epoch 17, batch  2299] avg loss: 0.513910
[epoch 17, batch  2399] avg loss: 0.481933
[epoch 18, batch    99] avg loss: 0.480435
[epoch 18, batch   199] avg loss: 0.522637
[epoch 18, batch   299] avg loss: 0.490366
[epoch 18, batch   399] avg loss: 0.484346
[epoch 18, batch   499] avg loss: 0.485629
[epoch 18, batch   599] avg loss: 0.487997
[epoch 18, batch   699] avg loss: 0.484913
[epoch 18, batch   799] avg loss: 0.504625
[epoch 18, batch   899] avg loss: 0.492491
[epoch 18, batch   999] avg loss: 0.511571
[epoch 18, batch  1099] avg loss: 0.483557
[epoch 18, batch  1199] avg loss: 0.492530
[epoch 18, batch  1299] avg loss: 0.470269
[epoch 18, batch  1399] avg loss: 0.494992
[epoch 18, batch  1499] avg loss: 0.473383
[epoch 18, batch  1599] avg loss: 0.501475
[epoch 18, batch  1699] avg loss: 0.478572
[epoch 18, batch  1799] avg loss: 0.457531
[epoch 18, batch  1899] avg loss: 0.479971
[epoch 18, batch  1999] avg loss: 0.485578
[epoch 18, batch  2099] avg loss: 0.474725
[epoch 18, batch  2199] avg loss: 0.486959
[epoch 18, batch  2299] avg loss: 0.529226
[epoch 18, batch  2399] avg loss: 0.473670
[epoch 19, batch    99] avg loss: 0.467320
[epoch 19, batch   199] avg loss: 0.480698
[epoch 19, batch   299] avg loss: 0.490399
[epoch 19, batch   399] avg loss: 0.481980
[epoch 19, batch   499] avg loss: 0.462190
[epoch 19, batch   599] avg loss: 0.480222
[epoch 19, batch   699] avg loss: 0.471017
[epoch 19, batch   799] avg loss: 0.503319
[epoch 19, batch   899] avg loss: 0.493825
[epoch 19, batch   999] avg loss: 0.458475
[epoch 19, batch  1099] avg loss: 0.478780
[epoch 19, batch  1199] avg loss: 0.471212
[epoch 19, batch  1299] avg loss: 0.524636
[epoch 19, batch  1399] avg loss: 0.471526
[epoch 19, batch  1499] avg loss: 0.481403
[epoch 19, batch  1599] avg loss: 0.459988
[epoch 19, batch  1699] avg loss: 0.491586
[epoch 19, batch  1799] avg loss: 0.487846
[epoch 19, batch  1899] avg loss: 0.497908
[epoch 19, batch  1999] avg loss: 0.477650
[epoch 19, batch  2099] avg loss: 0.503597
[epoch 19, batch  2199] avg loss: 0.569296
[epoch 19, batch  2299] avg loss: 0.508675
[epoch 19, batch  2399] avg loss: 0.470450
Model saved to model/20200502-031146.pth.
accuracy/TriangPrismIsosc : 0.49
n_examples/TriangPrismIsosc : 500.0
accuracy/parallelepiped : 0.588
n_examples/parallelepiped : 500.0
accuracy/sphere : 1.0
n_examples/sphere : 102.0
accuracy/wire : 0.8
n_examples/wire : 200.0
accuracy/avg_geom : 0.6152073732718893
loss/validation_geom : 0.8114045321849818
accuracy/Au : 0.1228878648233487
n_examples/Au : 1302.0
accuracy/SiN : 0.0
n_examples/SiN : 0.0
accuracy/SiO2 : 0.0
n_examples/SiO2 : 0.0
accuracy/avg_mat : 0.1228878648233487
loss/validation_mat : 1.445558606571133
MSE/ShortestDim : 1.0453820953720725
MAE/ShortestDim : 0.6410681670345653
MSE/MiddleDim : 5.202123451525898
MAE/MiddleDim : 1.5684923503072947
MSE/LongDim : 107.727783953173
MAE/LongDim : 6.331365336287773
MSE/log Area/Vol : 7.957621924521919
MAE/log Area/Vol : 2.571352778491886
loss/validation_dim : 121.93291142459289
loss/validation : 124.18987456334901
Metrics saved to model/20200502-031146_metrics.csv.
Parsed 2604 rows from data/sim_train_spectrum_SiN.
Parsed 2604 rows from data/sim_train_labels_SiN.
Parsed 9765 rows from data/gen_spectrum_SiN_00-of-16.
Parsed 9765 rows from data/gen_labels_SiN_00-of-16.
Parsed 9765 rows from data/gen_spectrum_SiN_01-of-16.
Parsed 9765 rows from data/gen_labels_SiN_01-of-16.
Parsed 9765 rows from data/gen_spectrum_SiN_02-of-16.
Parsed 9765 rows from data/gen_labels_SiN_02-of-16.
Parsed 9765 rows from data/gen_spectrum_SiN_03-of-16.
Parsed 9765 rows from data/gen_labels_SiN_03-of-16.
Parsed 9765 rows from data/gen_spectrum_SiN_04-of-16.
Parsed 9765 rows from data/gen_labels_SiN_04-of-16.
Parsed 9765 rows from data/gen_spectrum_SiN_05-of-16.
Parsed 9765 rows from data/gen_labels_SiN_05-of-16.
Parsed 9765 rows from data/gen_spectrum_SiN_06-of-16.
Parsed 9765 rows from data/gen_labels_SiN_06-of-16.
Parsed 9765 rows from data/gen_spectrum_SiN_07-of-16.
Parsed 9765 rows from data/gen_labels_SiN_07-of-16.
Parsed 9765 rows from data/gen_spectrum_SiN_08-of-16.
Parsed 9765 rows from data/gen_labels_SiN_08-of-16.
Parsed 9765 rows from data/gen_spectrum_SiN_09-of-16.
Parsed 9765 rows from data/gen_labels_SiN_09-of-16.
Parsed 9765 rows from data/gen_spectrum_SiN_10-of-16.
Parsed 9765 rows from data/gen_labels_SiN_10-of-16.
Parsed 9765 rows from data/gen_spectrum_SiN_11-of-16.
Parsed 9765 rows from data/gen_labels_SiN_11-of-16.
Parsed 9765 rows from data/gen_spectrum_SiN_12-of-16.
Parsed 9765 rows from data/gen_labels_SiN_12-of-16.
Parsed 9765 rows from data/gen_spectrum_SiN_13-of-16.
Parsed 9765 rows from data/gen_labels_SiN_13-of-16.
Parsed 9765 rows from data/gen_spectrum_SiN_14-of-16.
Parsed 9765 rows from data/gen_labels_SiN_14-of-16.
Parsed 9765 rows from data/gen_spectrum_SiN_15-of-16.
Parsed 9765 rows from data/gen_labels_SiN_15-of-16.
Parsed 1302 rows from data/sim_validation_spectrum_SiN.
Parsed 1302 rows from data/sim_validation_labels_SiN.
Logging training progress to tensorboard dir runs/alexnet-SiN-lr_0.000500-trainsize_158844-05_02_2020_03:12-joint.
[epoch 0, batch    99] avg loss: 1.360491
[epoch 0, batch   199] avg loss: 1.248494
[epoch 0, batch   299] avg loss: 0.898168
[epoch 0, batch   399] avg loss: 0.850325
[epoch 0, batch   499] avg loss: 0.799144
[epoch 0, batch   599] avg loss: 0.777472
[epoch 0, batch   699] avg loss: 0.754959
[epoch 0, batch   799] avg loss: 0.742136
[epoch 0, batch   899] avg loss: 0.725299
[epoch 0, batch   999] avg loss: 0.708251
[epoch 0, batch  1099] avg loss: 0.727921
[epoch 0, batch  1199] avg loss: 0.706532
[epoch 0, batch  1299] avg loss: 0.681342
[epoch 0, batch  1399] avg loss: 0.702736
[epoch 0, batch  1499] avg loss: 0.674689
[epoch 0, batch  1599] avg loss: 0.650532
[epoch 0, batch  1699] avg loss: 0.656328
[epoch 0, batch  1799] avg loss: 0.667835
[epoch 0, batch  1899] avg loss: 0.646384
[epoch 0, batch  1999] avg loss: 0.647776
[epoch 0, batch  2099] avg loss: 0.645628
[epoch 0, batch  2199] avg loss: 0.627660
[epoch 0, batch  2299] avg loss: 0.626170
[epoch 0, batch  2399] avg loss: 0.620342
[epoch 1, batch    99] avg loss: 0.629173
[epoch 1, batch   199] avg loss: 0.618915
[epoch 1, batch   299] avg loss: 0.600345
[epoch 1, batch   399] avg loss: 0.621902
[epoch 1, batch   499] avg loss: 0.604540
[epoch 1, batch   599] avg loss: 0.597597
[epoch 1, batch   699] avg loss: 0.581667
[epoch 1, batch   799] avg loss: 0.583029
[epoch 1, batch   899] avg loss: 0.593000
[epoch 1, batch   999] avg loss: 0.575749
[epoch 1, batch  1099] avg loss: 0.575796
[epoch 1, batch  1199] avg loss: 0.561837
[epoch 1, batch  1299] avg loss: 0.568563
[epoch 1, batch  1399] avg loss: 0.585433
[epoch 1, batch  1499] avg loss: 0.568367
[epoch 1, batch  1599] avg loss: 0.567812
[epoch 1, batch  1699] avg loss: 0.554793
[epoch 1, batch  1799] avg loss: 0.588926
[epoch 1, batch  1899] avg loss: 0.565918
[epoch 1, batch  1999] avg loss: 0.554446
[epoch 1, batch  2099] avg loss: 0.570274
[epoch 1, batch  2199] avg loss: 0.551278
[epoch 1, batch  2299] avg loss: 0.564336
[epoch 1, batch  2399] avg loss: 0.556123
[epoch 2, batch    99] avg loss: 0.526611
[epoch 2, batch   199] avg loss: 0.547715
[epoch 2, batch   299] avg loss: 0.551338
[epoch 2, batch   399] avg loss: 0.550508
[epoch 2, batch   499] avg loss: 0.571360
[epoch 2, batch   599] avg loss: 0.544129
[epoch 2, batch   699] avg loss: 0.540424
[epoch 2, batch   799] avg loss: 0.536064
[epoch 2, batch   899] avg loss: 0.530659
[epoch 2, batch   999] avg loss: 0.526412
[epoch 2, batch  1099] avg loss: 0.528577
[epoch 2, batch  1199] avg loss: 0.548685
[epoch 2, batch  1299] avg loss: 0.521027
[epoch 2, batch  1399] avg loss: 0.524650
[epoch 2, batch  1499] avg loss: 0.517928
[epoch 2, batch  1599] avg loss: 0.524448
[epoch 2, batch  1699] avg loss: 0.520159
[epoch 2, batch  1799] avg loss: 0.504831
[epoch 2, batch  1899] avg loss: 0.542534
[epoch 2, batch  1999] avg loss: 0.520101
[epoch 2, batch  2099] avg loss: 0.527540
[epoch 2, batch  2199] avg loss: 0.516006
[epoch 2, batch  2299] avg loss: 0.513369
[epoch 2, batch  2399] avg loss: 0.525809
[epoch 3, batch    99] avg loss: 0.499850
[epoch 3, batch   199] avg loss: 0.528090
[epoch 3, batch   299] avg loss: 0.502705
[epoch 3, batch   399] avg loss: 0.521437
[epoch 3, batch   499] avg loss: 0.531644
[epoch 3, batch   599] avg loss: 0.515070
[epoch 3, batch   699] avg loss: 0.530322
[epoch 3, batch   799] avg loss: 0.514484
[epoch 3, batch   899] avg loss: 0.521190
[epoch 3, batch   999] avg loss: 0.524520
[epoch 3, batch  1099] avg loss: 0.506156
[epoch 3, batch  1199] avg loss: 0.533795
[epoch 3, batch  1299] avg loss: 0.508638
[epoch 3, batch  1399] avg loss: 0.499761
[epoch 3, batch  1499] avg loss: 0.512219
[epoch 3, batch  1599] avg loss: 0.504932
[epoch 3, batch  1699] avg loss: 0.510881
[epoch 3, batch  1799] avg loss: 0.528627
[epoch 3, batch  1899] avg loss: 0.506040
[epoch 3, batch  1999] avg loss: 0.497051
[epoch 3, batch  2099] avg loss: 0.525429
[epoch 3, batch  2199] avg loss: 0.499948
[epoch 3, batch  2299] avg loss: 0.525117
[epoch 3, batch  2399] avg loss: 0.514094
[epoch 4, batch    99] avg loss: 0.527184
[epoch 4, batch   199] avg loss: 0.512407
[epoch 4, batch   299] avg loss: 0.504243
[epoch 4, batch   399] avg loss: 0.492353
[epoch 4, batch   499] avg loss: 0.506191
[epoch 4, batch   599] avg loss: 0.508964
[epoch 4, batch   699] avg loss: 0.512458
[epoch 4, batch   799] avg loss: 0.490040
[epoch 4, batch   899] avg loss: 0.503724
[epoch 4, batch   999] avg loss: 0.514583
[epoch 4, batch  1099] avg loss: 0.500448
[epoch 4, batch  1199] avg loss: 0.511249
[epoch 4, batch  1299] avg loss: 0.494462
[epoch 4, batch  1399] avg loss: 0.492751
[epoch 4, batch  1499] avg loss: 0.493547
[epoch 4, batch  1599] avg loss: 0.507304
[epoch 4, batch  1699] avg loss: 0.478590
[epoch 4, batch  1799] avg loss: 0.510465
[epoch 4, batch  1899] avg loss: 0.491502
[epoch 4, batch  1999] avg loss: 0.503845
[epoch 4, batch  2099] avg loss: 0.487722
[epoch 4, batch  2199] avg loss: 0.478875
[epoch 4, batch  2299] avg loss: 0.493699
[epoch 4, batch  2399] avg loss: 0.499522
[epoch 5, batch    99] avg loss: 0.492465
[epoch 5, batch   199] avg loss: 0.518128
[epoch 5, batch   299] avg loss: 0.500729
[epoch 5, batch   399] avg loss: 0.501767
[epoch 5, batch   499] avg loss: 0.486823
[epoch 5, batch   599] avg loss: 0.492419
[epoch 5, batch   699] avg loss: 0.484670
[epoch 5, batch   799] avg loss: 0.482043
[epoch 5, batch   899] avg loss: 0.493026
[epoch 5, batch   999] avg loss: 0.504565
[epoch 5, batch  1099] avg loss: 0.483057
[epoch 5, batch  1199] avg loss: 0.494315
[epoch 5, batch  1299] avg loss: 0.501519
[epoch 5, batch  1399] avg loss: 0.489946
[epoch 5, batch  1499] avg loss: 0.488514
[epoch 5, batch  1599] avg loss: 0.478544
[epoch 5, batch  1699] avg loss: 0.497579
[epoch 5, batch  1799] avg loss: 0.477929
[epoch 5, batch  1899] avg loss: 0.487785
[epoch 5, batch  1999] avg loss: 0.491024
[epoch 5, batch  2099] avg loss: 0.493656
[epoch 5, batch  2199] avg loss: 0.487672
[epoch 5, batch  2299] avg loss: 0.487344
[epoch 5, batch  2399] avg loss: 0.474545
[epoch 6, batch    99] avg loss: 0.504831
[epoch 6, batch   199] avg loss: 0.480302
[epoch 6, batch   299] avg loss: 0.479042
[epoch 6, batch   399] avg loss: 0.496261
[epoch 6, batch   499] avg loss: 0.501960
[epoch 6, batch   599] avg loss: 0.483961
[epoch 6, batch   699] avg loss: 0.484663
[epoch 6, batch   799] avg loss: 0.489743
[epoch 6, batch   899] avg loss: 0.466229
[epoch 6, batch   999] avg loss: 0.477388
[epoch 6, batch  1099] avg loss: 0.486645
[epoch 6, batch  1199] avg loss: 0.467901
[epoch 6, batch  1299] avg loss: 0.485389
[epoch 6, batch  1399] avg loss: 0.485459
[epoch 6, batch  1499] avg loss: 0.467956
[epoch 6, batch  1599] avg loss: 0.476376
[epoch 6, batch  1699] avg loss: 0.491618
[epoch 6, batch  1799] avg loss: 0.478983
[epoch 6, batch  1899] avg loss: 0.477078
[epoch 6, batch  1999] avg loss: 0.474356
[epoch 6, batch  2099] avg loss: 0.484655
[epoch 6, batch  2199] avg loss: 0.471110
[epoch 6, batch  2299] avg loss: 0.482786
[epoch 6, batch  2399] avg loss: 0.469828
[epoch 7, batch    99] avg loss: 0.508084
[epoch 7, batch   199] avg loss: 0.464765
[epoch 7, batch   299] avg loss: 0.481318
[epoch 7, batch   399] avg loss: 0.486799
[epoch 7, batch   499] avg loss: 0.493813
[epoch 7, batch   599] avg loss: 0.466895
[epoch 7, batch   699] avg loss: 0.468166
[epoch 7, batch   799] avg loss: 0.464104
[epoch 7, batch   899] avg loss: 0.490941
[epoch 7, batch   999] avg loss: 0.467807
[epoch 7, batch  1099] avg loss: 0.463653
[epoch 7, batch  1199] avg loss: 0.487212
[epoch 7, batch  1299] avg loss: 0.479854
[epoch 7, batch  1399] avg loss: 0.469504
[epoch 7, batch  1499] avg loss: 0.469644
[epoch 7, batch  1599] avg loss: 0.478606
[epoch 7, batch  1699] avg loss: 0.466079
[epoch 7, batch  1799] avg loss: 0.487080
[epoch 7, batch  1899] avg loss: 0.456873
[epoch 7, batch  1999] avg loss: 0.481404
[epoch 7, batch  2099] avg loss: 0.461819
[epoch 7, batch  2199] avg loss: 0.468202
[epoch 7, batch  2299] avg loss: 0.472723
[epoch 7, batch  2399] avg loss: 0.474222
[epoch 8, batch    99] avg loss: 0.464670
[epoch 8, batch   199] avg loss: 0.474118
[epoch 8, batch   299] avg loss: 0.466623
[epoch 8, batch   399] avg loss: 0.462859
[epoch 8, batch   499] avg loss: 0.463738
[epoch 8, batch   599] avg loss: 0.465538
[epoch 8, batch   699] avg loss: 0.460841
[epoch 8, batch   799] avg loss: 0.467038
[epoch 8, batch   899] avg loss: 0.474321
[epoch 8, batch   999] avg loss: 0.473366
[epoch 8, batch  1099] avg loss: 0.481270
[epoch 8, batch  1199] avg loss: 0.459445
[epoch 8, batch  1299] avg loss: 0.462976
[epoch 8, batch  1399] avg loss: 0.475733
[epoch 8, batch  1499] avg loss: 0.471377
[epoch 8, batch  1599] avg loss: 0.471443
[epoch 8, batch  1699] avg loss: 0.460884
[epoch 8, batch  1799] avg loss: 0.469964
[epoch 8, batch  1899] avg loss: 0.475918
[epoch 8, batch  1999] avg loss: 0.457550
[epoch 8, batch  2099] avg loss: 0.478487
[epoch 8, batch  2199] avg loss: 0.478535
[epoch 8, batch  2299] avg loss: 0.477025
[epoch 8, batch  2399] avg loss: 0.461448
[epoch 9, batch    99] avg loss: 0.456965
[epoch 9, batch   199] avg loss: 0.484020
[epoch 9, batch   299] avg loss: 0.471718
[epoch 9, batch   399] avg loss: 0.462358
[epoch 9, batch   499] avg loss: 0.457497
[epoch 9, batch   599] avg loss: 0.464365
[epoch 9, batch   699] avg loss: 0.461812
[epoch 9, batch   799] avg loss: 0.459071
[epoch 9, batch   899] avg loss: 0.456562
[epoch 9, batch   999] avg loss: 0.460056
[epoch 9, batch  1099] avg loss: 0.450367
[epoch 9, batch  1199] avg loss: 0.474763
[epoch 9, batch  1299] avg loss: 0.462510
[epoch 9, batch  1399] avg loss: 0.462606
[epoch 9, batch  1499] avg loss: 0.460917
[epoch 9, batch  1599] avg loss: 0.506615
[epoch 9, batch  1699] avg loss: 0.479013
[epoch 9, batch  1799] avg loss: 0.456767
[epoch 9, batch  1899] avg loss: 0.461390
[epoch 9, batch  1999] avg loss: 0.465474
[epoch 9, batch  2099] avg loss: 0.453119
[epoch 9, batch  2199] avg loss: 0.464034
[epoch 9, batch  2299] avg loss: 0.455344
[epoch 9, batch  2399] avg loss: 0.452378
[epoch 10, batch    99] avg loss: 0.460418
[epoch 10, batch   199] avg loss: 0.464692
[epoch 10, batch   299] avg loss: 0.452938
[epoch 10, batch   399] avg loss: 0.451301
[epoch 10, batch   499] avg loss: 0.471689
[epoch 10, batch   599] avg loss: 0.460982
[epoch 10, batch   699] avg loss: 0.450999
[epoch 10, batch   799] avg loss: 0.453439
[epoch 10, batch   899] avg loss: 0.457308
[epoch 10, batch   999] avg loss: 0.446012
[epoch 10, batch  1099] avg loss: 0.452382
[epoch 10, batch  1199] avg loss: 0.441847
[epoch 10, batch  1299] avg loss: 0.458094
[epoch 10, batch  1399] avg loss: 0.467233
[epoch 10, batch  1499] avg loss: 0.453426
[epoch 10, batch  1599] avg loss: 0.469636
[epoch 10, batch  1699] avg loss: 0.460084
[epoch 10, batch  1799] avg loss: 0.454811
[epoch 10, batch  1899] avg loss: 0.481094
[epoch 10, batch  1999] avg loss: 0.449659
[epoch 10, batch  2099] avg loss: 0.460653
[epoch 10, batch  2199] avg loss: 0.437034
[epoch 10, batch  2299] avg loss: 0.457228
[epoch 10, batch  2399] avg loss: 0.455066
[epoch 11, batch    99] avg loss: 0.448326
[epoch 11, batch   199] avg loss: 0.446977
[epoch 11, batch   299] avg loss: 0.446565
[epoch 11, batch   399] avg loss: 0.455194
[epoch 11, batch   499] avg loss: 0.470028
[epoch 11, batch   599] avg loss: 0.452167
[epoch 11, batch   699] avg loss: 0.430995
[epoch 11, batch   799] avg loss: 0.453065
[epoch 11, batch   899] avg loss: 0.450681
[epoch 11, batch   999] avg loss: 0.447817
[epoch 11, batch  1099] avg loss: 0.457752
[epoch 11, batch  1199] avg loss: 0.449743
[epoch 11, batch  1299] avg loss: 0.449865
[epoch 11, batch  1399] avg loss: 0.447775
[epoch 11, batch  1499] avg loss: 0.450340
[epoch 11, batch  1599] avg loss: 0.449314
[epoch 11, batch  1699] avg loss: 0.456881
[epoch 11, batch  1799] avg loss: 0.450961
[epoch 11, batch  1899] avg loss: 0.465092
[epoch 11, batch  1999] avg loss: 0.455003
[epoch 11, batch  2099] avg loss: 0.463112
[epoch 11, batch  2199] avg loss: 0.448861
[epoch 11, batch  2299] avg loss: 0.473203
[epoch 11, batch  2399] avg loss: 0.438105
[epoch 12, batch    99] avg loss: 0.430782
[epoch 12, batch   199] avg loss: 0.446830
[epoch 12, batch   299] avg loss: 0.448750
[epoch 12, batch   399] avg loss: 0.448811
[epoch 12, batch   499] avg loss: 0.452442
[epoch 12, batch   599] avg loss: 0.449437
[epoch 12, batch   699] avg loss: 0.449311
[epoch 12, batch   799] avg loss: 0.470627
[epoch 12, batch   899] avg loss: 0.460955
[epoch 12, batch   999] avg loss: 0.440437
[epoch 12, batch  1099] avg loss: 0.454378
[epoch 12, batch  1199] avg loss: 0.497938
[epoch 12, batch  1299] avg loss: 0.474217
[epoch 12, batch  1399] avg loss: 0.465422
[epoch 12, batch  1499] avg loss: 0.449771
[epoch 12, batch  1599] avg loss: 0.439170
[epoch 12, batch  1699] avg loss: 0.444240
[epoch 12, batch  1799] avg loss: 0.453574
[epoch 12, batch  1899] avg loss: 0.453436
[epoch 12, batch  1999] avg loss: 0.442822
[epoch 12, batch  2099] avg loss: 0.447245
[epoch 12, batch  2199] avg loss: 0.446085
[epoch 12, batch  2299] avg loss: 0.429665
[epoch 12, batch  2399] avg loss: 0.438417
[epoch 13, batch    99] avg loss: 0.441554
[epoch 13, batch   199] avg loss: 0.469515
[epoch 13, batch   299] avg loss: 0.440698
[epoch 13, batch   399] avg loss: 0.435188
[epoch 13, batch   499] avg loss: 0.435271
[epoch 13, batch   599] avg loss: 0.435476
[epoch 13, batch   699] avg loss: 0.428914
[epoch 13, batch   799] avg loss: 0.439273
[epoch 13, batch   899] avg loss: 0.443349
[epoch 13, batch   999] avg loss: 0.450601
[epoch 13, batch  1099] avg loss: 0.425627
[epoch 13, batch  1199] avg loss: 0.435664
[epoch 13, batch  1299] avg loss: 0.441450
[epoch 13, batch  1399] avg loss: 0.441820
[epoch 13, batch  1499] avg loss: 0.447723
[epoch 13, batch  1599] avg loss: 0.457531
[epoch 13, batch  1699] avg loss: 0.449274
[epoch 13, batch  1799] avg loss: 0.460753
[epoch 13, batch  1899] avg loss: 0.439615
[epoch 13, batch  1999] avg loss: 0.437049
[epoch 13, batch  2099] avg loss: 0.439181
[epoch 13, batch  2199] avg loss: 0.449094
[epoch 13, batch  2299] avg loss: 0.463210
[epoch 13, batch  2399] avg loss: 0.444702
[epoch 14, batch    99] avg loss: 0.449533
[epoch 14, batch   199] avg loss: 0.448597
[epoch 14, batch   299] avg loss: 0.421641
[epoch 14, batch   399] avg loss: 0.431949
[epoch 14, batch   499] avg loss: 0.438201
[epoch 14, batch   599] avg loss: 0.463691
[epoch 14, batch   699] avg loss: 0.434299
[epoch 14, batch   799] avg loss: 0.431841
[epoch 14, batch   899] avg loss: 0.458958
[epoch 14, batch   999] avg loss: 0.441432
[epoch 14, batch  1099] avg loss: 0.436711
[epoch 14, batch  1199] avg loss: 0.448976
[epoch 14, batch  1299] avg loss: 0.432484
[epoch 14, batch  1399] avg loss: 0.430052
[epoch 14, batch  1499] avg loss: 0.424239
[epoch 14, batch  1599] avg loss: 0.430555
[epoch 14, batch  1699] avg loss: 0.450485
[epoch 14, batch  1799] avg loss: 0.442229
[epoch 14, batch  1899] avg loss: 0.445199
[epoch 14, batch  1999] avg loss: 0.433272
[epoch 14, batch  2099] avg loss: 0.439090
[epoch 14, batch  2199] avg loss: 0.435465
[epoch 14, batch  2299] avg loss: 0.440224
[epoch 14, batch  2399] avg loss: 0.439186
[epoch 15, batch    99] avg loss: 0.425032
[epoch 15, batch   199] avg loss: 0.431226
[epoch 15, batch   299] avg loss: 0.429105
[epoch 15, batch   399] avg loss: 0.433827
[epoch 15, batch   499] avg loss: 0.432638
[epoch 15, batch   599] avg loss: 0.427431
[epoch 15, batch   699] avg loss: 0.430046
[epoch 15, batch   799] avg loss: 0.425962
[epoch 15, batch   899] avg loss: 0.425152
[epoch 15, batch   999] avg loss: 0.450990
[epoch 15, batch  1099] avg loss: 0.438621
[epoch 15, batch  1199] avg loss: 0.438975
[epoch 15, batch  1299] avg loss: 0.441006
[epoch 15, batch  1399] avg loss: 0.431802
[epoch 15, batch  1499] avg loss: 0.421870
[epoch 15, batch  1599] avg loss: 0.452669
[epoch 15, batch  1699] avg loss: 0.436004
[epoch 15, batch  1799] avg loss: 0.433497
[epoch 15, batch  1899] avg loss: 0.442071
[epoch 15, batch  1999] avg loss: 0.421438
[epoch 15, batch  2099] avg loss: 0.443907
[epoch 15, batch  2199] avg loss: 0.448078
[epoch 15, batch  2299] avg loss: 0.433376
[epoch 15, batch  2399] avg loss: 0.421472
[epoch 16, batch    99] avg loss: 0.431350
[epoch 16, batch   199] avg loss: 0.430744
[epoch 16, batch   299] avg loss: 0.437570
[epoch 16, batch   399] avg loss: 0.429693
[epoch 16, batch   499] avg loss: 0.443677
[epoch 16, batch   599] avg loss: 0.438528
[epoch 16, batch   699] avg loss: 0.436686
[epoch 16, batch   799] avg loss: 0.453020
[epoch 16, batch   899] avg loss: 0.430633
[epoch 16, batch   999] avg loss: 0.419116
[epoch 16, batch  1099] avg loss: 0.437117
[epoch 16, batch  1199] avg loss: 0.422118
[epoch 16, batch  1299] avg loss: 0.439244
[epoch 16, batch  1399] avg loss: 0.442383
[epoch 16, batch  1499] avg loss: 0.426591
[epoch 16, batch  1599] avg loss: 0.445230
[epoch 16, batch  1699] avg loss: 0.437292
[epoch 16, batch  1799] avg loss: 0.429577
[epoch 16, batch  1899] avg loss: 0.426771
[epoch 16, batch  1999] avg loss: 0.440032
[epoch 16, batch  2099] avg loss: 0.429601
[epoch 16, batch  2199] avg loss: 0.431572
[epoch 16, batch  2299] avg loss: 0.425270
[epoch 16, batch  2399] avg loss: 0.424698
[epoch 17, batch    99] avg loss: 0.421085
[epoch 17, batch   199] avg loss: 0.426420
[epoch 17, batch   299] avg loss: 0.457541
[epoch 17, batch   399] avg loss: 0.423937
[epoch 17, batch   499] avg loss: 0.508234
[epoch 17, batch   599] avg loss: 0.464799
[epoch 17, batch   699] avg loss: 0.433858
[epoch 17, batch   799] avg loss: 0.422241
[epoch 17, batch   899] avg loss: 0.430610
[epoch 17, batch   999] avg loss: 0.435913
[epoch 17, batch  1099] avg loss: 0.429554
[epoch 17, batch  1199] avg loss: 0.420989
[epoch 17, batch  1299] avg loss: 0.427533
[epoch 17, batch  1399] avg loss: 0.429226
[epoch 17, batch  1499] avg loss: 0.429440
[epoch 17, batch  1599] avg loss: 0.431789
[epoch 17, batch  1699] avg loss: 0.425513
[epoch 17, batch  1799] avg loss: 0.435460
[epoch 17, batch  1899] avg loss: 0.425570
[epoch 17, batch  1999] avg loss: 0.420088
[epoch 17, batch  2099] avg loss: 0.437899
[epoch 17, batch  2199] avg loss: 0.410543
[epoch 17, batch  2299] avg loss: 0.429057
[epoch 17, batch  2399] avg loss: 0.413508
[epoch 18, batch    99] avg loss: 0.429382
[epoch 18, batch   199] avg loss: 0.431325
[epoch 18, batch   299] avg loss: 0.421912
[epoch 18, batch   399] avg loss: 0.428408
[epoch 18, batch   499] avg loss: 0.450147
[epoch 18, batch   599] avg loss: 0.427705
[epoch 18, batch   699] avg loss: 0.420611
[epoch 18, batch   799] avg loss: 0.421264
[epoch 18, batch   899] avg loss: 0.448516
[epoch 18, batch   999] avg loss: 0.401401
[epoch 18, batch  1099] avg loss: 0.419865
[epoch 18, batch  1199] avg loss: 0.419329
[epoch 18, batch  1299] avg loss: 0.419565
[epoch 18, batch  1399] avg loss: 0.434619
[epoch 18, batch  1499] avg loss: 0.419600
[epoch 18, batch  1599] avg loss: 0.410798
[epoch 18, batch  1699] avg loss: 0.413949
[epoch 18, batch  1799] avg loss: 0.422417
[epoch 18, batch  1899] avg loss: 0.412621
[epoch 18, batch  1999] avg loss: 0.422819
[epoch 18, batch  2099] avg loss: 0.428988
[epoch 18, batch  2199] avg loss: 0.412921
[epoch 18, batch  2299] avg loss: 0.446920
[epoch 18, batch  2399] avg loss: 0.422908
[epoch 19, batch    99] avg loss: 0.411821
[epoch 19, batch   199] avg loss: 0.420627
[epoch 19, batch   299] avg loss: 0.439625
[epoch 19, batch   399] avg loss: 0.420844
[epoch 19, batch   499] avg loss: 0.416908
[epoch 19, batch   599] avg loss: 0.411296
[epoch 19, batch   699] avg loss: 0.427254
[epoch 19, batch   799] avg loss: 0.420436
[epoch 19, batch   899] avg loss: 0.407320
[epoch 19, batch   999] avg loss: 0.416944
[epoch 19, batch  1099] avg loss: 0.410422
[epoch 19, batch  1199] avg loss: 0.427828
[epoch 19, batch  1299] avg loss: 0.434623
[epoch 19, batch  1399] avg loss: 0.450866
[epoch 19, batch  1499] avg loss: 0.409648
[epoch 19, batch  1599] avg loss: 0.423987
[epoch 19, batch  1699] avg loss: 0.417509
[epoch 19, batch  1799] avg loss: 0.414254
[epoch 19, batch  1899] avg loss: 0.423283
[epoch 19, batch  1999] avg loss: 0.419948
[epoch 19, batch  2099] avg loss: 0.414270
[epoch 19, batch  2199] avg loss: 0.412109
[epoch 19, batch  2299] avg loss: 0.423402
[epoch 19, batch  2399] avg loss: 0.415069
Model saved to model/20200502-032834.pth.
accuracy/TriangPrismIsosc : 0.74
n_examples/TriangPrismIsosc : 500.0
accuracy/parallelepiped : 0.412
n_examples/parallelepiped : 500.0
accuracy/sphere : 1.0
n_examples/sphere : 102.0
accuracy/wire : 0.985
n_examples/wire : 200.0
accuracy/avg_geom : 0.6720430107526881
loss/validation_geom : 0.7158310607464815
accuracy/Au : 0.0
n_examples/Au : 0.0
accuracy/SiN : 0.5384024577572964
n_examples/SiN : 1302.0
accuracy/SiO2 : 0.0
n_examples/SiO2 : 0.0
accuracy/avg_mat : 0.5384024577572964
loss/validation_mat : 1.3836385680050711
MSE/ShortestDim : 5.267190680525819
MAE/ShortestDim : 0.7865733704808671
MSE/MiddleDim : 6.932594322755406
MAE/MiddleDim : 1.6175567199191374
MSE/LongDim : 194.56842219152026
MAE/LongDim : 8.748873228667884
MSE/log Area/Vol : 11.278088313277049
MAE/log Area/Vol : 2.7277878850652693
loss/validation_dim : 218.04629550807852
loss/validation : 220.14576513683008
Metrics saved to model/20200502-032834_metrics.csv.
Parsed 2604 rows from data/sim_train_spectrum_SiO2.
Parsed 2604 rows from data/sim_train_labels_SiO2.
Parsed 9765 rows from data/gen_spectrum_SiO2_00-of-16.
Parsed 9765 rows from data/gen_labels_SiO2_00-of-16.
Parsed 9765 rows from data/gen_spectrum_SiO2_01-of-16.
Parsed 9765 rows from data/gen_labels_SiO2_01-of-16.
Parsed 9765 rows from data/gen_spectrum_SiO2_02-of-16.
Parsed 9765 rows from data/gen_labels_SiO2_02-of-16.
Parsed 9765 rows from data/gen_spectrum_SiO2_03-of-16.
Parsed 9765 rows from data/gen_labels_SiO2_03-of-16.
Parsed 9765 rows from data/gen_spectrum_SiO2_04-of-16.
Parsed 9765 rows from data/gen_labels_SiO2_04-of-16.
Parsed 9765 rows from data/gen_spectrum_SiO2_05-of-16.
Parsed 9765 rows from data/gen_labels_SiO2_05-of-16.
Parsed 9765 rows from data/gen_spectrum_SiO2_06-of-16.
Parsed 9765 rows from data/gen_labels_SiO2_06-of-16.
Parsed 9765 rows from data/gen_spectrum_SiO2_07-of-16.
Parsed 9765 rows from data/gen_labels_SiO2_07-of-16.
Parsed 9765 rows from data/gen_spectrum_SiO2_08-of-16.
Parsed 9765 rows from data/gen_labels_SiO2_08-of-16.
Parsed 9765 rows from data/gen_spectrum_SiO2_09-of-16.
Parsed 9765 rows from data/gen_labels_SiO2_09-of-16.
Parsed 9765 rows from data/gen_spectrum_SiO2_10-of-16.
Parsed 9765 rows from data/gen_labels_SiO2_10-of-16.
Parsed 9765 rows from data/gen_spectrum_SiO2_11-of-16.
Parsed 9765 rows from data/gen_labels_SiO2_11-of-16.
Parsed 9765 rows from data/gen_spectrum_SiO2_12-of-16.
Parsed 9765 rows from data/gen_labels_SiO2_12-of-16.
Parsed 9765 rows from data/gen_spectrum_SiO2_13-of-16.
Parsed 9765 rows from data/gen_labels_SiO2_13-of-16.
Parsed 9765 rows from data/gen_spectrum_SiO2_14-of-16.
Parsed 9765 rows from data/gen_labels_SiO2_14-of-16.
Parsed 9765 rows from data/gen_spectrum_SiO2_15-of-16.
Parsed 9765 rows from data/gen_labels_SiO2_15-of-16.
Parsed 1302 rows from data/sim_validation_spectrum_SiO2.
Parsed 1302 rows from data/sim_validation_labels_SiO2.
Logging training progress to tensorboard dir runs/alexnet-SiO2-lr_0.000500-trainsize_158844-05_02_2020_03:29-joint.
[epoch 0, batch    99] avg loss: 1.019481
[epoch 0, batch   199] avg loss: 0.831485
[epoch 0, batch   299] avg loss: 0.803894
[epoch 0, batch   399] avg loss: 0.807098
[epoch 0, batch   499] avg loss: 0.789273
[epoch 0, batch   599] avg loss: 0.793175
[epoch 0, batch   699] avg loss: 0.742808
[epoch 0, batch   799] avg loss: 0.699045
[epoch 0, batch   899] avg loss: 0.662504
[epoch 0, batch   999] avg loss: 0.659800
[epoch 0, batch  1099] avg loss: 0.629602
[epoch 0, batch  1199] avg loss: 0.634180
[epoch 0, batch  1299] avg loss: 0.610183
[epoch 0, batch  1399] avg loss: 0.618831
[epoch 0, batch  1499] avg loss: 0.603933
[epoch 0, batch  1599] avg loss: 0.621059
[epoch 0, batch  1699] avg loss: 0.602304
[epoch 0, batch  1799] avg loss: 0.585203
[epoch 0, batch  1899] avg loss: 0.569153
[epoch 0, batch  1999] avg loss: 0.582519
[epoch 0, batch  2099] avg loss: 0.580073
[epoch 0, batch  2199] avg loss: 0.565188
[epoch 0, batch  2299] avg loss: 0.568956
[epoch 0, batch  2399] avg loss: 0.551295
[epoch 1, batch    99] avg loss: 0.556868
[epoch 1, batch   199] avg loss: 0.547315
[epoch 1, batch   299] avg loss: 0.530992
[epoch 1, batch   399] avg loss: 0.534871
[epoch 1, batch   499] avg loss: 0.533153
[epoch 1, batch   599] avg loss: 0.542359
[epoch 1, batch   699] avg loss: 0.524293
[epoch 1, batch   799] avg loss: 0.533577
[epoch 1, batch   899] avg loss: 0.534377
[epoch 1, batch   999] avg loss: 0.543290
[epoch 1, batch  1099] avg loss: 0.511550
[epoch 1, batch  1199] avg loss: 0.524661
[epoch 1, batch  1299] avg loss: 0.531042
[epoch 1, batch  1399] avg loss: 0.534873
[epoch 1, batch  1499] avg loss: 0.507138
[epoch 1, batch  1599] avg loss: 0.491845
[epoch 1, batch  1699] avg loss: 0.517100
[epoch 1, batch  1799] avg loss: 0.503688
[epoch 1, batch  1899] avg loss: 0.512020
[epoch 1, batch  1999] avg loss: 0.501118
[epoch 1, batch  2099] avg loss: 0.496321
[epoch 1, batch  2199] avg loss: 0.505974
[epoch 1, batch  2299] avg loss: 0.483998
[epoch 1, batch  2399] avg loss: 0.503447
[epoch 2, batch    99] avg loss: 0.487795
[epoch 2, batch   199] avg loss: 0.490215
[epoch 2, batch   299] avg loss: 0.486247
[epoch 2, batch   399] avg loss: 0.487572
[epoch 2, batch   499] avg loss: 0.488972
[epoch 2, batch   599] avg loss: 0.491713
[epoch 2, batch   699] avg loss: 0.496386
[epoch 2, batch   799] avg loss: 0.475727
[epoch 2, batch   899] avg loss: 0.482811
[epoch 2, batch   999] avg loss: 0.483886
[epoch 2, batch  1099] avg loss: 0.490126
[epoch 2, batch  1199] avg loss: 0.488093
[epoch 2, batch  1299] avg loss: 0.476595
[epoch 2, batch  1399] avg loss: 0.470758
[epoch 2, batch  1499] avg loss: 0.479344
[epoch 2, batch  1599] avg loss: 0.464478
[epoch 2, batch  1699] avg loss: 0.457856
[epoch 2, batch  1799] avg loss: 0.478228
[epoch 2, batch  1899] avg loss: 0.480998
[epoch 2, batch  1999] avg loss: 0.452450
[epoch 2, batch  2099] avg loss: 0.477291
[epoch 2, batch  2199] avg loss: 0.486843
[epoch 2, batch  2299] avg loss: 0.444419
[epoch 2, batch  2399] avg loss: 0.459606
[epoch 3, batch    99] avg loss: 0.459575
[epoch 3, batch   199] avg loss: 0.481544
[epoch 3, batch   299] avg loss: 0.460287
[epoch 3, batch   399] avg loss: 0.466158
[epoch 3, batch   499] avg loss: 0.464264
[epoch 3, batch   599] avg loss: 0.469383
[epoch 3, batch   699] avg loss: 0.459961
[epoch 3, batch   799] avg loss: 0.459979
[epoch 3, batch   899] avg loss: 0.457363
[epoch 3, batch   999] avg loss: 0.465915
[epoch 3, batch  1099] avg loss: 0.464435
[epoch 3, batch  1199] avg loss: 0.449654
[epoch 3, batch  1299] avg loss: 0.452880
[epoch 3, batch  1399] avg loss: 0.460356
[epoch 3, batch  1499] avg loss: 0.470789
[epoch 3, batch  1599] avg loss: 0.450662
[epoch 3, batch  1699] avg loss: 0.443794
[epoch 3, batch  1799] avg loss: 0.458445
[epoch 3, batch  1899] avg loss: 0.460131
[epoch 3, batch  1999] avg loss: 0.459429
[epoch 3, batch  2099] avg loss: 0.451028
[epoch 3, batch  2199] avg loss: 0.449120
[epoch 3, batch  2299] avg loss: 0.461881
[epoch 3, batch  2399] avg loss: 0.432350
[epoch 4, batch    99] avg loss: 0.448808
[epoch 4, batch   199] avg loss: 0.464324
[epoch 4, batch   299] avg loss: 0.455323
[epoch 4, batch   399] avg loss: 0.448558
[epoch 4, batch   499] avg loss: 0.453587
[epoch 4, batch   599] avg loss: 0.430852
[epoch 4, batch   699] avg loss: 0.443241
[epoch 4, batch   799] avg loss: 0.462715
[epoch 4, batch   899] avg loss: 0.448540
[epoch 4, batch   999] avg loss: 0.440110
[epoch 4, batch  1099] avg loss: 0.439483
[epoch 4, batch  1199] avg loss: 0.436824
[epoch 4, batch  1299] avg loss: 0.453892
[epoch 4, batch  1399] avg loss: 0.437763
[epoch 4, batch  1499] avg loss: 0.437637
[epoch 4, batch  1599] avg loss: 0.434162
[epoch 4, batch  1699] avg loss: 0.440326
[epoch 4, batch  1799] avg loss: 0.439323
[epoch 4, batch  1899] avg loss: 0.440755
[epoch 4, batch  1999] avg loss: 0.442831
[epoch 4, batch  2099] avg loss: 0.425264
[epoch 4, batch  2199] avg loss: 0.438506
[epoch 4, batch  2299] avg loss: 0.428477
[epoch 4, batch  2399] avg loss: 0.429249
[epoch 5, batch    99] avg loss: 0.430642
[epoch 5, batch   199] avg loss: 0.437360
[epoch 5, batch   299] avg loss: 0.449647
[epoch 5, batch   399] avg loss: 0.425460
[epoch 5, batch   499] avg loss: 0.443515
[epoch 5, batch   599] avg loss: 0.434998
[epoch 5, batch   699] avg loss: 0.441578
[epoch 5, batch   799] avg loss: 0.434745
[epoch 5, batch   899] avg loss: 0.420850
[epoch 5, batch   999] avg loss: 0.437841
[epoch 5, batch  1099] avg loss: 0.426747
[epoch 5, batch  1199] avg loss: 0.434648
[epoch 5, batch  1299] avg loss: 0.416717
[epoch 5, batch  1399] avg loss: 0.450935
[epoch 5, batch  1499] avg loss: 0.420537
[epoch 5, batch  1599] avg loss: 0.430174
[epoch 5, batch  1699] avg loss: 0.423457
[epoch 5, batch  1799] avg loss: 0.420291
[epoch 5, batch  1899] avg loss: 0.428031
[epoch 5, batch  1999] avg loss: 0.438262
[epoch 5, batch  2099] avg loss: 0.424802
[epoch 5, batch  2199] avg loss: 0.433291
[epoch 5, batch  2299] avg loss: 0.422698
[epoch 5, batch  2399] avg loss: 0.432185
[epoch 6, batch    99] avg loss: 0.429567
[epoch 6, batch   199] avg loss: 0.432696
[epoch 6, batch   299] avg loss: 0.416414
[epoch 6, batch   399] avg loss: 0.421206
[epoch 6, batch   499] avg loss: 0.437179
[epoch 6, batch   599] avg loss: 0.424226
[epoch 6, batch   699] avg loss: 0.437167
[epoch 6, batch   799] avg loss: 0.410642
[epoch 6, batch   899] avg loss: 0.424148
[epoch 6, batch   999] avg loss: 0.411958
[epoch 6, batch  1099] avg loss: 0.424417
[epoch 6, batch  1199] avg loss: 0.416855
[epoch 6, batch  1299] avg loss: 0.425268
[epoch 6, batch  1399] avg loss: 0.428696
[epoch 6, batch  1499] avg loss: 0.429647
[epoch 6, batch  1599] avg loss: 0.409571
[epoch 6, batch  1699] avg loss: 0.424933
[epoch 6, batch  1799] avg loss: 0.413718
[epoch 6, batch  1899] avg loss: 0.411148
[epoch 6, batch  1999] avg loss: 0.421480
[epoch 6, batch  2099] avg loss: 0.424512
[epoch 6, batch  2199] avg loss: 0.439538
[epoch 6, batch  2299] avg loss: 0.412221
[epoch 6, batch  2399] avg loss: 0.411150
[epoch 7, batch    99] avg loss: 0.406319
[epoch 7, batch   199] avg loss: 0.427335
[epoch 7, batch   299] avg loss: 0.411966
[epoch 7, batch   399] avg loss: 0.432135
[epoch 7, batch   499] avg loss: 0.413677
[epoch 7, batch   599] avg loss: 0.415181
[epoch 7, batch   699] avg loss: 0.426401
[epoch 7, batch   799] avg loss: 0.406823
[epoch 7, batch   899] avg loss: 0.408684
[epoch 7, batch   999] avg loss: 0.396345
[epoch 7, batch  1099] avg loss: 0.430116
[epoch 7, batch  1199] avg loss: 0.431822
[epoch 7, batch  1299] avg loss: 0.417361
[epoch 7, batch  1399] avg loss: 0.429088
[epoch 7, batch  1499] avg loss: 0.406295
[epoch 7, batch  1599] avg loss: 0.418502
[epoch 7, batch  1699] avg loss: 0.408382
[epoch 7, batch  1799] avg loss: 0.404854
[epoch 7, batch  1899] avg loss: 0.404079
[epoch 7, batch  1999] avg loss: 0.408798
[epoch 7, batch  2099] avg loss: 0.406298
[epoch 7, batch  2199] avg loss: 0.405790
[epoch 7, batch  2299] avg loss: 0.403480
[epoch 7, batch  2399] avg loss: 0.406021
[epoch 8, batch    99] avg loss: 0.416940
[epoch 8, batch   199] avg loss: 0.427808
[epoch 8, batch   299] avg loss: 0.408854
[epoch 8, batch   399] avg loss: 0.421496
[epoch 8, batch   499] avg loss: 0.396068
[epoch 8, batch   599] avg loss: 0.411337
[epoch 8, batch   699] avg loss: 0.404950
[epoch 8, batch   799] avg loss: 0.399485
[epoch 8, batch   899] avg loss: 0.420072
[epoch 8, batch   999] avg loss: 0.410311
[epoch 8, batch  1099] avg loss: 0.410336
[epoch 8, batch  1199] avg loss: 0.407641
[epoch 8, batch  1299] avg loss: 0.406134
[epoch 8, batch  1399] avg loss: 0.411682
[epoch 8, batch  1499] avg loss: 0.400959
[epoch 8, batch  1599] avg loss: 0.401063
[epoch 8, batch  1699] avg loss: 0.405521
[epoch 8, batch  1799] avg loss: 0.410203
[epoch 8, batch  1899] avg loss: 0.393452
[epoch 8, batch  1999] avg loss: 0.401089
[epoch 8, batch  2099] avg loss: 0.407857
[epoch 8, batch  2199] avg loss: 0.399036
[epoch 8, batch  2299] avg loss: 0.398994
[epoch 8, batch  2399] avg loss: 0.389691
[epoch 9, batch    99] avg loss: 0.393909
[epoch 9, batch   199] avg loss: 0.400462
[epoch 9, batch   299] avg loss: 0.402658
[epoch 9, batch   399] avg loss: 0.409223
[epoch 9, batch   499] avg loss: 0.392155
[epoch 9, batch   599] avg loss: 0.408688
[epoch 9, batch   699] avg loss: 0.392936
[epoch 9, batch   799] avg loss: 0.404201
[epoch 9, batch   899] avg loss: 0.392959
[epoch 9, batch   999] avg loss: 0.411779
[epoch 9, batch  1099] avg loss: 0.406123
[epoch 9, batch  1199] avg loss: 0.386003
[epoch 9, batch  1299] avg loss: 0.405439
[epoch 9, batch  1399] avg loss: 0.396263
[epoch 9, batch  1499] avg loss: 0.401894
[epoch 9, batch  1599] avg loss: 0.386809
[epoch 9, batch  1699] avg loss: 0.393658
[epoch 9, batch  1799] avg loss: 0.395891
[epoch 9, batch  1899] avg loss: 0.394401
[epoch 9, batch  1999] avg loss: 0.399018
[epoch 9, batch  2099] avg loss: 0.392154
[epoch 9, batch  2199] avg loss: 0.382823
[epoch 9, batch  2299] avg loss: 0.384385
[epoch 9, batch  2399] avg loss: 0.385107
[epoch 10, batch    99] avg loss: 0.389523
[epoch 10, batch   199] avg loss: 0.382236
[epoch 10, batch   299] avg loss: 0.378714
[epoch 10, batch   399] avg loss: 0.387574
[epoch 10, batch   499] avg loss: 0.403424
[epoch 10, batch   599] avg loss: 0.393708
[epoch 10, batch   699] avg loss: 0.395947
[epoch 10, batch   799] avg loss: 0.397885
[epoch 10, batch   899] avg loss: 0.404057
[epoch 10, batch   999] avg loss: 0.365345
[epoch 10, batch  1099] avg loss: 0.395077
[epoch 10, batch  1199] avg loss: 0.395452
[epoch 10, batch  1299] avg loss: 0.388926
[epoch 10, batch  1399] avg loss: 0.396876
[epoch 10, batch  1499] avg loss: 0.389425
[epoch 10, batch  1599] avg loss: 0.387971
[epoch 10, batch  1699] avg loss: 0.389872
[epoch 10, batch  1799] avg loss: 0.381848
[epoch 10, batch  1899] avg loss: 0.388456
[epoch 10, batch  1999] avg loss: 0.393375
[epoch 10, batch  2099] avg loss: 0.381437
[epoch 10, batch  2199] avg loss: 0.403583
[epoch 10, batch  2299] avg loss: 0.381448
[epoch 10, batch  2399] avg loss: 0.387149
[epoch 11, batch    99] avg loss: 0.386472
[epoch 11, batch   199] avg loss: 0.394322
[epoch 11, batch   299] avg loss: 0.393999
[epoch 11, batch   399] avg loss: 0.387989
[epoch 11, batch   499] avg loss: 0.385073
[epoch 11, batch   599] avg loss: 0.384609
[epoch 11, batch   699] avg loss: 0.372511
[epoch 11, batch   799] avg loss: 0.381840
[epoch 11, batch   899] avg loss: 0.381731
[epoch 11, batch   999] avg loss: 0.390443
[epoch 11, batch  1099] avg loss: 0.378407
[epoch 11, batch  1199] avg loss: 0.378134
[epoch 11, batch  1299] avg loss: 0.392713
[epoch 11, batch  1399] avg loss: 0.383575
[epoch 11, batch  1499] avg loss: 0.385083
[epoch 11, batch  1599] avg loss: 0.386589
[epoch 11, batch  1699] avg loss: 0.396021
[epoch 11, batch  1799] avg loss: 0.370320
[epoch 11, batch  1899] avg loss: 0.381467
[epoch 11, batch  1999] avg loss: 0.397847
[epoch 11, batch  2099] avg loss: 0.377200
[epoch 11, batch  2199] avg loss: 0.377469
[epoch 11, batch  2299] avg loss: 0.394448
[epoch 11, batch  2399] avg loss: 0.380460
[epoch 12, batch    99] avg loss: 0.384779
[epoch 12, batch   199] avg loss: 0.388612
[epoch 12, batch   299] avg loss: 0.391751
[epoch 12, batch   399] avg loss: 0.386473
[epoch 12, batch   499] avg loss: 0.372918
[epoch 12, batch   599] avg loss: 0.388673
[epoch 12, batch   699] avg loss: 0.376880
[epoch 12, batch   799] avg loss: 0.376581
[epoch 12, batch   899] avg loss: 0.370069
[epoch 12, batch   999] avg loss: 0.382982
[epoch 12, batch  1099] avg loss: 0.367484
[epoch 12, batch  1199] avg loss: 0.379944
[epoch 12, batch  1299] avg loss: 0.390434
[epoch 12, batch  1399] avg loss: 0.368958
[epoch 12, batch  1499] avg loss: 0.373045
[epoch 12, batch  1599] avg loss: 0.390217
[epoch 12, batch  1699] avg loss: 0.366304
[epoch 12, batch  1799] avg loss: 0.388214
[epoch 12, batch  1899] avg loss: 0.380177
[epoch 12, batch  1999] avg loss: 0.376359
[epoch 12, batch  2099] avg loss: 0.369343
[epoch 12, batch  2199] avg loss: 0.396999
[epoch 12, batch  2299] avg loss: 0.382050
[epoch 12, batch  2399] avg loss: 0.376810
[epoch 13, batch    99] avg loss: 0.368245
[epoch 13, batch   199] avg loss: 0.402525
[epoch 13, batch   299] avg loss: 0.376936
[epoch 13, batch   399] avg loss: 0.371977
[epoch 13, batch   499] avg loss: 0.373509
[epoch 13, batch   599] avg loss: 0.374581
[epoch 13, batch   699] avg loss: 0.376543
[epoch 13, batch   799] avg loss: 0.368469
[epoch 13, batch   899] avg loss: 0.368376
[epoch 13, batch   999] avg loss: 0.380535
[epoch 13, batch  1099] avg loss: 0.369109
[epoch 13, batch  1199] avg loss: 0.374109
[epoch 13, batch  1299] avg loss: 0.366104
[epoch 13, batch  1399] avg loss: 0.381462
[epoch 13, batch  1499] avg loss: 0.370759
[epoch 13, batch  1599] avg loss: 0.384799
[epoch 13, batch  1699] avg loss: 0.365469
[epoch 13, batch  1799] avg loss: 0.380108
[epoch 13, batch  1899] avg loss: 0.374320
[epoch 13, batch  1999] avg loss: 0.371196
[epoch 13, batch  2099] avg loss: 0.363931
[epoch 13, batch  2199] avg loss: 0.361684
[epoch 13, batch  2299] avg loss: 0.373340
[epoch 13, batch  2399] avg loss: 0.365328
[epoch 14, batch    99] avg loss: 0.372329
[epoch 14, batch   199] avg loss: 0.377084
[epoch 14, batch   299] avg loss: 0.366281
[epoch 14, batch   399] avg loss: 0.370371
[epoch 14, batch   499] avg loss: 0.363582
[epoch 14, batch   599] avg loss: 0.363661
[epoch 14, batch   699] avg loss: 0.389785
[epoch 14, batch   799] avg loss: 0.375354
[epoch 14, batch   899] avg loss: 0.359266
[epoch 14, batch   999] avg loss: 0.364158
[epoch 14, batch  1099] avg loss: 0.381584
[epoch 14, batch  1199] avg loss: 0.375370
[epoch 14, batch  1299] avg loss: 0.373726
[epoch 14, batch  1399] avg loss: 0.375309
[epoch 14, batch  1499] avg loss: 0.356205
[epoch 14, batch  1599] avg loss: 0.377326
[epoch 14, batch  1699] avg loss: 0.362820
[epoch 14, batch  1799] avg loss: 0.356013
[epoch 14, batch  1899] avg loss: 0.363599
[epoch 14, batch  1999] avg loss: 0.382968
[epoch 14, batch  2099] avg loss: 0.376212
[epoch 14, batch  2199] avg loss: 0.355275
[epoch 14, batch  2299] avg loss: 0.373817
[epoch 14, batch  2399] avg loss: 0.368333
[epoch 15, batch    99] avg loss: 0.372964
[epoch 15, batch   199] avg loss: 0.371262
[epoch 15, batch   299] avg loss: 0.363228
[epoch 15, batch   399] avg loss: 0.362164
[epoch 15, batch   499] avg loss: 0.366810
[epoch 15, batch   599] avg loss: 0.367064
[epoch 15, batch   699] avg loss: 0.353717
[epoch 15, batch   799] avg loss: 0.371687
[epoch 15, batch   899] avg loss: 0.357242
[epoch 15, batch   999] avg loss: 0.362342
[epoch 15, batch  1099] avg loss: 0.374880
[epoch 15, batch  1199] avg loss: 0.366817
[epoch 15, batch  1299] avg loss: 0.373542
[epoch 15, batch  1399] avg loss: 0.357015
[epoch 15, batch  1499] avg loss: 0.367206
[epoch 15, batch  1599] avg loss: 0.358693
[epoch 15, batch  1699] avg loss: 0.361700
[epoch 15, batch  1799] avg loss: 0.354728
[epoch 15, batch  1899] avg loss: 0.359478
[epoch 15, batch  1999] avg loss: 0.351449
[epoch 15, batch  2099] avg loss: 0.367986
[epoch 15, batch  2199] avg loss: 0.346466
[epoch 15, batch  2299] avg loss: 0.362868
[epoch 15, batch  2399] avg loss: 0.371043
[epoch 16, batch    99] avg loss: 0.362534
[epoch 16, batch   199] avg loss: 0.345340
[epoch 16, batch   299] avg loss: 0.356765
[epoch 16, batch   399] avg loss: 0.359321
[epoch 16, batch   499] avg loss: 0.363825
[epoch 16, batch   599] avg loss: 0.362904
[epoch 16, batch   699] avg loss: 0.356352
[epoch 16, batch   799] avg loss: 0.367286
[epoch 16, batch   899] avg loss: 0.338322
[epoch 16, batch   999] avg loss: 0.360525
[epoch 16, batch  1099] avg loss: 0.346774
[epoch 16, batch  1199] avg loss: 0.357799
[epoch 16, batch  1299] avg loss: 0.346090
[epoch 16, batch  1399] avg loss: 0.346368
[epoch 16, batch  1499] avg loss: 0.355758
[epoch 16, batch  1599] avg loss: 0.352451
[epoch 16, batch  1699] avg loss: 0.375067
[epoch 16, batch  1799] avg loss: 0.356298
[epoch 16, batch  1899] avg loss: 0.351990
[epoch 16, batch  1999] avg loss: 0.345050
[epoch 16, batch  2099] avg loss: 0.351496
[epoch 16, batch  2199] avg loss: 0.362021
[epoch 16, batch  2299] avg loss: 0.348142
[epoch 16, batch  2399] avg loss: 0.356635
[epoch 17, batch    99] avg loss: 0.361439
[epoch 17, batch   199] avg loss: 0.344088
[epoch 17, batch   299] avg loss: 0.359827
[epoch 17, batch   399] avg loss: 0.350821
[epoch 17, batch   499] avg loss: 0.338316
[epoch 17, batch   599] avg loss: 0.356219
[epoch 17, batch   699] avg loss: 0.353867
[epoch 17, batch   799] avg loss: 0.369349
[epoch 17, batch   899] avg loss: 0.329125
[epoch 17, batch   999] avg loss: 0.348709
[epoch 17, batch  1099] avg loss: 0.351795
[epoch 17, batch  1199] avg loss: 0.354969
[epoch 17, batch  1299] avg loss: 0.335755
[epoch 17, batch  1399] avg loss: 0.364666
[epoch 17, batch  1499] avg loss: 0.352976
[epoch 17, batch  1599] avg loss: 0.339742
[epoch 17, batch  1699] avg loss: 0.342401
[epoch 17, batch  1799] avg loss: 0.347737
[epoch 17, batch  1899] avg loss: 0.348624
[epoch 17, batch  1999] avg loss: 0.354630
[epoch 17, batch  2099] avg loss: 0.348963
[epoch 17, batch  2199] avg loss: 0.339344
[epoch 17, batch  2299] avg loss: 0.349925
[epoch 17, batch  2399] avg loss: 0.342863
[epoch 18, batch    99] avg loss: 0.338636
[epoch 18, batch   199] avg loss: 0.327457
[epoch 18, batch   299] avg loss: 0.360581
[epoch 18, batch   399] avg loss: 0.346782
[epoch 18, batch   499] avg loss: 0.339741
[epoch 18, batch   599] avg loss: 0.343481
[epoch 18, batch   699] avg loss: 0.342466
[epoch 18, batch   799] avg loss: 0.348925
[epoch 18, batch   899] avg loss: 0.345865
[epoch 18, batch   999] avg loss: 0.340464
[epoch 18, batch  1099] avg loss: 0.345763
[epoch 18, batch  1199] avg loss: 0.331889
[epoch 18, batch  1299] avg loss: 0.348642
[epoch 18, batch  1399] avg loss: 0.340402
[epoch 18, batch  1499] avg loss: 0.349745
[epoch 18, batch  1599] avg loss: 0.352210
[epoch 18, batch  1699] avg loss: 0.350045
[epoch 18, batch  1799] avg loss: 0.328846
[epoch 18, batch  1899] avg loss: 0.338317
[epoch 18, batch  1999] avg loss: 0.347914
[epoch 18, batch  2099] avg loss: 0.348414
[epoch 18, batch  2199] avg loss: 0.343078
[epoch 18, batch  2299] avg loss: 0.336882
[epoch 18, batch  2399] avg loss: 0.333034
[epoch 19, batch    99] avg loss: 0.346895
[epoch 19, batch   199] avg loss: 0.340044
[epoch 19, batch   299] avg loss: 0.326969
[epoch 19, batch   399] avg loss: 0.320649
[epoch 19, batch   499] avg loss: 0.342156
[epoch 19, batch   599] avg loss: 0.333113
[epoch 19, batch   699] avg loss: 0.333502
[epoch 19, batch   799] avg loss: 0.334760
[epoch 19, batch   899] avg loss: 0.336285
[epoch 19, batch   999] avg loss: 0.343164
[epoch 19, batch  1099] avg loss: 0.342713
[epoch 19, batch  1199] avg loss: 0.335267
[epoch 19, batch  1299] avg loss: 0.334318
[epoch 19, batch  1399] avg loss: 0.341405
[epoch 19, batch  1499] avg loss: 0.333923
[epoch 19, batch  1599] avg loss: 0.334258
[epoch 19, batch  1699] avg loss: 0.337178
[epoch 19, batch  1799] avg loss: 0.349389
[epoch 19, batch  1899] avg loss: 0.330808
[epoch 19, batch  1999] avg loss: 0.335132
[epoch 19, batch  2099] avg loss: 0.338153
[epoch 19, batch  2199] avg loss: 0.340146
[epoch 19, batch  2299] avg loss: 0.338514
[epoch 19, batch  2399] avg loss: 0.349000
Model saved to model/20200502-034710.pth.
accuracy/TriangPrismIsosc : 0.662
n_examples/TriangPrismIsosc : 500.0
accuracy/parallelepiped : 0.608
n_examples/parallelepiped : 500.0
accuracy/sphere : 1.0
n_examples/sphere : 102.0
accuracy/wire : 0.985
n_examples/wire : 200.0
accuracy/avg_geom : 0.717357910906298
loss/validation_geom : 0.7089387319421255
accuracy/Au : 0.0
n_examples/Au : 0.0
accuracy/SiN : 0.0
n_examples/SiN : 0.0
accuracy/SiO2 : 0.10522273425499232
n_examples/SiO2 : 1302.0
accuracy/avg_mat : 0.10522273425499232
loss/validation_mat : 1.6877974672800935
MSE/ShortestDim : 1.9305999333957373
MAE/ShortestDim : 0.8480200108295212
MSE/MiddleDim : 7.411590792982626
MAE/MiddleDim : 1.8736969732469129
MSE/LongDim : 144.95667160198252
MAE/LongDim : 7.568537282870479
MSE/log Area/Vol : 20.450535330354892
MAE/log Area/Vol : 3.649270281813661
loss/validation_dim : 174.74939765871576
loss/validation : 177.146133857938
Metrics saved to model/20200502-034710_metrics.csv.
Parsed 7812 rows from data/sim_train_spectrum_all.
Parsed 7812 rows from data/sim_train_labels_all.
Parsed 9765 rows from data/gen_spectrum_all_00-of-16.
Parsed 9765 rows from data/gen_labels_all_00-of-16.
Parsed 9765 rows from data/gen_spectrum_all_01-of-16.
Parsed 9765 rows from data/gen_labels_all_01-of-16.
Parsed 9765 rows from data/gen_spectrum_all_02-of-16.
Parsed 9765 rows from data/gen_labels_all_02-of-16.
Parsed 9765 rows from data/gen_spectrum_all_03-of-16.
Parsed 9765 rows from data/gen_labels_all_03-of-16.
Parsed 9765 rows from data/gen_spectrum_all_04-of-16.
Parsed 9765 rows from data/gen_labels_all_04-of-16.
Parsed 9765 rows from data/gen_spectrum_all_05-of-16.
Parsed 9765 rows from data/gen_labels_all_05-of-16.
Parsed 9765 rows from data/gen_spectrum_all_06-of-16.
Parsed 9765 rows from data/gen_labels_all_06-of-16.
Parsed 9765 rows from data/gen_spectrum_all_07-of-16.
Parsed 9765 rows from data/gen_labels_all_07-of-16.
Parsed 9765 rows from data/gen_spectrum_all_08-of-16.
Parsed 9765 rows from data/gen_labels_all_08-of-16.
Parsed 9765 rows from data/gen_spectrum_all_09-of-16.
Parsed 9765 rows from data/gen_labels_all_09-of-16.
Parsed 9765 rows from data/gen_spectrum_all_10-of-16.
Parsed 9765 rows from data/gen_labels_all_10-of-16.
Parsed 9765 rows from data/gen_spectrum_all_11-of-16.
Parsed 9765 rows from data/gen_labels_all_11-of-16.
Parsed 9765 rows from data/gen_spectrum_all_12-of-16.
Parsed 9765 rows from data/gen_labels_all_12-of-16.
Parsed 9765 rows from data/gen_spectrum_all_13-of-16.
Parsed 9765 rows from data/gen_labels_all_13-of-16.
Parsed 9765 rows from data/gen_spectrum_all_14-of-16.
Parsed 9765 rows from data/gen_labels_all_14-of-16.
Parsed 9765 rows from data/gen_spectrum_all_15-of-16.
Parsed 9765 rows from data/gen_labels_all_15-of-16.
Parsed 3906 rows from data/sim_validation_spectrum_all.
Parsed 3906 rows from data/sim_validation_labels_all.
Logging training progress to tensorboard dir runs/alexnet-all-lr_0.000500-trainsize_164052-05_02_2020_03:48-multistage-joint.
[epoch 0, batch    99] avg loss: 0.521818
[epoch 0, batch   199] avg loss: 0.036449
[epoch 0, batch   299] avg loss: 0.015814
[epoch 0, batch   399] avg loss: 0.007660
[epoch 0, batch   499] avg loss: 0.006654
[epoch 0, batch   599] avg loss: 0.009218
[epoch 0, batch   699] avg loss: 0.004731
[epoch 0, batch   799] avg loss: 0.001859
[epoch 0, batch   899] avg loss: 0.000802
[epoch 0, batch   999] avg loss: 0.000161
[epoch 0, batch  1099] avg loss: 0.000277
[epoch 0, batch  1199] avg loss: 0.002678
[epoch 0, batch  1299] avg loss: 0.003103
[epoch 0, batch  1399] avg loss: 0.025616
[epoch 0, batch  1499] avg loss: 0.008109
[epoch 0, batch  1599] avg loss: 0.000651
[epoch 0, batch  1699] avg loss: 0.000316
[epoch 0, batch  1799] avg loss: 0.000045
[epoch 0, batch  1899] avg loss: 0.000068
[epoch 0, batch  1999] avg loss: 0.000045
[epoch 0, batch  2099] avg loss: 0.001286
[epoch 0, batch  2199] avg loss: 0.000092
[epoch 0, batch  2299] avg loss: 0.000051
[epoch 0, batch  2399] avg loss: 0.000094
[epoch 0, batch  2499] avg loss: 0.000052
[epoch 1, batch    99] avg loss: 0.000550
[epoch 1, batch   199] avg loss: 0.000085
[epoch 1, batch   299] avg loss: 0.000091
[epoch 1, batch   399] avg loss: 0.000015
[epoch 1, batch   499] avg loss: 0.000012
[epoch 1, batch   599] avg loss: 0.000013
[epoch 1, batch   699] avg loss: 0.000051
[epoch 1, batch   799] avg loss: 0.000024
[epoch 1, batch   899] avg loss: 0.010586
[epoch 1, batch   999] avg loss: 0.004172
[epoch 1, batch  1099] avg loss: 0.000060
[epoch 1, batch  1199] avg loss: 0.000047
[epoch 1, batch  1299] avg loss: 0.000029
[epoch 1, batch  1399] avg loss: 0.000031
[epoch 1, batch  1499] avg loss: 0.008243
[epoch 1, batch  1599] avg loss: 0.000669
[epoch 1, batch  1699] avg loss: 0.000087
[epoch 1, batch  1799] avg loss: 0.000036
[epoch 1, batch  1899] avg loss: 0.000044
[epoch 1, batch  1999] avg loss: 0.000070
[epoch 1, batch  2099] avg loss: 0.000033
[epoch 1, batch  2199] avg loss: 0.000017
[epoch 1, batch  2299] avg loss: 0.003999
[epoch 1, batch  2399] avg loss: 0.000050
[epoch 1, batch  2499] avg loss: 0.000038
[epoch 2, batch    99] avg loss: 0.000035
[epoch 2, batch   199] avg loss: 0.000026
[epoch 2, batch   299] avg loss: 0.000012
[epoch 2, batch   399] avg loss: 0.000013
[epoch 2, batch   499] avg loss: 0.000005
[epoch 2, batch   599] avg loss: 0.000009
[epoch 2, batch   699] avg loss: 0.000012
[epoch 2, batch   799] avg loss: 0.000004
[epoch 2, batch   899] avg loss: 0.000003
[epoch 2, batch   999] avg loss: 0.000006
[epoch 2, batch  1099] avg loss: 0.000004
[epoch 2, batch  1199] avg loss: 0.000005
[epoch 2, batch  1299] avg loss: 0.000003
[epoch 2, batch  1399] avg loss: 0.000002
[epoch 2, batch  1499] avg loss: 0.000005
[epoch 2, batch  1599] avg loss: 0.002410
[epoch 2, batch  1699] avg loss: 0.001631
[epoch 2, batch  1799] avg loss: 0.001974
[epoch 2, batch  1899] avg loss: 0.000590
[epoch 2, batch  1999] avg loss: 0.018289
[epoch 2, batch  2099] avg loss: 0.000047
[epoch 2, batch  2199] avg loss: 0.000015
[epoch 2, batch  2299] avg loss: 0.000128
[epoch 2, batch  2399] avg loss: 0.000046
[epoch 2, batch  2499] avg loss: 0.004053
[epoch 3, batch    99] avg loss: 0.000056
[epoch 3, batch   199] avg loss: 0.000026
[epoch 3, batch   299] avg loss: 0.000011
[epoch 3, batch   399] avg loss: 0.000011
[epoch 3, batch   499] avg loss: 0.001729
[epoch 3, batch   599] avg loss: 0.000029
[epoch 3, batch   699] avg loss: 0.000014
[epoch 3, batch   799] avg loss: 0.000010
[epoch 3, batch   899] avg loss: 0.000005
[epoch 3, batch   999] avg loss: 0.000011
[epoch 3, batch  1099] avg loss: 0.000005
[epoch 3, batch  1199] avg loss: 0.000092
[epoch 3, batch  1299] avg loss: 0.000031
[epoch 3, batch  1399] avg loss: 0.000020
[epoch 3, batch  1499] avg loss: 0.000017
[epoch 3, batch  1599] avg loss: 0.000036
[epoch 3, batch  1699] avg loss: 0.000006
[epoch 3, batch  1799] avg loss: 0.000022
[epoch 3, batch  1899] avg loss: 0.000003
[epoch 3, batch  1999] avg loss: 0.000001
[epoch 3, batch  2099] avg loss: 0.000001
[epoch 3, batch  2199] avg loss: 0.000002
[epoch 3, batch  2299] avg loss: 0.000001
[epoch 3, batch  2399] avg loss: 0.000003
[epoch 3, batch  2499] avg loss: 0.000001
[epoch 4, batch    99] avg loss: 0.000002
[epoch 4, batch   199] avg loss: 0.000000
[epoch 4, batch   299] avg loss: 0.000001
[epoch 4, batch   399] avg loss: 0.000001
[epoch 4, batch   499] avg loss: 0.001016
[epoch 4, batch   599] avg loss: 0.000005
[epoch 4, batch   699] avg loss: 0.000015
[epoch 4, batch   799] avg loss: 0.000002
[epoch 4, batch   899] avg loss: 0.000003
[epoch 4, batch   999] avg loss: 0.000025
[epoch 4, batch  1099] avg loss: 0.000016
[epoch 4, batch  1199] avg loss: 0.000005
[epoch 4, batch  1299] avg loss: 0.000003
[epoch 4, batch  1399] avg loss: 0.000001
[epoch 4, batch  1499] avg loss: 0.000002
[epoch 4, batch  1599] avg loss: 0.000000
[epoch 4, batch  1699] avg loss: 0.000001
[epoch 4, batch  1799] avg loss: 0.000001
[epoch 4, batch  1899] avg loss: 0.000003
[epoch 4, batch  1999] avg loss: 0.000000
[epoch 4, batch  2099] avg loss: 0.000000
[epoch 4, batch  2199] avg loss: 0.000001
[epoch 4, batch  2299] avg loss: 0.000001
[epoch 4, batch  2399] avg loss: 0.000001
[epoch 4, batch  2499] avg loss: 0.000003
Model saved to model/20200502-035240.pth.
accuracy/TriangPrismIsosc : 0.006666666666666667
n_examples/TriangPrismIsosc : 1500.0
accuracy/parallelepiped : 0.7253333333333334
n_examples/parallelepiped : 1500.0
accuracy/sphere : 0.06862745098039216
n_examples/sphere : 306.0
accuracy/wire : 0.17
n_examples/wire : 600.0
accuracy/avg_geom : 0.31259600614439326
loss/validation_geom : 5.840646058183661
accuracy/Au : 1.0
n_examples/Au : 1302.0
accuracy/SiN : 1.0
n_examples/SiN : 1302.0
accuracy/SiO2 : 1.0
n_examples/SiO2 : 1302.0
accuracy/avg_mat : 1.0
loss/validation_mat : 6.261847413115804e-05
MSE/ShortestDim : 278.70431013834883
MAE/ShortestDim : 10.083921662001993
MSE/MiddleDim : 376.0274405218184
MAE/MiddleDim : 12.702808104718702
MSE/LongDim : 114.51735649977014
MAE/LongDim : 6.474057103021293
MSE/log Area/Vol : 262.5909174593424
MAE/log Area/Vol : 10.319714009059862
loss/validation_dim : 1031.8400246192798
loss/validation : 1037.6807332959377
Metrics saved to model/20200502-035240_metrics.csv.
[epoch 0, batch    99] avg loss: 0.327371
[epoch 0, batch   199] avg loss: 0.222356
[epoch 0, batch   299] avg loss: 0.188720
[epoch 0, batch   399] avg loss: 0.163484
[epoch 0, batch   499] avg loss: 0.146094
[epoch 0, batch   599] avg loss: 0.142024
[epoch 0, batch   699] avg loss: 0.150843
[epoch 0, batch   799] avg loss: 0.129545
[epoch 0, batch   899] avg loss: 0.135512
[epoch 0, batch   999] avg loss: 0.122740
[epoch 0, batch  1099] avg loss: 0.126633
[epoch 0, batch  1199] avg loss: 0.125099
[epoch 0, batch  1299] avg loss: 0.109334
[epoch 0, batch  1399] avg loss: 0.124991
[epoch 0, batch  1499] avg loss: 0.117043
[epoch 0, batch  1599] avg loss: 0.125964
[epoch 0, batch  1699] avg loss: 0.119747
[epoch 0, batch  1799] avg loss: 0.127257
[epoch 0, batch  1899] avg loss: 0.126946
[epoch 0, batch  1999] avg loss: 0.100717
[epoch 0, batch  2099] avg loss: 0.111005
[epoch 0, batch  2199] avg loss: 0.100911
[epoch 0, batch  2299] avg loss: 0.109458
[epoch 0, batch  2399] avg loss: 0.096877
[epoch 0, batch  2499] avg loss: 0.102877
[epoch 1, batch    99] avg loss: 0.089898
[epoch 1, batch   199] avg loss: 0.094291
[epoch 1, batch   299] avg loss: 0.105299
[epoch 1, batch   399] avg loss: 0.095090
[epoch 1, batch   499] avg loss: 0.096895
[epoch 1, batch   599] avg loss: 0.090723
[epoch 1, batch   699] avg loss: 0.086019
[epoch 1, batch   799] avg loss: 0.090462
[epoch 1, batch   899] avg loss: 0.086243
[epoch 1, batch   999] avg loss: 0.090317
[epoch 1, batch  1099] avg loss: 0.097042
[epoch 1, batch  1199] avg loss: 0.084188
[epoch 1, batch  1299] avg loss: 0.098030
[epoch 1, batch  1399] avg loss: 0.083618
[epoch 1, batch  1499] avg loss: 0.098203
[epoch 1, batch  1599] avg loss: 0.077020
[epoch 1, batch  1699] avg loss: 0.080911
[epoch 1, batch  1799] avg loss: 0.078517
[epoch 1, batch  1899] avg loss: 0.088473
[epoch 1, batch  1999] avg loss: 0.088011
[epoch 1, batch  2099] avg loss: 0.078489
[epoch 1, batch  2199] avg loss: 0.067525
[epoch 1, batch  2299] avg loss: 0.074241
[epoch 1, batch  2399] avg loss: 0.083262
[epoch 1, batch  2499] avg loss: 0.079429
[epoch 2, batch    99] avg loss: 0.079938
[epoch 2, batch   199] avg loss: 0.077373
[epoch 2, batch   299] avg loss: 0.087978
[epoch 2, batch   399] avg loss: 0.074039
[epoch 2, batch   499] avg loss: 0.082278
[epoch 2, batch   599] avg loss: 0.083537
[epoch 2, batch   699] avg loss: 0.086977
[epoch 2, batch   799] avg loss: 0.080165
[epoch 2, batch   899] avg loss: 0.076053
[epoch 2, batch   999] avg loss: 0.077446
[epoch 2, batch  1099] avg loss: 0.071128
[epoch 2, batch  1199] avg loss: 0.085966
[epoch 2, batch  1299] avg loss: 0.064292
[epoch 2, batch  1399] avg loss: 0.074446
[epoch 2, batch  1499] avg loss: 0.065033
[epoch 2, batch  1599] avg loss: 0.068546
[epoch 2, batch  1699] avg loss: 0.068497
[epoch 2, batch  1799] avg loss: 0.069468
[epoch 2, batch  1899] avg loss: 0.066396
[epoch 2, batch  1999] avg loss: 0.078229
[epoch 2, batch  2099] avg loss: 0.076087
[epoch 2, batch  2199] avg loss: 0.071006
[epoch 2, batch  2299] avg loss: 0.071002
[epoch 2, batch  2399] avg loss: 0.072460
[epoch 2, batch  2499] avg loss: 0.073051
[epoch 3, batch    99] avg loss: 0.063285
[epoch 3, batch   199] avg loss: 0.073647
[epoch 3, batch   299] avg loss: 0.068452
[epoch 3, batch   399] avg loss: 0.065748
[epoch 3, batch   499] avg loss: 0.073465
[epoch 3, batch   599] avg loss: 0.065751
[epoch 3, batch   699] avg loss: 0.069199
[epoch 3, batch   799] avg loss: 0.075533
[epoch 3, batch   899] avg loss: 0.065794
[epoch 3, batch   999] avg loss: 0.074737
[epoch 3, batch  1099] avg loss: 0.076999
[epoch 3, batch  1199] avg loss: 0.077000
[epoch 3, batch  1299] avg loss: 0.066838
[epoch 3, batch  1399] avg loss: 0.073520
[epoch 3, batch  1499] avg loss: 0.065881
[epoch 3, batch  1599] avg loss: 0.061000
[epoch 3, batch  1699] avg loss: 0.071816
[epoch 3, batch  1799] avg loss: 0.072167
[epoch 3, batch  1899] avg loss: 0.074713
[epoch 3, batch  1999] avg loss: 0.066030
[epoch 3, batch  2099] avg loss: 0.057472
[epoch 3, batch  2199] avg loss: 0.073184
[epoch 3, batch  2299] avg loss: 0.057710
[epoch 3, batch  2399] avg loss: 0.075888
[epoch 3, batch  2499] avg loss: 0.053297
[epoch 4, batch    99] avg loss: 0.064568
[epoch 4, batch   199] avg loss: 0.059963
[epoch 4, batch   299] avg loss: 0.069866
[epoch 4, batch   399] avg loss: 0.060328
[epoch 4, batch   499] avg loss: 0.072759
[epoch 4, batch   599] avg loss: 0.069465
[epoch 4, batch   699] avg loss: 0.062390
[epoch 4, batch   799] avg loss: 0.070341
[epoch 4, batch   899] avg loss: 0.071837
[epoch 4, batch   999] avg loss: 0.075629
[epoch 4, batch  1099] avg loss: 0.066563
[epoch 4, batch  1199] avg loss: 0.065067
[epoch 4, batch  1299] avg loss: 0.068879
[epoch 4, batch  1399] avg loss: 0.062390
[epoch 4, batch  1499] avg loss: 0.065558
[epoch 4, batch  1599] avg loss: 0.066618
[epoch 4, batch  1699] avg loss: 0.065209
[epoch 4, batch  1799] avg loss: 0.060178
[epoch 4, batch  1899] avg loss: 0.063678
[epoch 4, batch  1999] avg loss: 0.062841
[epoch 4, batch  2099] avg loss: 0.065402
[epoch 4, batch  2199] avg loss: 0.061255
[epoch 4, batch  2299] avg loss: 0.071868
[epoch 4, batch  2399] avg loss: 0.065737
[epoch 4, batch  2499] avg loss: 0.063022
[epoch 5, batch    99] avg loss: 0.069723
[epoch 5, batch   199] avg loss: 0.062752
[epoch 5, batch   299] avg loss: 0.057391
[epoch 5, batch   399] avg loss: 0.055298
[epoch 5, batch   499] avg loss: 0.063022
[epoch 5, batch   599] avg loss: 0.065517
[epoch 5, batch   699] avg loss: 0.072124
[epoch 5, batch   799] avg loss: 0.071102
[epoch 5, batch   899] avg loss: 0.064185
[epoch 5, batch   999] avg loss: 0.063689
[epoch 5, batch  1099] avg loss: 0.057562
[epoch 5, batch  1199] avg loss: 0.066334
[epoch 5, batch  1299] avg loss: 0.066650
[epoch 5, batch  1399] avg loss: 0.067343
[epoch 5, batch  1499] avg loss: 0.062449
[epoch 5, batch  1599] avg loss: 0.064126
[epoch 5, batch  1699] avg loss: 0.057166
[epoch 5, batch  1799] avg loss: 0.057374
[epoch 5, batch  1899] avg loss: 0.064102
[epoch 5, batch  1999] avg loss: 0.062072
[epoch 5, batch  2099] avg loss: 0.064554
[epoch 5, batch  2199] avg loss: 0.056554
[epoch 5, batch  2299] avg loss: 0.062678
[epoch 5, batch  2399] avg loss: 0.070284
[epoch 5, batch  2499] avg loss: 0.055538
[epoch 6, batch    99] avg loss: 0.069357
[epoch 6, batch   199] avg loss: 0.068577
[epoch 6, batch   299] avg loss: 0.060609
[epoch 6, batch   399] avg loss: 0.065043
[epoch 6, batch   499] avg loss: 0.057991
[epoch 6, batch   599] avg loss: 0.065944
[epoch 6, batch   699] avg loss: 0.064216
[epoch 6, batch   799] avg loss: 0.055280
[epoch 6, batch   899] avg loss: 0.056807
[epoch 6, batch   999] avg loss: 0.070541
[epoch 6, batch  1099] avg loss: 0.050724
[epoch 6, batch  1199] avg loss: 0.072565
[epoch 6, batch  1299] avg loss: 0.070535
[epoch 6, batch  1399] avg loss: 0.056032
[epoch 6, batch  1499] avg loss: 0.059124
[epoch 6, batch  1599] avg loss: 0.053074
[epoch 6, batch  1699] avg loss: 0.055433
[epoch 6, batch  1799] avg loss: 0.061095
[epoch 6, batch  1899] avg loss: 0.064631
[epoch 6, batch  1999] avg loss: 0.067109
[epoch 6, batch  2099] avg loss: 0.059689
[epoch 6, batch  2199] avg loss: 0.071631
[epoch 6, batch  2299] avg loss: 0.060501
[epoch 6, batch  2399] avg loss: 0.058896
[epoch 6, batch  2499] avg loss: 0.057891
[epoch 7, batch    99] avg loss: 0.050578
[epoch 7, batch   199] avg loss: 0.054764
[epoch 7, batch   299] avg loss: 0.063244
[epoch 7, batch   399] avg loss: 0.058644
[epoch 7, batch   499] avg loss: 0.066102
[epoch 7, batch   599] avg loss: 0.066819
[epoch 7, batch   699] avg loss: 0.054695
[epoch 7, batch   799] avg loss: 0.055480
[epoch 7, batch   899] avg loss: 0.060583
[epoch 7, batch   999] avg loss: 0.053656
[epoch 7, batch  1099] avg loss: 0.071246
[epoch 7, batch  1199] avg loss: 0.068474
[epoch 7, batch  1299] avg loss: 0.052353
[epoch 7, batch  1399] avg loss: 0.050840
[epoch 7, batch  1499] avg loss: 0.065449
[epoch 7, batch  1599] avg loss: 0.059097
[epoch 7, batch  1699] avg loss: 0.062832
[epoch 7, batch  1799] avg loss: 0.060835
[epoch 7, batch  1899] avg loss: 0.068665
[epoch 7, batch  1999] avg loss: 0.056393
[epoch 7, batch  2099] avg loss: 0.062527
[epoch 7, batch  2199] avg loss: 0.051500
[epoch 7, batch  2299] avg loss: 0.062095
[epoch 7, batch  2399] avg loss: 0.065349
[epoch 7, batch  2499] avg loss: 0.070773
[epoch 8, batch    99] avg loss: 0.060802
[epoch 8, batch   199] avg loss: 0.065254
[epoch 8, batch   299] avg loss: 0.050891
[epoch 8, batch   399] avg loss: 0.058687
[epoch 8, batch   499] avg loss: 0.057715
[epoch 8, batch   599] avg loss: 0.060150
[epoch 8, batch   699] avg loss: 0.061286
[epoch 8, batch   799] avg loss: 0.077640
[epoch 8, batch   899] avg loss: 0.056908
[epoch 8, batch   999] avg loss: 0.063880
[epoch 8, batch  1099] avg loss: 0.063679
[epoch 8, batch  1199] avg loss: 0.057760
[epoch 8, batch  1299] avg loss: 0.052458
[epoch 8, batch  1399] avg loss: 0.059904
[epoch 8, batch  1499] avg loss: 0.050397
[epoch 8, batch  1599] avg loss: 0.056461
[epoch 8, batch  1699] avg loss: 0.048971
[epoch 8, batch  1799] avg loss: 0.059113
[epoch 8, batch  1899] avg loss: 0.057584
[epoch 8, batch  1999] avg loss: 0.055437
[epoch 8, batch  2099] avg loss: 0.054720
[epoch 8, batch  2199] avg loss: 0.061729
[epoch 8, batch  2299] avg loss: 0.062909
[epoch 8, batch  2399] avg loss: 0.058503
[epoch 8, batch  2499] avg loss: 0.060982
[epoch 9, batch    99] avg loss: 0.054707
[epoch 9, batch   199] avg loss: 0.068535
[epoch 9, batch   299] avg loss: 0.054518
[epoch 9, batch   399] avg loss: 0.053390
[epoch 9, batch   499] avg loss: 0.058919
[epoch 9, batch   599] avg loss: 0.063633
[epoch 9, batch   699] avg loss: 0.066829
[epoch 9, batch   799] avg loss: 0.058994
[epoch 9, batch   899] avg loss: 0.055005
[epoch 9, batch   999] avg loss: 0.058311
[epoch 9, batch  1099] avg loss: 0.052121
[epoch 9, batch  1199] avg loss: 0.054851
[epoch 9, batch  1299] avg loss: 0.057793
[epoch 9, batch  1399] avg loss: 0.061728
[epoch 9, batch  1499] avg loss: 0.060857
[epoch 9, batch  1599] avg loss: 0.056600
[epoch 9, batch  1699] avg loss: 0.050502
[epoch 9, batch  1799] avg loss: 0.057278
[epoch 9, batch  1899] avg loss: 0.068888
[epoch 9, batch  1999] avg loss: 0.056376
[epoch 9, batch  2099] avg loss: 0.061280
[epoch 9, batch  2199] avg loss: 0.055625
[epoch 9, batch  2299] avg loss: 0.057519
[epoch 9, batch  2399] avg loss: 0.066043
[epoch 9, batch  2499] avg loss: 0.052846
[epoch 10, batch    99] avg loss: 0.054290
[epoch 10, batch   199] avg loss: 0.061366
[epoch 10, batch   299] avg loss: 0.056821
[epoch 10, batch   399] avg loss: 0.053799
[epoch 10, batch   499] avg loss: 0.057538
[epoch 10, batch   599] avg loss: 0.055973
[epoch 10, batch   699] avg loss: 0.061702
[epoch 10, batch   799] avg loss: 0.056859
[epoch 10, batch   899] avg loss: 0.058049
[epoch 10, batch   999] avg loss: 0.059764
[epoch 10, batch  1099] avg loss: 0.054831
[epoch 10, batch  1199] avg loss: 0.058677
[epoch 10, batch  1299] avg loss: 0.056839
[epoch 10, batch  1399] avg loss: 0.052369
[epoch 10, batch  1499] avg loss: 0.059233
[epoch 10, batch  1599] avg loss: 0.057582
[epoch 10, batch  1699] avg loss: 0.053331
[epoch 10, batch  1799] avg loss: 0.066039
[epoch 10, batch  1899] avg loss: 0.062504
[epoch 10, batch  1999] avg loss: 0.049668
[epoch 10, batch  2099] avg loss: 0.054702
[epoch 10, batch  2199] avg loss: 0.057686
[epoch 10, batch  2299] avg loss: 0.057751
[epoch 10, batch  2399] avg loss: 0.052491
[epoch 10, batch  2499] avg loss: 0.053828
[epoch 11, batch    99] avg loss: 0.059338
[epoch 11, batch   199] avg loss: 0.049918
[epoch 11, batch   299] avg loss: 0.058740
[epoch 11, batch   399] avg loss: 0.064585
[epoch 11, batch   499] avg loss: 0.049592
[epoch 11, batch   599] avg loss: 0.055129
[epoch 11, batch   699] avg loss: 0.064854
[epoch 11, batch   799] avg loss: 0.060843
[epoch 11, batch   899] avg loss: 0.049293
[epoch 11, batch   999] avg loss: 0.059728
[epoch 11, batch  1099] avg loss: 0.050704
[epoch 11, batch  1199] avg loss: 0.059545
[epoch 11, batch  1299] avg loss: 0.052733
[epoch 11, batch  1399] avg loss: 0.058333
[epoch 11, batch  1499] avg loss: 0.064199
[epoch 11, batch  1599] avg loss: 0.060208
[epoch 11, batch  1699] avg loss: 0.058539
[epoch 11, batch  1799] avg loss: 0.047794
[epoch 11, batch  1899] avg loss: 0.052618
[epoch 11, batch  1999] avg loss: 0.058287
[epoch 11, batch  2099] avg loss: 0.054575
[epoch 11, batch  2199] avg loss: 0.057874
[epoch 11, batch  2299] avg loss: 0.056053
[epoch 11, batch  2399] avg loss: 0.048512
[epoch 11, batch  2499] avg loss: 0.054081
[epoch 12, batch    99] avg loss: 0.052099
[epoch 12, batch   199] avg loss: 0.055116
[epoch 12, batch   299] avg loss: 0.049219
[epoch 12, batch   399] avg loss: 0.066693
[epoch 12, batch   499] avg loss: 0.046621
[epoch 12, batch   599] avg loss: 0.056837
[epoch 12, batch   699] avg loss: 0.054322
[epoch 12, batch   799] avg loss: 0.055707
[epoch 12, batch   899] avg loss: 0.052089
[epoch 12, batch   999] avg loss: 0.057692
[epoch 12, batch  1099] avg loss: 0.048150
[epoch 12, batch  1199] avg loss: 0.060234
[epoch 12, batch  1299] avg loss: 0.060541
[epoch 12, batch  1399] avg loss: 0.062613
[epoch 12, batch  1499] avg loss: 0.058421
[epoch 12, batch  1599] avg loss: 0.052329
[epoch 12, batch  1699] avg loss: 0.052141
[epoch 12, batch  1799] avg loss: 0.054962
[epoch 12, batch  1899] avg loss: 0.055623
[epoch 12, batch  1999] avg loss: 0.061519
[epoch 12, batch  2099] avg loss: 0.062167
[epoch 12, batch  2199] avg loss: 0.045729
[epoch 12, batch  2299] avg loss: 0.061766
[epoch 12, batch  2399] avg loss: 0.059792
[epoch 12, batch  2499] avg loss: 0.057264
[epoch 13, batch    99] avg loss: 0.051768
[epoch 13, batch   199] avg loss: 0.048837
[epoch 13, batch   299] avg loss: 0.055715
[epoch 13, batch   399] avg loss: 0.058874
[epoch 13, batch   499] avg loss: 0.059879
[epoch 13, batch   599] avg loss: 0.058448
[epoch 13, batch   699] avg loss: 0.053163
[epoch 13, batch   799] avg loss: 0.057889
[epoch 13, batch   899] avg loss: 0.046279
[epoch 13, batch   999] avg loss: 0.054755
[epoch 13, batch  1099] avg loss: 0.067728
[epoch 13, batch  1199] avg loss: 0.060385
[epoch 13, batch  1299] avg loss: 0.055124
[epoch 13, batch  1399] avg loss: 0.060276
[epoch 13, batch  1499] avg loss: 0.062320
[epoch 13, batch  1599] avg loss: 0.053579
[epoch 13, batch  1699] avg loss: 0.049078
[epoch 13, batch  1799] avg loss: 0.055920
[epoch 13, batch  1899] avg loss: 0.056154
[epoch 13, batch  1999] avg loss: 0.052644
[epoch 13, batch  2099] avg loss: 0.046716
[epoch 13, batch  2199] avg loss: 0.048111
[epoch 13, batch  2299] avg loss: 0.057142
[epoch 13, batch  2399] avg loss: 0.054501
[epoch 13, batch  2499] avg loss: 0.054222
[epoch 14, batch    99] avg loss: 0.047218
[epoch 14, batch   199] avg loss: 0.058552
[epoch 14, batch   299] avg loss: 0.047752
[epoch 14, batch   399] avg loss: 0.057280
[epoch 14, batch   499] avg loss: 0.051139
[epoch 14, batch   599] avg loss: 0.051290
[epoch 14, batch   699] avg loss: 0.059911
[epoch 14, batch   799] avg loss: 0.057741
[epoch 14, batch   899] avg loss: 0.051421
[epoch 14, batch   999] avg loss: 0.058535
[epoch 14, batch  1099] avg loss: 0.056170
[epoch 14, batch  1199] avg loss: 0.052330
[epoch 14, batch  1299] avg loss: 0.053240
[epoch 14, batch  1399] avg loss: 0.052467
[epoch 14, batch  1499] avg loss: 0.060775
[epoch 14, batch  1599] avg loss: 0.053044
[epoch 14, batch  1699] avg loss: 0.052872
[epoch 14, batch  1799] avg loss: 0.056367
[epoch 14, batch  1899] avg loss: 0.054373
[epoch 14, batch  1999] avg loss: 0.052069
[epoch 14, batch  2099] avg loss: 0.070309
[epoch 14, batch  2199] avg loss: 0.049114
[epoch 14, batch  2299] avg loss: 0.050881
[epoch 14, batch  2399] avg loss: 0.060393
[epoch 14, batch  2499] avg loss: 0.052103
[epoch 15, batch    99] avg loss: 0.057895
[epoch 15, batch   199] avg loss: 0.048378
[epoch 15, batch   299] avg loss: 0.045185
[epoch 15, batch   399] avg loss: 0.054941
[epoch 15, batch   499] avg loss: 0.049648
[epoch 15, batch   599] avg loss: 0.049042
[epoch 15, batch   699] avg loss: 0.061815
[epoch 15, batch   799] avg loss: 0.059445
[epoch 15, batch   899] avg loss: 0.050273
[epoch 15, batch   999] avg loss: 0.056547
[epoch 15, batch  1099] avg loss: 0.049567
[epoch 15, batch  1199] avg loss: 0.048841
[epoch 15, batch  1299] avg loss: 0.046194
[epoch 15, batch  1399] avg loss: 0.053302
[epoch 15, batch  1499] avg loss: 0.055023
[epoch 15, batch  1599] avg loss: 0.055545
[epoch 15, batch  1699] avg loss: 0.052779
[epoch 15, batch  1799] avg loss: 0.059269
[epoch 15, batch  1899] avg loss: 0.052030
[epoch 15, batch  1999] avg loss: 0.051999
[epoch 15, batch  2099] avg loss: 0.056593
[epoch 15, batch  2199] avg loss: 0.063078
[epoch 15, batch  2299] avg loss: 0.051851
[epoch 15, batch  2399] avg loss: 0.053039
[epoch 15, batch  2499] avg loss: 0.047730
[epoch 16, batch    99] avg loss: 0.054434
[epoch 16, batch   199] avg loss: 0.046244
[epoch 16, batch   299] avg loss: 0.057228
[epoch 16, batch   399] avg loss: 0.060574
[epoch 16, batch   499] avg loss: 0.051692
[epoch 16, batch   599] avg loss: 0.053502
[epoch 16, batch   699] avg loss: 0.054540
[epoch 16, batch   799] avg loss: 0.056180
[epoch 16, batch   899] avg loss: 0.049478
[epoch 16, batch   999] avg loss: 0.056060
[epoch 16, batch  1099] avg loss: 0.057726
[epoch 16, batch  1199] avg loss: 0.044952
[epoch 16, batch  1299] avg loss: 0.059340
[epoch 16, batch  1399] avg loss: 0.049440
[epoch 16, batch  1499] avg loss: 0.052238
[epoch 16, batch  1599] avg loss: 0.050230
[epoch 16, batch  1699] avg loss: 0.045679
[epoch 16, batch  1799] avg loss: 0.058547
[epoch 16, batch  1899] avg loss: 0.052708
[epoch 16, batch  1999] avg loss: 0.047943
[epoch 16, batch  2099] avg loss: 0.042414
[epoch 16, batch  2199] avg loss: 0.056693
[epoch 16, batch  2299] avg loss: 0.052289
[epoch 16, batch  2399] avg loss: 0.065624
[epoch 16, batch  2499] avg loss: 0.058910
[epoch 17, batch    99] avg loss: 0.046600
[epoch 17, batch   199] avg loss: 0.049121
[epoch 17, batch   299] avg loss: 0.056951
[epoch 17, batch   399] avg loss: 0.057786
[epoch 17, batch   499] avg loss: 0.053256
[epoch 17, batch   599] avg loss: 0.053215
[epoch 17, batch   699] avg loss: 0.049364
[epoch 17, batch   799] avg loss: 0.044956
[epoch 17, batch   899] avg loss: 0.051757
[epoch 17, batch   999] avg loss: 0.045358
[epoch 17, batch  1099] avg loss: 0.056646
[epoch 17, batch  1199] avg loss: 0.053337
[epoch 17, batch  1299] avg loss: 0.051285
[epoch 17, batch  1399] avg loss: 0.058722
[epoch 17, batch  1499] avg loss: 0.056968
[epoch 17, batch  1599] avg loss: 0.051227
[epoch 17, batch  1699] avg loss: 0.062595
[epoch 17, batch  1799] avg loss: 0.057227
[epoch 17, batch  1899] avg loss: 0.054419
[epoch 17, batch  1999] avg loss: 0.052419
[epoch 17, batch  2099] avg loss: 0.058761
[epoch 17, batch  2199] avg loss: 0.052644
[epoch 17, batch  2299] avg loss: 0.055898
[epoch 17, batch  2399] avg loss: 0.049850
[epoch 17, batch  2499] avg loss: 0.053375
[epoch 18, batch    99] avg loss: 0.048786
[epoch 18, batch   199] avg loss: 0.057190
[epoch 18, batch   299] avg loss: 0.057009
[epoch 18, batch   399] avg loss: 0.048965
[epoch 18, batch   499] avg loss: 0.049020
[epoch 18, batch   599] avg loss: 0.065127
[epoch 18, batch   699] avg loss: 0.050875
[epoch 18, batch   799] avg loss: 0.050618
[epoch 18, batch   899] avg loss: 0.062045
[epoch 18, batch   999] avg loss: 0.042976
[epoch 18, batch  1099] avg loss: 0.054406
[epoch 18, batch  1199] avg loss: 0.063741
[epoch 18, batch  1299] avg loss: 0.054651
[epoch 18, batch  1399] avg loss: 0.055386
[epoch 18, batch  1499] avg loss: 0.046873
[epoch 18, batch  1599] avg loss: 0.044607
[epoch 18, batch  1699] avg loss: 0.051915
[epoch 18, batch  1799] avg loss: 0.046299
[epoch 18, batch  1899] avg loss: 0.060412
[epoch 18, batch  1999] avg loss: 0.055666
[epoch 18, batch  2099] avg loss: 0.052711
[epoch 18, batch  2199] avg loss: 0.050698
[epoch 18, batch  2299] avg loss: 0.050606
[epoch 18, batch  2399] avg loss: 0.055073
[epoch 18, batch  2499] avg loss: 0.057410
[epoch 19, batch    99] avg loss: 0.044916
[epoch 19, batch   199] avg loss: 0.053211
[epoch 19, batch   299] avg loss: 0.060177
[epoch 19, batch   399] avg loss: 0.044682
[epoch 19, batch   499] avg loss: 0.053220
[epoch 19, batch   599] avg loss: 0.045324
[epoch 19, batch   699] avg loss: 0.045646
[epoch 19, batch   799] avg loss: 0.058650
[epoch 19, batch   899] avg loss: 0.046473
[epoch 19, batch   999] avg loss: 0.051979
[epoch 19, batch  1099] avg loss: 0.047157
[epoch 19, batch  1199] avg loss: 0.049590
[epoch 19, batch  1299] avg loss: 0.043297
[epoch 19, batch  1399] avg loss: 0.050154
[epoch 19, batch  1499] avg loss: 0.056615
[epoch 19, batch  1599] avg loss: 0.054740
[epoch 19, batch  1699] avg loss: 0.056100
[epoch 19, batch  1799] avg loss: 0.058382
[epoch 19, batch  1899] avg loss: 0.057687
[epoch 19, batch  1999] avg loss: 0.061672
[epoch 19, batch  2099] avg loss: 0.053215
[epoch 19, batch  2199] avg loss: 0.055013
[epoch 19, batch  2299] avg loss: 0.056874
[epoch 19, batch  2399] avg loss: 0.054956
[epoch 19, batch  2499] avg loss: 0.053304
Model saved to model/20200502-040922.pth.
accuracy/TriangPrismIsosc : 0.0
n_examples/TriangPrismIsosc : 1500.0
accuracy/parallelepiped : 0.52
n_examples/parallelepiped : 1500.0
accuracy/sphere : 0.4411764705882353
n_examples/sphere : 306.0
accuracy/wire : 0.115
n_examples/wire : 600.0
accuracy/avg_geom : 0.2519201228878648
loss/validation_geom : 1.976514051953036
accuracy/Au : 0.999231950844854
n_examples/Au : 1302.0
accuracy/SiN : 0.999231950844854
n_examples/SiN : 1302.0
accuracy/SiO2 : 1.0
n_examples/SiO2 : 1302.0
accuracy/avg_mat : 0.9994879672299027
loss/validation_mat : 0.0010903257302810967
MSE/ShortestDim : 0.2830638553116507
MAE/ShortestDim : 0.3207573091608405
MSE/MiddleDim : 0.691977298326878
MAE/MiddleDim : 0.5350387395496436
MSE/LongDim : 23.698442989413824
MAE/LongDim : 2.4302494060791764
MSE/log Area/Vol : 0.254100715410569
MAE/log Area/Vol : 0.3879419343629008
loss/validation_dim : 24.92758485846292
loss/validation : 26.90518923614624
Metrics saved to model/20200502-040922_metrics.csv.
[epoch 0, batch    99] avg loss: 1.151576
[epoch 0, batch   199] avg loss: 0.912979
[epoch 0, batch   299] avg loss: 0.870047
[epoch 0, batch   399] avg loss: 0.845249
[epoch 0, batch   499] avg loss: 0.820320
[epoch 0, batch   599] avg loss: 0.789210
[epoch 0, batch   699] avg loss: 0.780133
[epoch 0, batch   799] avg loss: 0.768011
[epoch 0, batch   899] avg loss: 0.750602
[epoch 0, batch   999] avg loss: 0.742363
[epoch 0, batch  1099] avg loss: 0.746318
[epoch 0, batch  1199] avg loss: 0.741551
[epoch 0, batch  1299] avg loss: 0.719490
[epoch 0, batch  1399] avg loss: 0.715148
[epoch 0, batch  1499] avg loss: 0.712830
[epoch 0, batch  1599] avg loss: 0.713546
[epoch 0, batch  1699] avg loss: 0.700351
[epoch 0, batch  1799] avg loss: 0.710821
[epoch 0, batch  1899] avg loss: 0.685526
[epoch 0, batch  1999] avg loss: 0.693867
[epoch 0, batch  2099] avg loss: 0.687427
[epoch 0, batch  2199] avg loss: 0.703960
[epoch 0, batch  2299] avg loss: 0.695620
[epoch 0, batch  2399] avg loss: 0.692496
[epoch 0, batch  2499] avg loss: 0.681729
[epoch 1, batch    99] avg loss: 0.672902
[epoch 1, batch   199] avg loss: 0.691303
[epoch 1, batch   299] avg loss: 0.700106
[epoch 1, batch   399] avg loss: 0.667997
[epoch 1, batch   499] avg loss: 0.673067
[epoch 1, batch   599] avg loss: 0.656596
[epoch 1, batch   699] avg loss: 0.668948
[epoch 1, batch   799] avg loss: 0.676491
[epoch 1, batch   899] avg loss: 0.648328
[epoch 1, batch   999] avg loss: 0.667044
[epoch 1, batch  1099] avg loss: 0.666434
[epoch 1, batch  1199] avg loss: 0.661255
[epoch 1, batch  1299] avg loss: 0.656563
[epoch 1, batch  1399] avg loss: 0.682591
[epoch 1, batch  1499] avg loss: 0.653696
[epoch 1, batch  1599] avg loss: 0.657778
[epoch 1, batch  1699] avg loss: 0.665140
[epoch 1, batch  1799] avg loss: 0.638436
[epoch 1, batch  1899] avg loss: 0.645988
[epoch 1, batch  1999] avg loss: 0.659107
[epoch 1, batch  2099] avg loss: 0.640425
[epoch 1, batch  2199] avg loss: 0.653429
[epoch 1, batch  2299] avg loss: 0.664855
[epoch 1, batch  2399] avg loss: 0.669787
[epoch 1, batch  2499] avg loss: 0.655887
[epoch 2, batch    99] avg loss: 0.654659
[epoch 2, batch   199] avg loss: 0.638989
[epoch 2, batch   299] avg loss: 0.635345
[epoch 2, batch   399] avg loss: 0.656911
[epoch 2, batch   499] avg loss: 0.648784
[epoch 2, batch   599] avg loss: 0.641254
[epoch 2, batch   699] avg loss: 0.626145
[epoch 2, batch   799] avg loss: 0.630695
[epoch 2, batch   899] avg loss: 0.648318
[epoch 2, batch   999] avg loss: 0.638205
[epoch 2, batch  1099] avg loss: 0.632069
[epoch 2, batch  1199] avg loss: 0.636619
[epoch 2, batch  1299] avg loss: 0.636322
[epoch 2, batch  1399] avg loss: 0.639734
[epoch 2, batch  1499] avg loss: 0.652905
[epoch 2, batch  1599] avg loss: 0.649658
[epoch 2, batch  1699] avg loss: 0.638845
[epoch 2, batch  1799] avg loss: 0.643848
[epoch 2, batch  1899] avg loss: 0.638945
[epoch 2, batch  1999] avg loss: 0.631631
[epoch 2, batch  2099] avg loss: 0.623223
[epoch 2, batch  2199] avg loss: 0.636115
[epoch 2, batch  2299] avg loss: 0.628547
[epoch 2, batch  2399] avg loss: 0.636818
[epoch 2, batch  2499] avg loss: 0.627167
[epoch 3, batch    99] avg loss: 0.621783
[epoch 3, batch   199] avg loss: 0.617583
[epoch 3, batch   299] avg loss: 0.642389
[epoch 3, batch   399] avg loss: 0.612998
[epoch 3, batch   499] avg loss: 0.628672
[epoch 3, batch   599] avg loss: 0.611014
[epoch 3, batch   699] avg loss: 0.615569
[epoch 3, batch   799] avg loss: 0.635699
[epoch 3, batch   899] avg loss: 0.622986
[epoch 3, batch   999] avg loss: 0.615205
[epoch 3, batch  1099] avg loss: 0.626907
[epoch 3, batch  1199] avg loss: 0.621364
[epoch 3, batch  1299] avg loss: 0.623794
[epoch 3, batch  1399] avg loss: 0.633687
[epoch 3, batch  1499] avg loss: 0.641419
[epoch 3, batch  1599] avg loss: 0.620275
[epoch 3, batch  1699] avg loss: 0.608726
[epoch 3, batch  1799] avg loss: 0.606687
[epoch 3, batch  1899] avg loss: 0.630987
[epoch 3, batch  1999] avg loss: 0.623155
[epoch 3, batch  2099] avg loss: 0.622096
[epoch 3, batch  2199] avg loss: 0.607290
[epoch 3, batch  2299] avg loss: 0.620421
[epoch 3, batch  2399] avg loss: 0.613711
[epoch 3, batch  2499] avg loss: 0.604644
[epoch 4, batch    99] avg loss: 0.626906
[epoch 4, batch   199] avg loss: 0.615833
[epoch 4, batch   299] avg loss: 0.624126
[epoch 4, batch   399] avg loss: 0.601807
[epoch 4, batch   499] avg loss: 0.618336
[epoch 4, batch   599] avg loss: 0.616198
[epoch 4, batch   699] avg loss: 0.600046
[epoch 4, batch   799] avg loss: 0.624146
[epoch 4, batch   899] avg loss: 0.629547
[epoch 4, batch   999] avg loss: 0.616570
[epoch 4, batch  1099] avg loss: 0.617712
[epoch 4, batch  1199] avg loss: 0.609291
[epoch 4, batch  1299] avg loss: 0.599964
[epoch 4, batch  1399] avg loss: 0.591319
[epoch 4, batch  1499] avg loss: 0.612139
[epoch 4, batch  1599] avg loss: 0.592383
[epoch 4, batch  1699] avg loss: 0.597822
[epoch 4, batch  1799] avg loss: 0.603218
[epoch 4, batch  1899] avg loss: 0.604820
[epoch 4, batch  1999] avg loss: 0.619878
[epoch 4, batch  2099] avg loss: 0.609036
[epoch 4, batch  2199] avg loss: 0.614716
[epoch 4, batch  2299] avg loss: 0.593078
[epoch 4, batch  2399] avg loss: 0.615621
[epoch 4, batch  2499] avg loss: 0.602888
[epoch 5, batch    99] avg loss: 0.606465
[epoch 5, batch   199] avg loss: 0.615321
[epoch 5, batch   299] avg loss: 0.610306
[epoch 5, batch   399] avg loss: 0.631287
[epoch 5, batch   499] avg loss: 0.596504
[epoch 5, batch   599] avg loss: 0.617889
[epoch 5, batch   699] avg loss: 0.596636
[epoch 5, batch   799] avg loss: 0.608024
[epoch 5, batch   899] avg loss: 0.599639
[epoch 5, batch   999] avg loss: 0.599042
[epoch 5, batch  1099] avg loss: 0.591029
[epoch 5, batch  1199] avg loss: 0.605405
[epoch 5, batch  1299] avg loss: 0.615527
[epoch 5, batch  1399] avg loss: 0.603250
[epoch 5, batch  1499] avg loss: 0.608659
[epoch 5, batch  1599] avg loss: 0.612219
[epoch 5, batch  1699] avg loss: 0.586954
[epoch 5, batch  1799] avg loss: 0.604911
[epoch 5, batch  1899] avg loss: 0.595050
[epoch 5, batch  1999] avg loss: 0.601613
[epoch 5, batch  2099] avg loss: 0.612418
[epoch 5, batch  2199] avg loss: 0.586348
[epoch 5, batch  2299] avg loss: 0.590889
[epoch 5, batch  2399] avg loss: 0.595508
[epoch 5, batch  2499] avg loss: 0.613761
[epoch 6, batch    99] avg loss: 0.616704
[epoch 6, batch   199] avg loss: 0.598263
[epoch 6, batch   299] avg loss: 0.593695
[epoch 6, batch   399] avg loss: 0.599211
[epoch 6, batch   499] avg loss: 0.593820
[epoch 6, batch   599] avg loss: 0.597599
[epoch 6, batch   699] avg loss: 0.594153
[epoch 6, batch   799] avg loss: 0.599548
[epoch 6, batch   899] avg loss: 0.606257
[epoch 6, batch   999] avg loss: 0.592050
[epoch 6, batch  1099] avg loss: 0.594784
[epoch 6, batch  1199] avg loss: 0.601729
[epoch 6, batch  1299] avg loss: 0.590489
[epoch 6, batch  1399] avg loss: 0.590929
[epoch 6, batch  1499] avg loss: 0.591230
[epoch 6, batch  1599] avg loss: 0.600307
[epoch 6, batch  1699] avg loss: 0.608908
[epoch 6, batch  1799] avg loss: 0.594132
[epoch 6, batch  1899] avg loss: 0.598317
[epoch 6, batch  1999] avg loss: 0.598928
[epoch 6, batch  2099] avg loss: 0.601831
[epoch 6, batch  2199] avg loss: 0.580265
[epoch 6, batch  2299] avg loss: 0.591838
[epoch 6, batch  2399] avg loss: 0.609468
[epoch 6, batch  2499] avg loss: 0.578410
[epoch 7, batch    99] avg loss: 0.589419
[epoch 7, batch   199] avg loss: 0.587031
[epoch 7, batch   299] avg loss: 0.608349
[epoch 7, batch   399] avg loss: 0.587174
[epoch 7, batch   499] avg loss: 0.605148
[epoch 7, batch   599] avg loss: 0.575222
[epoch 7, batch   699] avg loss: 0.591062
[epoch 7, batch   799] avg loss: 0.590790
[epoch 7, batch   899] avg loss: 0.577136
[epoch 7, batch   999] avg loss: 0.599252
[epoch 7, batch  1099] avg loss: 0.583692
[epoch 7, batch  1199] avg loss: 0.579102
[epoch 7, batch  1299] avg loss: 0.588555
[epoch 7, batch  1399] avg loss: 0.572943
[epoch 7, batch  1499] avg loss: 0.578850
[epoch 7, batch  1599] avg loss: 0.583733
[epoch 7, batch  1699] avg loss: 0.590152
[epoch 7, batch  1799] avg loss: 0.577711
[epoch 7, batch  1899] avg loss: 0.595210
[epoch 7, batch  1999] avg loss: 0.583420
[epoch 7, batch  2099] avg loss: 0.591206
[epoch 7, batch  2199] avg loss: 0.568687
[epoch 7, batch  2299] avg loss: 0.592024
[epoch 7, batch  2399] avg loss: 0.598861
[epoch 7, batch  2499] avg loss: 0.594560
[epoch 8, batch    99] avg loss: 0.580981
[epoch 8, batch   199] avg loss: 0.602711
[epoch 8, batch   299] avg loss: 0.592104
[epoch 8, batch   399] avg loss: 0.570541
[epoch 8, batch   499] avg loss: 0.581243
[epoch 8, batch   599] avg loss: 0.586239
[epoch 8, batch   699] avg loss: 0.572630
[epoch 8, batch   799] avg loss: 0.577150
[epoch 8, batch   899] avg loss: 0.595310
[epoch 8, batch   999] avg loss: 0.571533
[epoch 8, batch  1099] avg loss: 0.575882
[epoch 8, batch  1199] avg loss: 0.582014
[epoch 8, batch  1299] avg loss: 0.573370
[epoch 8, batch  1399] avg loss: 0.585064
[epoch 8, batch  1499] avg loss: 0.585927
[epoch 8, batch  1599] avg loss: 0.575468
[epoch 8, batch  1699] avg loss: 0.590974
[epoch 8, batch  1799] avg loss: 0.595496
[epoch 8, batch  1899] avg loss: 0.563997
[epoch 8, batch  1999] avg loss: 0.584785
[epoch 8, batch  2099] avg loss: 0.590991
[epoch 8, batch  2199] avg loss: 0.579937
[epoch 8, batch  2299] avg loss: 0.573642
[epoch 8, batch  2399] avg loss: 0.576429
[epoch 8, batch  2499] avg loss: 0.583686
[epoch 9, batch    99] avg loss: 0.587147
[epoch 9, batch   199] avg loss: 0.574609
[epoch 9, batch   299] avg loss: 0.578042
[epoch 9, batch   399] avg loss: 0.571206
[epoch 9, batch   499] avg loss: 0.579153
[epoch 9, batch   599] avg loss: 0.577170
[epoch 9, batch   699] avg loss: 0.570123
[epoch 9, batch   799] avg loss: 0.580104
[epoch 9, batch   899] avg loss: 0.573587
[epoch 9, batch   999] avg loss: 0.578889
[epoch 9, batch  1099] avg loss: 0.565555
[epoch 9, batch  1199] avg loss: 0.599553
[epoch 9, batch  1299] avg loss: 0.585843
[epoch 9, batch  1399] avg loss: 0.569368
[epoch 9, batch  1499] avg loss: 0.584926
[epoch 9, batch  1599] avg loss: 0.606316
[epoch 9, batch  1699] avg loss: 0.575799
[epoch 9, batch  1799] avg loss: 0.583360
[epoch 9, batch  1899] avg loss: 0.568760
[epoch 9, batch  1999] avg loss: 0.583393
[epoch 9, batch  2099] avg loss: 0.577341
[epoch 9, batch  2199] avg loss: 0.579851
[epoch 9, batch  2299] avg loss: 0.568244
[epoch 9, batch  2399] avg loss: 0.556499
[epoch 9, batch  2499] avg loss: 0.557872
[epoch 10, batch    99] avg loss: 0.578406
[epoch 10, batch   199] avg loss: 0.577659
[epoch 10, batch   299] avg loss: 0.581516
[epoch 10, batch   399] avg loss: 0.591535
[epoch 10, batch   499] avg loss: 0.551169
[epoch 10, batch   599] avg loss: 0.568314
[epoch 10, batch   699] avg loss: 0.590259
[epoch 10, batch   799] avg loss: 0.569769
[epoch 10, batch   899] avg loss: 0.567116
[epoch 10, batch   999] avg loss: 0.573029
[epoch 10, batch  1099] avg loss: 0.569208
[epoch 10, batch  1199] avg loss: 0.581270
[epoch 10, batch  1299] avg loss: 0.560524
[epoch 10, batch  1399] avg loss: 0.574920
[epoch 10, batch  1499] avg loss: 0.568076
[epoch 10, batch  1599] avg loss: 0.589423
[epoch 10, batch  1699] avg loss: 0.561925
[epoch 10, batch  1799] avg loss: 0.571918
[epoch 10, batch  1899] avg loss: 0.562424
[epoch 10, batch  1999] avg loss: 0.558661
[epoch 10, batch  2099] avg loss: 0.577294
[epoch 10, batch  2199] avg loss: 0.575133
[epoch 10, batch  2299] avg loss: 0.592237
[epoch 10, batch  2399] avg loss: 0.577133
[epoch 10, batch  2499] avg loss: 0.558723
[epoch 11, batch    99] avg loss: 0.550743
[epoch 11, batch   199] avg loss: 0.555118
[epoch 11, batch   299] avg loss: 0.555718
[epoch 11, batch   399] avg loss: 0.583850
[epoch 11, batch   499] avg loss: 0.561136
[epoch 11, batch   599] avg loss: 0.565132
[epoch 11, batch   699] avg loss: 0.583935
[epoch 11, batch   799] avg loss: 0.579831
[epoch 11, batch   899] avg loss: 0.588213
[epoch 11, batch   999] avg loss: 0.584325
[epoch 11, batch  1099] avg loss: 0.570223
[epoch 11, batch  1199] avg loss: 0.554365
[epoch 11, batch  1299] avg loss: 0.579763
[epoch 11, batch  1399] avg loss: 0.563892
[epoch 11, batch  1499] avg loss: 0.562983
[epoch 11, batch  1599] avg loss: 0.559156
[epoch 11, batch  1699] avg loss: 0.560145
[epoch 11, batch  1799] avg loss: 0.580741
[epoch 11, batch  1899] avg loss: 0.575601
[epoch 11, batch  1999] avg loss: 0.555133
[epoch 11, batch  2099] avg loss: 0.575421
[epoch 11, batch  2199] avg loss: 0.562259
[epoch 11, batch  2299] avg loss: 0.573947
[epoch 11, batch  2399] avg loss: 0.569979
[epoch 11, batch  2499] avg loss: 0.559489
[epoch 12, batch    99] avg loss: 0.559065
[epoch 12, batch   199] avg loss: 0.563652
[epoch 12, batch   299] avg loss: 0.573409
[epoch 12, batch   399] avg loss: 0.559295
[epoch 12, batch   499] avg loss: 0.553078
[epoch 12, batch   599] avg loss: 0.571906
[epoch 12, batch   699] avg loss: 0.572209
[epoch 12, batch   799] avg loss: 0.559110
[epoch 12, batch   899] avg loss: 0.556469
[epoch 12, batch   999] avg loss: 0.549791
[epoch 12, batch  1099] avg loss: 0.574933
[epoch 12, batch  1199] avg loss: 0.558126
[epoch 12, batch  1299] avg loss: 0.545011
[epoch 12, batch  1399] avg loss: 0.557170
[epoch 12, batch  1499] avg loss: 0.579530
[epoch 12, batch  1599] avg loss: 0.572178
[epoch 12, batch  1699] avg loss: 0.563827
[epoch 12, batch  1799] avg loss: 0.584003
[epoch 12, batch  1899] avg loss: 0.575221
[epoch 12, batch  1999] avg loss: 0.567099
[epoch 12, batch  2099] avg loss: 0.576844
[epoch 12, batch  2199] avg loss: 0.554763
[epoch 12, batch  2299] avg loss: 0.571987
[epoch 12, batch  2399] avg loss: 0.579686
[epoch 12, batch  2499] avg loss: 0.563515
[epoch 13, batch    99] avg loss: 0.555572
[epoch 13, batch   199] avg loss: 0.559290
[epoch 13, batch   299] avg loss: 0.570391
[epoch 13, batch   399] avg loss: 0.559424
[epoch 13, batch   499] avg loss: 0.568389
[epoch 13, batch   599] avg loss: 0.538778
[epoch 13, batch   699] avg loss: 0.554705
[epoch 13, batch   799] avg loss: 0.567251
[epoch 13, batch   899] avg loss: 0.553624
[epoch 13, batch   999] avg loss: 0.574172
[epoch 13, batch  1099] avg loss: 0.555955
[epoch 13, batch  1199] avg loss: 0.556892
[epoch 13, batch  1299] avg loss: 0.547250
[epoch 13, batch  1399] avg loss: 0.584202
[epoch 13, batch  1499] avg loss: 0.551303
[epoch 13, batch  1599] avg loss: 0.587376
[epoch 13, batch  1699] avg loss: 0.568007
[epoch 13, batch  1799] avg loss: 0.541447
[epoch 13, batch  1899] avg loss: 0.541896
[epoch 13, batch  1999] avg loss: 0.557361
[epoch 13, batch  2099] avg loss: 0.566602
[epoch 13, batch  2199] avg loss: 0.556799
[epoch 13, batch  2299] avg loss: 0.562124
[epoch 13, batch  2399] avg loss: 0.563204
[epoch 13, batch  2499] avg loss: 0.553460
[epoch 14, batch    99] avg loss: 0.575541
[epoch 14, batch   199] avg loss: 0.560776
[epoch 14, batch   299] avg loss: 0.547518
[epoch 14, batch   399] avg loss: 0.550784
[epoch 14, batch   499] avg loss: 0.558497
[epoch 14, batch   599] avg loss: 0.558268
[epoch 14, batch   699] avg loss: 0.565872
[epoch 14, batch   799] avg loss: 0.556022
[epoch 14, batch   899] avg loss: 0.550244
[epoch 14, batch   999] avg loss: 0.555067
[epoch 14, batch  1099] avg loss: 0.544191
[epoch 14, batch  1199] avg loss: 0.564507
[epoch 14, batch  1299] avg loss: 0.559994
[epoch 14, batch  1399] avg loss: 0.545860
[epoch 14, batch  1499] avg loss: 0.537916
[epoch 14, batch  1599] avg loss: 0.563931
[epoch 14, batch  1699] avg loss: 0.553992
[epoch 14, batch  1799] avg loss: 0.574283
[epoch 14, batch  1899] avg loss: 0.561872
[epoch 14, batch  1999] avg loss: 0.547150
[epoch 14, batch  2099] avg loss: 0.550725
[epoch 14, batch  2199] avg loss: 0.559187
[epoch 14, batch  2299] avg loss: 0.550967
[epoch 14, batch  2399] avg loss: 0.545328
[epoch 14, batch  2499] avg loss: 0.554006
[epoch 15, batch    99] avg loss: 0.547872
[epoch 15, batch   199] avg loss: 0.534990
[epoch 15, batch   299] avg loss: 0.541093
[epoch 15, batch   399] avg loss: 0.555618
[epoch 15, batch   499] avg loss: 0.532888
[epoch 15, batch   599] avg loss: 0.546970
[epoch 15, batch   699] avg loss: 0.557225
[epoch 15, batch   799] avg loss: 0.562780
[epoch 15, batch   899] avg loss: 0.551319
[epoch 15, batch   999] avg loss: 0.535564
[epoch 15, batch  1099] avg loss: 0.551217
[epoch 15, batch  1199] avg loss: 0.536227
[epoch 15, batch  1299] avg loss: 0.560143
[epoch 15, batch  1399] avg loss: 0.562182
[epoch 15, batch  1499] avg loss: 0.552128
[epoch 15, batch  1599] avg loss: 0.550449
[epoch 15, batch  1699] avg loss: 0.549829
[epoch 15, batch  1799] avg loss: 0.558311
[epoch 15, batch  1899] avg loss: 0.554956
[epoch 15, batch  1999] avg loss: 0.530884
[epoch 15, batch  2099] avg loss: 0.534895
[epoch 15, batch  2199] avg loss: 0.533138
[epoch 15, batch  2299] avg loss: 0.563699
[epoch 15, batch  2399] avg loss: 0.574990
[epoch 15, batch  2499] avg loss: 0.555562
[epoch 16, batch    99] avg loss: 0.538456
[epoch 16, batch   199] avg loss: 0.539698
[epoch 16, batch   299] avg loss: 0.545043
[epoch 16, batch   399] avg loss: 0.555524
[epoch 16, batch   499] avg loss: 0.550119
[epoch 16, batch   599] avg loss: 0.534914
[epoch 16, batch   699] avg loss: 0.552661
[epoch 16, batch   799] avg loss: 0.560540
[epoch 16, batch   899] avg loss: 0.535318
[epoch 16, batch   999] avg loss: 0.551683
[epoch 16, batch  1099] avg loss: 0.533916
[epoch 16, batch  1199] avg loss: 0.531811
[epoch 16, batch  1299] avg loss: 0.546495
[epoch 16, batch  1399] avg loss: 0.544133
[epoch 16, batch  1499] avg loss: 0.530862
[epoch 16, batch  1599] avg loss: 0.545066
[epoch 16, batch  1699] avg loss: 0.531535
[epoch 16, batch  1799] avg loss: 0.540683
[epoch 16, batch  1899] avg loss: 0.534961
[epoch 16, batch  1999] avg loss: 0.537481
[epoch 16, batch  2099] avg loss: 0.548403
[epoch 16, batch  2199] avg loss: 0.541430
[epoch 16, batch  2299] avg loss: 0.545678
[epoch 16, batch  2399] avg loss: 0.567726
[epoch 16, batch  2499] avg loss: 0.542757
[epoch 17, batch    99] avg loss: 0.546138
[epoch 17, batch   199] avg loss: 0.530073
[epoch 17, batch   299] avg loss: 0.538204
[epoch 17, batch   399] avg loss: 0.531598
[epoch 17, batch   499] avg loss: 0.533634
[epoch 17, batch   599] avg loss: 0.530768
[epoch 17, batch   699] avg loss: 0.524213
[epoch 17, batch   799] avg loss: 0.560697
[epoch 17, batch   899] avg loss: 0.534673
[epoch 17, batch   999] avg loss: 0.549576
[epoch 17, batch  1099] avg loss: 0.550784
[epoch 17, batch  1199] avg loss: 0.533150
[epoch 17, batch  1299] avg loss: 0.538212
[epoch 17, batch  1399] avg loss: 0.528753
[epoch 17, batch  1499] avg loss: 0.559666
[epoch 17, batch  1599] avg loss: 0.539085
[epoch 17, batch  1699] avg loss: 0.525905
[epoch 17, batch  1799] avg loss: 0.517484
[epoch 17, batch  1899] avg loss: 0.542789
[epoch 17, batch  1999] avg loss: 0.561134
[epoch 17, batch  2099] avg loss: 0.540431
[epoch 17, batch  2199] avg loss: 0.533626
[epoch 17, batch  2299] avg loss: 0.540791
[epoch 17, batch  2399] avg loss: 0.535731
[epoch 17, batch  2499] avg loss: 0.526264
[epoch 18, batch    99] avg loss: 0.546396
[epoch 18, batch   199] avg loss: 0.555524
[epoch 18, batch   299] avg loss: 0.556578
[epoch 18, batch   399] avg loss: 0.548634
[epoch 18, batch   499] avg loss: 0.546776
[epoch 18, batch   599] avg loss: 0.535330
[epoch 18, batch   699] avg loss: 0.535614
[epoch 18, batch   799] avg loss: 0.545291
[epoch 18, batch   899] avg loss: 0.512399
[epoch 18, batch   999] avg loss: 0.535505
[epoch 18, batch  1099] avg loss: 0.550562
[epoch 18, batch  1199] avg loss: 0.544731
[epoch 18, batch  1299] avg loss: 0.523183
[epoch 18, batch  1399] avg loss: 0.535149
[epoch 18, batch  1499] avg loss: 0.540451
[epoch 18, batch  1599] avg loss: 0.548259
[epoch 18, batch  1699] avg loss: 0.531907
[epoch 18, batch  1799] avg loss: 0.540523
[epoch 18, batch  1899] avg loss: 0.551672
[epoch 18, batch  1999] avg loss: 0.541072
[epoch 18, batch  2099] avg loss: 0.525436
[epoch 18, batch  2199] avg loss: 0.536798
[epoch 18, batch  2299] avg loss: 0.530666
[epoch 18, batch  2399] avg loss: 0.533768
[epoch 18, batch  2499] avg loss: 0.539832
[epoch 19, batch    99] avg loss: 0.529084
[epoch 19, batch   199] avg loss: 0.549384
[epoch 19, batch   299] avg loss: 0.547144
[epoch 19, batch   399] avg loss: 0.539710
[epoch 19, batch   499] avg loss: 0.526273
[epoch 19, batch   599] avg loss: 0.544845
[epoch 19, batch   699] avg loss: 0.552280
[epoch 19, batch   799] avg loss: 0.533207
[epoch 19, batch   899] avg loss: 0.532319
[epoch 19, batch   999] avg loss: 0.523369
[epoch 19, batch  1099] avg loss: 0.548375
[epoch 19, batch  1199] avg loss: 0.529549
[epoch 19, batch  1299] avg loss: 0.526879
[epoch 19, batch  1399] avg loss: 0.539694
[epoch 19, batch  1499] avg loss: 0.543032
[epoch 19, batch  1599] avg loss: 0.532068
[epoch 19, batch  1699] avg loss: 0.550480
[epoch 19, batch  1799] avg loss: 0.549597
[epoch 19, batch  1899] avg loss: 0.542817
[epoch 19, batch  1999] avg loss: 0.523863
[epoch 19, batch  2099] avg loss: 0.541902
[epoch 19, batch  2199] avg loss: 0.538978
[epoch 19, batch  2299] avg loss: 0.543483
[epoch 19, batch  2399] avg loss: 0.540768
[epoch 19, batch  2499] avg loss: 0.539746
Model saved to model/20200502-042610.pth.
accuracy/TriangPrismIsosc : 0.5273333333333333
n_examples/TriangPrismIsosc : 1500.0
accuracy/parallelepiped : 0.43466666666666665
n_examples/parallelepiped : 1500.0
accuracy/sphere : 0.9967320261437909
n_examples/sphere : 306.0
accuracy/wire : 0.89
n_examples/wire : 600.0
accuracy/avg_geom : 0.5842293906810035
loss/validation_geom : 0.8348786435063777
accuracy/Au : 0.1912442396313364
n_examples/Au : 1302.0
accuracy/SiN : 0.6274961597542242
n_examples/SiN : 1302.0
accuracy/SiO2 : 0.38325652841781876
n_examples/SiO2 : 1302.0
accuracy/avg_mat : 0.40066564260112647
loss/validation_mat : 2.7976377434490645
MSE/ShortestDim : 19.446536191353356
MAE/ShortestDim : 1.5562810048163396
MSE/MiddleDim : 15.458806722028648
MAE/MiddleDim : 2.6941474243365247
MSE/LongDim : 125.43272053359169
MAE/LongDim : 6.187813475201573
MSE/log Area/Vol : 5.583146047909566
MAE/log Area/Vol : 1.5456855044386903
loss/validation_dim : 165.92120949488327
loss/validation : 169.55372588183872
Metrics saved to model/20200502-042610_metrics.csv.
Parsed 2604 rows from data/sim_train_spectrum_Au.
Parsed 2604 rows from data/sim_train_labels_Au.
Parsed 9765 rows from data/gen_spectrum_Au_00-of-16.
Parsed 9765 rows from data/gen_labels_Au_00-of-16.
Parsed 9765 rows from data/gen_spectrum_Au_01-of-16.
Parsed 9765 rows from data/gen_labels_Au_01-of-16.
Parsed 9765 rows from data/gen_spectrum_Au_02-of-16.
Parsed 9765 rows from data/gen_labels_Au_02-of-16.
Parsed 9765 rows from data/gen_spectrum_Au_03-of-16.
Parsed 9765 rows from data/gen_labels_Au_03-of-16.
Parsed 9765 rows from data/gen_spectrum_Au_04-of-16.
Parsed 9765 rows from data/gen_labels_Au_04-of-16.
Parsed 9765 rows from data/gen_spectrum_Au_05-of-16.
Parsed 9765 rows from data/gen_labels_Au_05-of-16.
Parsed 9765 rows from data/gen_spectrum_Au_06-of-16.
Parsed 9765 rows from data/gen_labels_Au_06-of-16.
Parsed 9765 rows from data/gen_spectrum_Au_07-of-16.
Parsed 9765 rows from data/gen_labels_Au_07-of-16.
Parsed 9765 rows from data/gen_spectrum_Au_08-of-16.
Parsed 9765 rows from data/gen_labels_Au_08-of-16.
Parsed 9765 rows from data/gen_spectrum_Au_09-of-16.
Parsed 9765 rows from data/gen_labels_Au_09-of-16.
Parsed 9765 rows from data/gen_spectrum_Au_10-of-16.
Parsed 9765 rows from data/gen_labels_Au_10-of-16.
Parsed 9765 rows from data/gen_spectrum_Au_11-of-16.
Parsed 9765 rows from data/gen_labels_Au_11-of-16.
Parsed 9765 rows from data/gen_spectrum_Au_12-of-16.
Parsed 9765 rows from data/gen_labels_Au_12-of-16.
Parsed 9765 rows from data/gen_spectrum_Au_13-of-16.
Parsed 9765 rows from data/gen_labels_Au_13-of-16.
Parsed 9765 rows from data/gen_spectrum_Au_14-of-16.
Parsed 9765 rows from data/gen_labels_Au_14-of-16.
Parsed 9765 rows from data/gen_spectrum_Au_15-of-16.
Parsed 9765 rows from data/gen_labels_Au_15-of-16.
Parsed 1302 rows from data/sim_validation_spectrum_Au.
Parsed 1302 rows from data/sim_validation_labels_Au.
Logging training progress to tensorboard dir runs/alexnet-Au-lr_0.000500-trainsize_158844-05_02_2020_04:27-multistage-joint.
[epoch 0, batch    99] avg loss: 0.380945
[epoch 0, batch   199] avg loss: 0.133457
[epoch 0, batch   299] avg loss: 0.056692
[epoch 0, batch   399] avg loss: 0.043900
[epoch 0, batch   499] avg loss: 0.048655
[epoch 0, batch   599] avg loss: 0.038864
[epoch 0, batch   699] avg loss: 0.039423
[epoch 0, batch   799] avg loss: 0.031253
[epoch 0, batch   899] avg loss: 0.030289
[epoch 0, batch   999] avg loss: 0.024471
[epoch 0, batch  1099] avg loss: 0.026951
[epoch 0, batch  1199] avg loss: 0.028535
[epoch 0, batch  1299] avg loss: 0.028479
[epoch 0, batch  1399] avg loss: 0.026208
[epoch 0, batch  1499] avg loss: 0.023791
[epoch 0, batch  1599] avg loss: 0.024116
[epoch 0, batch  1699] avg loss: 0.025735
[epoch 0, batch  1799] avg loss: 0.026856
[epoch 0, batch  1899] avg loss: 0.022713
[epoch 0, batch  1999] avg loss: 0.019645
[epoch 0, batch  2099] avg loss: 0.022540
[epoch 0, batch  2199] avg loss: 0.020618
[epoch 0, batch  2299] avg loss: 0.021078
[epoch 0, batch  2399] avg loss: 0.020247
[epoch 1, batch    99] avg loss: 0.017997
[epoch 1, batch   199] avg loss: 0.018624
[epoch 1, batch   299] avg loss: 0.023903
[epoch 1, batch   399] avg loss: 0.023664
[epoch 1, batch   499] avg loss: 0.018700
[epoch 1, batch   599] avg loss: 0.016638
[epoch 1, batch   699] avg loss: 0.017208
[epoch 1, batch   799] avg loss: 0.015616
[epoch 1, batch   899] avg loss: 0.017753
[epoch 1, batch   999] avg loss: 0.015545
[epoch 1, batch  1099] avg loss: 0.017114
[epoch 1, batch  1199] avg loss: 0.017748
[epoch 1, batch  1299] avg loss: 0.016683
[epoch 1, batch  1399] avg loss: 0.016785
[epoch 1, batch  1499] avg loss: 0.014333
[epoch 1, batch  1599] avg loss: 0.016179
[epoch 1, batch  1699] avg loss: 0.018132
[epoch 1, batch  1799] avg loss: 0.014367
[epoch 1, batch  1899] avg loss: 0.017268
[epoch 1, batch  1999] avg loss: 0.015827
[epoch 1, batch  2099] avg loss: 0.018443
[epoch 1, batch  2199] avg loss: 0.014902
[epoch 1, batch  2299] avg loss: 0.014296
[epoch 1, batch  2399] avg loss: 0.015191
[epoch 2, batch    99] avg loss: 0.019283
[epoch 2, batch   199] avg loss: 0.017724
[epoch 2, batch   299] avg loss: 0.015526
[epoch 2, batch   399] avg loss: 0.011782
[epoch 2, batch   499] avg loss: 0.013381
[epoch 2, batch   599] avg loss: 0.012986
[epoch 2, batch   699] avg loss: 0.013303
[epoch 2, batch   799] avg loss: 0.014707
[epoch 2, batch   899] avg loss: 0.013813
[epoch 2, batch   999] avg loss: 0.013612
[epoch 2, batch  1099] avg loss: 0.016334
[epoch 2, batch  1199] avg loss: 0.012607
[epoch 2, batch  1299] avg loss: 0.015003
[epoch 2, batch  1399] avg loss: 0.012063
[epoch 2, batch  1499] avg loss: 0.013375
[epoch 2, batch  1599] avg loss: 0.016169
[epoch 2, batch  1699] avg loss: 0.010294
[epoch 2, batch  1799] avg loss: 0.013211
[epoch 2, batch  1899] avg loss: 0.011295
[epoch 2, batch  1999] avg loss: 0.014067
[epoch 2, batch  2099] avg loss: 0.010572
[epoch 2, batch  2199] avg loss: 0.011899
[epoch 2, batch  2299] avg loss: 0.009990
[epoch 2, batch  2399] avg loss: 0.011933
[epoch 3, batch    99] avg loss: 0.013664
[epoch 3, batch   199] avg loss: 0.011282
[epoch 3, batch   299] avg loss: 0.012073
[epoch 3, batch   399] avg loss: 0.013287
[epoch 3, batch   499] avg loss: 0.012063
[epoch 3, batch   599] avg loss: 0.010205
[epoch 3, batch   699] avg loss: 0.011559
[epoch 3, batch   799] avg loss: 0.011610
[epoch 3, batch   899] avg loss: 0.011730
[epoch 3, batch   999] avg loss: 0.010566
[epoch 3, batch  1099] avg loss: 0.012140
[epoch 3, batch  1199] avg loss: 0.010872
[epoch 3, batch  1299] avg loss: 0.010340
[epoch 3, batch  1399] avg loss: 0.009658
[epoch 3, batch  1499] avg loss: 0.010770
[epoch 3, batch  1599] avg loss: 0.009944
[epoch 3, batch  1699] avg loss: 0.010986
[epoch 3, batch  1799] avg loss: 0.008541
[epoch 3, batch  1899] avg loss: 0.011045
[epoch 3, batch  1999] avg loss: 0.012025
[epoch 3, batch  2099] avg loss: 0.011096
[epoch 3, batch  2199] avg loss: 0.011067
[epoch 3, batch  2299] avg loss: 0.010564
[epoch 3, batch  2399] avg loss: 0.011371
[epoch 4, batch    99] avg loss: 0.009356
[epoch 4, batch   199] avg loss: 0.011109
[epoch 4, batch   299] avg loss: 0.008617
[epoch 4, batch   399] avg loss: 0.010790
[epoch 4, batch   499] avg loss: 0.011390
[epoch 4, batch   599] avg loss: 0.008605
[epoch 4, batch   699] avg loss: 0.011169
[epoch 4, batch   799] avg loss: 0.010604
[epoch 4, batch   899] avg loss: 0.012273
[epoch 4, batch   999] avg loss: 0.008915
[epoch 4, batch  1099] avg loss: 0.008905
[epoch 4, batch  1199] avg loss: 0.008578
[epoch 4, batch  1299] avg loss: 0.009923
[epoch 4, batch  1399] avg loss: 0.010439
[epoch 4, batch  1499] avg loss: 0.009216
[epoch 4, batch  1599] avg loss: 0.008593
[epoch 4, batch  1699] avg loss: 0.006766
[epoch 4, batch  1799] avg loss: 0.010768
[epoch 4, batch  1899] avg loss: 0.009282
[epoch 4, batch  1999] avg loss: 0.008931
[epoch 4, batch  2099] avg loss: 0.009793
[epoch 4, batch  2199] avg loss: 0.010527
[epoch 4, batch  2299] avg loss: 0.009690
[epoch 4, batch  2399] avg loss: 0.010057
[epoch 5, batch    99] avg loss: 0.011918
[epoch 5, batch   199] avg loss: 0.011218
[epoch 5, batch   299] avg loss: 0.011602
[epoch 5, batch   399] avg loss: 0.008960
[epoch 5, batch   499] avg loss: 0.008805
[epoch 5, batch   599] avg loss: 0.009218
[epoch 5, batch   699] avg loss: 0.009153
[epoch 5, batch   799] avg loss: 0.009361
[epoch 5, batch   899] avg loss: 0.008016
[epoch 5, batch   999] avg loss: 0.009536
[epoch 5, batch  1099] avg loss: 0.008630
[epoch 5, batch  1199] avg loss: 0.008863
[epoch 5, batch  1299] avg loss: 0.007128
[epoch 5, batch  1399] avg loss: 0.008327
[epoch 5, batch  1499] avg loss: 0.008620
[epoch 5, batch  1599] avg loss: 0.007353
[epoch 5, batch  1699] avg loss: 0.007209
[epoch 5, batch  1799] avg loss: 0.007666
[epoch 5, batch  1899] avg loss: 0.009464
[epoch 5, batch  1999] avg loss: 0.009697
[epoch 5, batch  2099] avg loss: 0.009671
[epoch 5, batch  2199] avg loss: 0.008253
[epoch 5, batch  2299] avg loss: 0.010050
[epoch 5, batch  2399] avg loss: 0.009643
[epoch 6, batch    99] avg loss: 0.009162
[epoch 6, batch   199] avg loss: 0.007601
[epoch 6, batch   299] avg loss: 0.007670
[epoch 6, batch   399] avg loss: 0.007355
[epoch 6, batch   499] avg loss: 0.009846
[epoch 6, batch   599] avg loss: 0.007323
[epoch 6, batch   699] avg loss: 0.007562
[epoch 6, batch   799] avg loss: 0.007620
[epoch 6, batch   899] avg loss: 0.009148
[epoch 6, batch   999] avg loss: 0.010091
[epoch 6, batch  1099] avg loss: 0.007906
[epoch 6, batch  1199] avg loss: 0.008527
[epoch 6, batch  1299] avg loss: 0.007515
[epoch 6, batch  1399] avg loss: 0.007249
[epoch 6, batch  1499] avg loss: 0.008524
[epoch 6, batch  1599] avg loss: 0.009313
[epoch 6, batch  1699] avg loss: 0.009777
[epoch 6, batch  1799] avg loss: 0.006910
[epoch 6, batch  1899] avg loss: 0.006530
[epoch 6, batch  1999] avg loss: 0.007939
[epoch 6, batch  2099] avg loss: 0.007206
[epoch 6, batch  2199] avg loss: 0.008424
[epoch 6, batch  2299] avg loss: 0.010906
[epoch 6, batch  2399] avg loss: 0.008721
[epoch 7, batch    99] avg loss: 0.007722
[epoch 7, batch   199] avg loss: 0.008546
[epoch 7, batch   299] avg loss: 0.007943
[epoch 7, batch   399] avg loss: 0.009853
[epoch 7, batch   499] avg loss: 0.008393
[epoch 7, batch   599] avg loss: 0.007280
[epoch 7, batch   699] avg loss: 0.007148
[epoch 7, batch   799] avg loss: 0.007096
[epoch 7, batch   899] avg loss: 0.008065
[epoch 7, batch   999] avg loss: 0.006072
[epoch 7, batch  1099] avg loss: 0.008127
[epoch 7, batch  1199] avg loss: 0.006485
[epoch 7, batch  1299] avg loss: 0.007370
[epoch 7, batch  1399] avg loss: 0.005753
[epoch 7, batch  1499] avg loss: 0.006693
[epoch 7, batch  1599] avg loss: 0.008004
[epoch 7, batch  1699] avg loss: 0.007939
[epoch 7, batch  1799] avg loss: 0.007650
[epoch 7, batch  1899] avg loss: 0.007643
[epoch 7, batch  1999] avg loss: 0.007800
[epoch 7, batch  2099] avg loss: 0.007635
[epoch 7, batch  2199] avg loss: 0.010296
[epoch 7, batch  2299] avg loss: 0.007559
[epoch 7, batch  2399] avg loss: 0.006816
[epoch 8, batch    99] avg loss: 0.006995
[epoch 8, batch   199] avg loss: 0.007936
[epoch 8, batch   299] avg loss: 0.006277
[epoch 8, batch   399] avg loss: 0.009277
[epoch 8, batch   499] avg loss: 0.008254
[epoch 8, batch   599] avg loss: 0.006876
[epoch 8, batch   699] avg loss: 0.007449
[epoch 8, batch   799] avg loss: 0.006117
[epoch 8, batch   899] avg loss: 0.007069
[epoch 8, batch   999] avg loss: 0.005829
[epoch 8, batch  1099] avg loss: 0.006870
[epoch 8, batch  1199] avg loss: 0.008822
[epoch 8, batch  1299] avg loss: 0.008951
[epoch 8, batch  1399] avg loss: 0.008516
[epoch 8, batch  1499] avg loss: 0.006461
[epoch 8, batch  1599] avg loss: 0.007495
[epoch 8, batch  1699] avg loss: 0.006797
[epoch 8, batch  1799] avg loss: 0.007820
[epoch 8, batch  1899] avg loss: 0.006002
[epoch 8, batch  1999] avg loss: 0.007690
[epoch 8, batch  2099] avg loss: 0.009252
[epoch 8, batch  2199] avg loss: 0.005761
[epoch 8, batch  2299] avg loss: 0.007534
[epoch 8, batch  2399] avg loss: 0.006223
[epoch 9, batch    99] avg loss: 0.006214
[epoch 9, batch   199] avg loss: 0.007160
[epoch 9, batch   299] avg loss: 0.006438
[epoch 9, batch   399] avg loss: 0.007210
[epoch 9, batch   499] avg loss: 0.008209
[epoch 9, batch   599] avg loss: 0.004618
[epoch 9, batch   699] avg loss: 0.005824
[epoch 9, batch   799] avg loss: 0.006153
[epoch 9, batch   899] avg loss: 0.006791
[epoch 9, batch   999] avg loss: 0.006707
[epoch 9, batch  1099] avg loss: 0.006591
[epoch 9, batch  1199] avg loss: 0.007808
[epoch 9, batch  1299] avg loss: 0.004826
[epoch 9, batch  1399] avg loss: 0.006278
[epoch 9, batch  1499] avg loss: 0.006489
[epoch 9, batch  1599] avg loss: 0.007181
[epoch 9, batch  1699] avg loss: 0.010248
[epoch 9, batch  1799] avg loss: 0.008314
[epoch 9, batch  1899] avg loss: 0.007090
[epoch 9, batch  1999] avg loss: 0.006841
[epoch 9, batch  2099] avg loss: 0.006874
[epoch 9, batch  2199] avg loss: 0.007387
[epoch 9, batch  2299] avg loss: 0.007649
[epoch 9, batch  2399] avg loss: 0.006317
[epoch 10, batch    99] avg loss: 0.007040
[epoch 10, batch   199] avg loss: 0.007003
[epoch 10, batch   299] avg loss: 0.008420
[epoch 10, batch   399] avg loss: 0.007072
[epoch 10, batch   499] avg loss: 0.007359
[epoch 10, batch   599] avg loss: 0.006252
[epoch 10, batch   699] avg loss: 0.006671
[epoch 10, batch   799] avg loss: 0.006146
[epoch 10, batch   899] avg loss: 0.007768
[epoch 10, batch   999] avg loss: 0.008829
[epoch 10, batch  1099] avg loss: 0.008067
[epoch 10, batch  1199] avg loss: 0.007155
[epoch 10, batch  1299] avg loss: 0.006158
[epoch 10, batch  1399] avg loss: 0.007657
[epoch 10, batch  1499] avg loss: 0.007044
[epoch 10, batch  1599] avg loss: 0.006624
[epoch 10, batch  1699] avg loss: 0.006483
[epoch 10, batch  1799] avg loss: 0.005765
[epoch 10, batch  1899] avg loss: 0.007213
[epoch 10, batch  1999] avg loss: 0.005080
[epoch 10, batch  2099] avg loss: 0.006419
[epoch 10, batch  2199] avg loss: 0.006657
[epoch 10, batch  2299] avg loss: 0.004990
[epoch 10, batch  2399] avg loss: 0.005979
[epoch 11, batch    99] avg loss: 0.006663
[epoch 11, batch   199] avg loss: 0.005584
[epoch 11, batch   299] avg loss: 0.007115
[epoch 11, batch   399] avg loss: 0.006158
[epoch 11, batch   499] avg loss: 0.005443
[epoch 11, batch   599] avg loss: 0.006522
[epoch 11, batch   699] avg loss: 0.006232
[epoch 11, batch   799] avg loss: 0.006468
[epoch 11, batch   899] avg loss: 0.006506
[epoch 11, batch   999] avg loss: 0.008644
[epoch 11, batch  1099] avg loss: 0.006532
[epoch 11, batch  1199] avg loss: 0.008127
[epoch 11, batch  1299] avg loss: 0.007099
[epoch 11, batch  1399] avg loss: 0.005971
[epoch 11, batch  1499] avg loss: 0.005920
[epoch 11, batch  1599] avg loss: 0.006795
[epoch 11, batch  1699] avg loss: 0.006934
[epoch 11, batch  1799] avg loss: 0.006080
[epoch 11, batch  1899] avg loss: 0.007209
[epoch 11, batch  1999] avg loss: 0.005719
[epoch 11, batch  2099] avg loss: 0.006981
[epoch 11, batch  2199] avg loss: 0.006635
[epoch 11, batch  2299] avg loss: 0.007470
[epoch 11, batch  2399] avg loss: 0.004731
[epoch 12, batch    99] avg loss: 0.006545
[epoch 12, batch   199] avg loss: 0.006001
[epoch 12, batch   299] avg loss: 0.005250
[epoch 12, batch   399] avg loss: 0.005122
[epoch 12, batch   499] avg loss: 0.006707
[epoch 12, batch   599] avg loss: 0.006191
[epoch 12, batch   699] avg loss: 0.005541
[epoch 12, batch   799] avg loss: 0.006332
[epoch 12, batch   899] avg loss: 0.004579
[epoch 12, batch   999] avg loss: 0.006296
[epoch 12, batch  1099] avg loss: 0.005613
[epoch 12, batch  1199] avg loss: 0.006428
[epoch 12, batch  1299] avg loss: 0.006520
[epoch 12, batch  1399] avg loss: 0.005351
[epoch 12, batch  1499] avg loss: 0.006808
[epoch 12, batch  1599] avg loss: 0.005674
[epoch 12, batch  1699] avg loss: 0.006183
[epoch 12, batch  1799] avg loss: 0.006636
[epoch 12, batch  1899] avg loss: 0.005251
[epoch 12, batch  1999] avg loss: 0.007647
[epoch 12, batch  2099] avg loss: 0.007190
[epoch 12, batch  2199] avg loss: 0.008372
[epoch 12, batch  2299] avg loss: 0.005225
[epoch 12, batch  2399] avg loss: 0.005077
[epoch 13, batch    99] avg loss: 0.007512
[epoch 13, batch   199] avg loss: 0.007238
[epoch 13, batch   299] avg loss: 0.005563
[epoch 13, batch   399] avg loss: 0.004768
[epoch 13, batch   499] avg loss: 0.004884
[epoch 13, batch   599] avg loss: 0.005164
[epoch 13, batch   699] avg loss: 0.007121
[epoch 13, batch   799] avg loss: 0.006389
[epoch 13, batch   899] avg loss: 0.005753
[epoch 13, batch   999] avg loss: 0.005079
[epoch 13, batch  1099] avg loss: 0.006540
[epoch 13, batch  1199] avg loss: 0.004629
[epoch 13, batch  1299] avg loss: 0.006125
[epoch 13, batch  1399] avg loss: 0.006115
[epoch 13, batch  1499] avg loss: 0.005218
[epoch 13, batch  1599] avg loss: 0.007115
[epoch 13, batch  1699] avg loss: 0.006654
[epoch 13, batch  1799] avg loss: 0.005760
[epoch 13, batch  1899] avg loss: 0.005996
[epoch 13, batch  1999] avg loss: 0.004619
[epoch 13, batch  2099] avg loss: 0.007369
[epoch 13, batch  2199] avg loss: 0.006800
[epoch 13, batch  2299] avg loss: 0.007044
[epoch 13, batch  2399] avg loss: 0.006571
[epoch 14, batch    99] avg loss: 0.005410
[epoch 14, batch   199] avg loss: 0.005716
[epoch 14, batch   299] avg loss: 0.006011
[epoch 14, batch   399] avg loss: 0.005771
[epoch 14, batch   499] avg loss: 0.006017
[epoch 14, batch   599] avg loss: 0.006021
[epoch 14, batch   699] avg loss: 0.005411
[epoch 14, batch   799] avg loss: 0.007174
[epoch 14, batch   899] avg loss: 0.006916
[epoch 14, batch   999] avg loss: 0.007017
[epoch 14, batch  1099] avg loss: 0.006378
[epoch 14, batch  1199] avg loss: 0.006489
[epoch 14, batch  1299] avg loss: 0.006073
[epoch 14, batch  1399] avg loss: 0.006001
[epoch 14, batch  1499] avg loss: 0.005685
[epoch 14, batch  1599] avg loss: 0.005913
[epoch 14, batch  1699] avg loss: 0.004126
[epoch 14, batch  1799] avg loss: 0.005805
[epoch 14, batch  1899] avg loss: 0.005744
[epoch 14, batch  1999] avg loss: 0.006315
[epoch 14, batch  2099] avg loss: 0.006203
[epoch 14, batch  2199] avg loss: 0.006647
[epoch 14, batch  2299] avg loss: 0.004201
[epoch 14, batch  2399] avg loss: 0.006823
[epoch 15, batch    99] avg loss: 0.006228
[epoch 15, batch   199] avg loss: 0.006815
[epoch 15, batch   299] avg loss: 0.004230
[epoch 15, batch   399] avg loss: 0.005685
[epoch 15, batch   499] avg loss: 0.004429
[epoch 15, batch   599] avg loss: 0.006123
[epoch 15, batch   699] avg loss: 0.005790
[epoch 15, batch   799] avg loss: 0.007127
[epoch 15, batch   899] avg loss: 0.007088
[epoch 15, batch   999] avg loss: 0.005084
[epoch 15, batch  1099] avg loss: 0.005325
[epoch 15, batch  1199] avg loss: 0.006825
[epoch 15, batch  1299] avg loss: 0.006958
[epoch 15, batch  1399] avg loss: 0.005190
[epoch 15, batch  1499] avg loss: 0.005091
[epoch 15, batch  1599] avg loss: 0.004960
[epoch 15, batch  1699] avg loss: 0.005654
[epoch 15, batch  1799] avg loss: 0.005214
[epoch 15, batch  1899] avg loss: 0.005137
[epoch 15, batch  1999] avg loss: 0.005065
[epoch 15, batch  2099] avg loss: 0.006167
[epoch 15, batch  2199] avg loss: 0.005975
[epoch 15, batch  2299] avg loss: 0.005568
[epoch 15, batch  2399] avg loss: 0.004751
[epoch 16, batch    99] avg loss: 0.004799
[epoch 16, batch   199] avg loss: 0.005071
[epoch 16, batch   299] avg loss: 0.005729
[epoch 16, batch   399] avg loss: 0.005369
[epoch 16, batch   499] avg loss: 0.005540
[epoch 16, batch   599] avg loss: 0.004707
[epoch 16, batch   699] avg loss: 0.005001
[epoch 16, batch   799] avg loss: 0.006638
[epoch 16, batch   899] avg loss: 0.005852
[epoch 16, batch   999] avg loss: 0.005186
[epoch 16, batch  1099] avg loss: 0.005733
[epoch 16, batch  1199] avg loss: 0.006307
[epoch 16, batch  1299] avg loss: 0.005737
[epoch 16, batch  1399] avg loss: 0.005125
[epoch 16, batch  1499] avg loss: 0.005201
[epoch 16, batch  1599] avg loss: 0.004788
[epoch 16, batch  1699] avg loss: 0.007160
[epoch 16, batch  1799] avg loss: 0.005458
[epoch 16, batch  1899] avg loss: 0.006126
[epoch 16, batch  1999] avg loss: 0.005771
[epoch 16, batch  2099] avg loss: 0.005861
[epoch 16, batch  2199] avg loss: 0.006147
[epoch 16, batch  2299] avg loss: 0.005794
[epoch 16, batch  2399] avg loss: 0.004611
[epoch 17, batch    99] avg loss: 0.006437
[epoch 17, batch   199] avg loss: 0.006454
[epoch 17, batch   299] avg loss: 0.005509
[epoch 17, batch   399] avg loss: 0.005498
[epoch 17, batch   499] avg loss: 0.004647
[epoch 17, batch   599] avg loss: 0.005428
[epoch 17, batch   699] avg loss: 0.005641
[epoch 17, batch   799] avg loss: 0.004844
[epoch 17, batch   899] avg loss: 0.006316
[epoch 17, batch   999] avg loss: 0.004043
[epoch 17, batch  1099] avg loss: 0.006619
[epoch 17, batch  1199] avg loss: 0.005216
[epoch 17, batch  1299] avg loss: 0.004066
[epoch 17, batch  1399] avg loss: 0.005428
[epoch 17, batch  1499] avg loss: 0.006208
[epoch 17, batch  1599] avg loss: 0.006018
[epoch 17, batch  1699] avg loss: 0.006310
[epoch 17, batch  1799] avg loss: 0.004846
[epoch 17, batch  1899] avg loss: 0.005309
[epoch 17, batch  1999] avg loss: 0.005193
[epoch 17, batch  2099] avg loss: 0.005413
[epoch 17, batch  2199] avg loss: 0.005621
[epoch 17, batch  2299] avg loss: 0.005674
[epoch 17, batch  2399] avg loss: 0.006186
[epoch 18, batch    99] avg loss: 0.005349
[epoch 18, batch   199] avg loss: 0.005013
[epoch 18, batch   299] avg loss: 0.006152
[epoch 18, batch   399] avg loss: 0.005620
[epoch 18, batch   499] avg loss: 0.005323
[epoch 18, batch   599] avg loss: 0.004872
[epoch 18, batch   699] avg loss: 0.005095
[epoch 18, batch   799] avg loss: 0.005198
[epoch 18, batch   899] avg loss: 0.004109
[epoch 18, batch   999] avg loss: 0.005992
[epoch 18, batch  1099] avg loss: 0.005066
[epoch 18, batch  1199] avg loss: 0.005664
[epoch 18, batch  1299] avg loss: 0.005708
[epoch 18, batch  1399] avg loss: 0.004164
[epoch 18, batch  1499] avg loss: 0.004548
[epoch 18, batch  1599] avg loss: 0.006524
[epoch 18, batch  1699] avg loss: 0.005517
[epoch 18, batch  1799] avg loss: 0.007169
[epoch 18, batch  1899] avg loss: 0.004868
[epoch 18, batch  1999] avg loss: 0.006486
[epoch 18, batch  2099] avg loss: 0.006101
[epoch 18, batch  2199] avg loss: 0.004646
[epoch 18, batch  2299] avg loss: 0.005127
[epoch 18, batch  2399] avg loss: 0.005076
[epoch 19, batch    99] avg loss: 0.004732
[epoch 19, batch   199] avg loss: 0.005502
[epoch 19, batch   299] avg loss: 0.005503
[epoch 19, batch   399] avg loss: 0.005594
[epoch 19, batch   499] avg loss: 0.005483
[epoch 19, batch   599] avg loss: 0.007033
[epoch 19, batch   699] avg loss: 0.005129
[epoch 19, batch   799] avg loss: 0.005075
[epoch 19, batch   899] avg loss: 0.005159
[epoch 19, batch   999] avg loss: 0.005322
[epoch 19, batch  1099] avg loss: 0.005234
[epoch 19, batch  1199] avg loss: 0.004672
[epoch 19, batch  1299] avg loss: 0.004991
[epoch 19, batch  1399] avg loss: 0.005485
[epoch 19, batch  1499] avg loss: 0.005796
[epoch 19, batch  1599] avg loss: 0.005475
[epoch 19, batch  1699] avg loss: 0.005057
[epoch 19, batch  1799] avg loss: 0.004726
[epoch 19, batch  1899] avg loss: 0.005132
[epoch 19, batch  1999] avg loss: 0.003844
[epoch 19, batch  2099] avg loss: 0.004741
[epoch 19, batch  2199] avg loss: 0.004962
[epoch 19, batch  2299] avg loss: 0.005869
[epoch 19, batch  2399] avg loss: 0.004499
Model saved to model/20200502-044309.pth.
accuracy/TriangPrismIsosc : 0.51
n_examples/TriangPrismIsosc : 500.0
accuracy/parallelepiped : 0.038
n_examples/parallelepiped : 500.0
accuracy/sphere : 0.8725490196078431
n_examples/sphere : 102.0
accuracy/wire : 0.025
n_examples/wire : 200.0
accuracy/avg_geom : 0.282642089093702
loss/validation_geom : 1.4486477744927237
accuracy/Au : 1.0
n_examples/Au : 1302.0
accuracy/SiN : 0.0
n_examples/SiN : 0.0
accuracy/SiO2 : 0.0
n_examples/SiO2 : 0.0
accuracy/avg_mat : 1.0
loss/validation_mat : 0.0
MSE/ShortestDim : 0.1845834284517256
MAE/ShortestDim : 0.23312951818955477
MSE/MiddleDim : 0.2820429754330449
MAE/MiddleDim : 0.3247700301549768
MSE/LongDim : 1.0204865372126004
MAE/LongDim : 0.5534618644304173
MSE/log Area/Vol : 0.15999947165563908
MAE/log Area/Vol : 0.298798645330098
loss/validation_dim : 1.64711241275301
loss/validation : 3.095760187245734
Metrics saved to model/20200502-044309_metrics.csv.
[epoch 0, batch    99] avg loss: 1.165295
[epoch 0, batch   199] avg loss: 0.943091
[epoch 0, batch   299] avg loss: 0.873919
[epoch 0, batch   399] avg loss: 0.865689
[epoch 0, batch   499] avg loss: 0.815507
[epoch 0, batch   599] avg loss: 0.846667
[epoch 0, batch   699] avg loss: 0.811006
[epoch 0, batch   799] avg loss: 0.797949
[epoch 0, batch   899] avg loss: 0.802638
[epoch 0, batch   999] avg loss: 0.795434
[epoch 0, batch  1099] avg loss: 0.767652
[epoch 0, batch  1199] avg loss: 0.800796
[epoch 0, batch  1299] avg loss: 0.770169
[epoch 0, batch  1399] avg loss: 0.768041
[epoch 0, batch  1499] avg loss: 0.786994
[epoch 0, batch  1599] avg loss: 0.727801
[epoch 0, batch  1699] avg loss: 0.764282
[epoch 0, batch  1799] avg loss: 0.767244
[epoch 0, batch  1899] avg loss: 0.728830
[epoch 0, batch  1999] avg loss: 0.726155
[epoch 0, batch  2099] avg loss: 0.731700
[epoch 0, batch  2199] avg loss: 0.784830
[epoch 0, batch  2299] avg loss: 0.732687
[epoch 0, batch  2399] avg loss: 0.746846
[epoch 1, batch    99] avg loss: 0.738889
[epoch 1, batch   199] avg loss: 0.741422
[epoch 1, batch   299] avg loss: 0.692220
[epoch 1, batch   399] avg loss: 0.727649
[epoch 1, batch   499] avg loss: 0.713225
[epoch 1, batch   599] avg loss: 0.719878
[epoch 1, batch   699] avg loss: 0.734964
[epoch 1, batch   799] avg loss: 0.698642
[epoch 1, batch   899] avg loss: 0.700134
[epoch 1, batch   999] avg loss: 0.709388
[epoch 1, batch  1099] avg loss: 0.692463
[epoch 1, batch  1199] avg loss: 0.714089
[epoch 1, batch  1299] avg loss: 0.689633
[epoch 1, batch  1399] avg loss: 0.672638
[epoch 1, batch  1499] avg loss: 0.714980
[epoch 1, batch  1599] avg loss: 0.676358
[epoch 1, batch  1699] avg loss: 0.674993
[epoch 1, batch  1799] avg loss: 0.670433
[epoch 1, batch  1899] avg loss: 0.685599
[epoch 1, batch  1999] avg loss: 0.660523
[epoch 1, batch  2099] avg loss: 0.688717
[epoch 1, batch  2199] avg loss: 0.685584
[epoch 1, batch  2299] avg loss: 0.674500
[epoch 1, batch  2399] avg loss: 0.676379
[epoch 2, batch    99] avg loss: 0.690316
[epoch 2, batch   199] avg loss: 0.666728
[epoch 2, batch   299] avg loss: 0.680794
[epoch 2, batch   399] avg loss: 0.668532
[epoch 2, batch   499] avg loss: 0.655354
[epoch 2, batch   599] avg loss: 0.657300
[epoch 2, batch   699] avg loss: 0.637187
[epoch 2, batch   799] avg loss: 0.664573
[epoch 2, batch   899] avg loss: 0.653152
[epoch 2, batch   999] avg loss: 0.650473
[epoch 2, batch  1099] avg loss: 0.646626
[epoch 2, batch  1199] avg loss: 0.645764
[epoch 2, batch  1299] avg loss: 0.646378
[epoch 2, batch  1399] avg loss: 0.650425
[epoch 2, batch  1499] avg loss: 0.650224
[epoch 2, batch  1599] avg loss: 0.626911
[epoch 2, batch  1699] avg loss: 0.638314
[epoch 2, batch  1799] avg loss: 0.615957
[epoch 2, batch  1899] avg loss: 0.654057
[epoch 2, batch  1999] avg loss: 0.688274
[epoch 2, batch  2099] avg loss: 0.650770
[epoch 2, batch  2199] avg loss: 0.643328
[epoch 2, batch  2299] avg loss: 0.629270
[epoch 2, batch  2399] avg loss: 0.629554
[epoch 3, batch    99] avg loss: 0.635269
[epoch 3, batch   199] avg loss: 0.643944
[epoch 3, batch   299] avg loss: 0.628119
[epoch 3, batch   399] avg loss: 0.624470
[epoch 3, batch   499] avg loss: 0.618836
[epoch 3, batch   599] avg loss: 0.650360
[epoch 3, batch   699] avg loss: 0.630161
[epoch 3, batch   799] avg loss: 0.640933
[epoch 3, batch   899] avg loss: 0.605513
[epoch 3, batch   999] avg loss: 0.603698
[epoch 3, batch  1099] avg loss: 0.619143
[epoch 3, batch  1199] avg loss: 0.599986
[epoch 3, batch  1299] avg loss: 0.639247
[epoch 3, batch  1399] avg loss: 0.595238
[epoch 3, batch  1499] avg loss: 0.596178
[epoch 3, batch  1599] avg loss: 0.606436
[epoch 3, batch  1699] avg loss: 0.608906
[epoch 3, batch  1799] avg loss: 0.593138
[epoch 3, batch  1899] avg loss: 0.625319
[epoch 3, batch  1999] avg loss: 0.597414
[epoch 3, batch  2099] avg loss: 0.621002
[epoch 3, batch  2199] avg loss: 0.630497
[epoch 3, batch  2299] avg loss: 0.591320
[epoch 3, batch  2399] avg loss: 0.627438
[epoch 4, batch    99] avg loss: 0.623523
[epoch 4, batch   199] avg loss: 0.593261
[epoch 4, batch   299] avg loss: 0.596458
[epoch 4, batch   399] avg loss: 0.644131
[epoch 4, batch   499] avg loss: 0.597450
[epoch 4, batch   599] avg loss: 0.615834
[epoch 4, batch   699] avg loss: 0.576979
[epoch 4, batch   799] avg loss: 0.570502
[epoch 4, batch   899] avg loss: 0.578658
[epoch 4, batch   999] avg loss: 0.616216
[epoch 4, batch  1099] avg loss: 0.593055
[epoch 4, batch  1199] avg loss: 0.575670
[epoch 4, batch  1299] avg loss: 0.597179
[epoch 4, batch  1399] avg loss: 0.577209
[epoch 4, batch  1499] avg loss: 0.568711
[epoch 4, batch  1599] avg loss: 0.596879
[epoch 4, batch  1699] avg loss: 0.589376
[epoch 4, batch  1799] avg loss: 0.580611
[epoch 4, batch  1899] avg loss: 0.597563
[epoch 4, batch  1999] avg loss: 0.602424
[epoch 4, batch  2099] avg loss: 0.567350
[epoch 4, batch  2199] avg loss: 0.579925
[epoch 4, batch  2299] avg loss: 0.564907
[epoch 4, batch  2399] avg loss: 0.568949
[epoch 5, batch    99] avg loss: 0.559336
[epoch 5, batch   199] avg loss: 0.560557
[epoch 5, batch   299] avg loss: 0.561482
[epoch 5, batch   399] avg loss: 0.558622
[epoch 5, batch   499] avg loss: 0.596350
[epoch 5, batch   599] avg loss: 0.571687
[epoch 5, batch   699] avg loss: 0.573614
[epoch 5, batch   799] avg loss: 0.591745
[epoch 5, batch   899] avg loss: 0.570714
[epoch 5, batch   999] avg loss: 0.590472
[epoch 5, batch  1099] avg loss: 0.555060
[epoch 5, batch  1199] avg loss: 0.569477
[epoch 5, batch  1299] avg loss: 0.564536
[epoch 5, batch  1399] avg loss: 0.554414
[epoch 5, batch  1499] avg loss: 0.556040
[epoch 5, batch  1599] avg loss: 0.647529
[epoch 5, batch  1699] avg loss: 0.559771
[epoch 5, batch  1799] avg loss: 0.546346
[epoch 5, batch  1899] avg loss: 0.532829
[epoch 5, batch  1999] avg loss: 0.570355
[epoch 5, batch  2099] avg loss: 0.540231
[epoch 5, batch  2199] avg loss: 0.585554
[epoch 5, batch  2299] avg loss: 0.579369
[epoch 5, batch  2399] avg loss: 0.566169
[epoch 6, batch    99] avg loss: 0.535863
[epoch 6, batch   199] avg loss: 0.550286
[epoch 6, batch   299] avg loss: 0.556453
[epoch 6, batch   399] avg loss: 0.551888
[epoch 6, batch   499] avg loss: 0.562487
[epoch 6, batch   599] avg loss: 0.541306
[epoch 6, batch   699] avg loss: 0.555037
[epoch 6, batch   799] avg loss: 0.576468
[epoch 6, batch   899] avg loss: 0.556754
[epoch 6, batch   999] avg loss: 0.547776
[epoch 6, batch  1099] avg loss: 0.539708
[epoch 6, batch  1199] avg loss: 0.526114
[epoch 6, batch  1299] avg loss: 0.520748
[epoch 6, batch  1399] avg loss: 0.524583
[epoch 6, batch  1499] avg loss: 0.556731
[epoch 6, batch  1599] avg loss: 0.548092
[epoch 6, batch  1699] avg loss: 0.573093
[epoch 6, batch  1799] avg loss: 0.563630
[epoch 6, batch  1899] avg loss: 0.563673
[epoch 6, batch  1999] avg loss: 0.525649
[epoch 6, batch  2099] avg loss: 0.522356
[epoch 6, batch  2199] avg loss: 0.549148
[epoch 6, batch  2299] avg loss: 0.539693
[epoch 6, batch  2399] avg loss: 0.522046
[epoch 7, batch    99] avg loss: 0.555784
[epoch 7, batch   199] avg loss: 0.544932
[epoch 7, batch   299] avg loss: 0.545746
[epoch 7, batch   399] avg loss: 0.535414
[epoch 7, batch   499] avg loss: 0.512206
[epoch 7, batch   599] avg loss: 0.533112
[epoch 7, batch   699] avg loss: 0.575729
[epoch 7, batch   799] avg loss: 0.525864
[epoch 7, batch   899] avg loss: 0.522877
[epoch 7, batch   999] avg loss: 0.540086
[epoch 7, batch  1099] avg loss: 0.551141
[epoch 7, batch  1199] avg loss: 0.555419
[epoch 7, batch  1299] avg loss: 0.628646
[epoch 7, batch  1399] avg loss: 0.534948
[epoch 7, batch  1499] avg loss: 0.521161
[epoch 7, batch  1599] avg loss: 0.535907
[epoch 7, batch  1699] avg loss: 0.550446
[epoch 7, batch  1799] avg loss: 0.541960
[epoch 7, batch  1899] avg loss: 0.544467
[epoch 7, batch  1999] avg loss: 0.535185
[epoch 7, batch  2099] avg loss: 0.527365
[epoch 7, batch  2199] avg loss: 0.523686
[epoch 7, batch  2299] avg loss: 0.530503
[epoch 7, batch  2399] avg loss: 0.512656
[epoch 8, batch    99] avg loss: 0.501821
[epoch 8, batch   199] avg loss: 0.527379
[epoch 8, batch   299] avg loss: 0.527047
[epoch 8, batch   399] avg loss: 0.534880
[epoch 8, batch   499] avg loss: 0.519198
[epoch 8, batch   599] avg loss: 0.516997
[epoch 8, batch   699] avg loss: 0.517688
[epoch 8, batch   799] avg loss: 0.532493
[epoch 8, batch   899] avg loss: 0.532916
[epoch 8, batch   999] avg loss: 0.519243
[epoch 8, batch  1099] avg loss: 0.531952
[epoch 8, batch  1199] avg loss: 0.515932
[epoch 8, batch  1299] avg loss: 0.517957
[epoch 8, batch  1399] avg loss: 0.513458
[epoch 8, batch  1499] avg loss: 0.517816
[epoch 8, batch  1599] avg loss: 0.577929
[epoch 8, batch  1699] avg loss: 0.523055
[epoch 8, batch  1799] avg loss: 0.526099
[epoch 8, batch  1899] avg loss: 0.506777
[epoch 8, batch  1999] avg loss: 0.509258
[epoch 8, batch  2099] avg loss: 0.514391
[epoch 8, batch  2199] avg loss: 0.523417
[epoch 8, batch  2299] avg loss: 0.513661
[epoch 8, batch  2399] avg loss: 0.532094
[epoch 9, batch    99] avg loss: 0.529493
[epoch 9, batch   199] avg loss: 0.491341
[epoch 9, batch   299] avg loss: 0.504225
[epoch 9, batch   399] avg loss: 0.503154
[epoch 9, batch   499] avg loss: 0.517029
[epoch 9, batch   599] avg loss: 0.512282
[epoch 9, batch   699] avg loss: 0.506572
[epoch 9, batch   799] avg loss: 0.560358
[epoch 9, batch   899] avg loss: 0.493590
[epoch 9, batch   999] avg loss: 0.522134
[epoch 9, batch  1099] avg loss: 0.522320
[epoch 9, batch  1199] avg loss: 0.507281
[epoch 9, batch  1299] avg loss: 0.530769
[epoch 9, batch  1399] avg loss: 0.558456
[epoch 9, batch  1499] avg loss: 0.499452
[epoch 9, batch  1599] avg loss: 0.502880
[epoch 9, batch  1699] avg loss: 0.501946
[epoch 9, batch  1799] avg loss: 0.498396
[epoch 9, batch  1899] avg loss: 0.501451
[epoch 9, batch  1999] avg loss: 0.506259
[epoch 9, batch  2099] avg loss: 0.500347
[epoch 9, batch  2199] avg loss: 0.501193
[epoch 9, batch  2299] avg loss: 0.503216
[epoch 9, batch  2399] avg loss: 0.521507
[epoch 10, batch    99] avg loss: 0.517969
[epoch 10, batch   199] avg loss: 0.478708
[epoch 10, batch   299] avg loss: 0.529595
[epoch 10, batch   399] avg loss: 0.535703
[epoch 10, batch   499] avg loss: 0.506652
[epoch 10, batch   599] avg loss: 0.541489
[epoch 10, batch   699] avg loss: 0.490464
[epoch 10, batch   799] avg loss: 0.499806
[epoch 10, batch   899] avg loss: 0.513856
[epoch 10, batch   999] avg loss: 0.500045
[epoch 10, batch  1099] avg loss: 0.486913
[epoch 10, batch  1199] avg loss: 0.500944
[epoch 10, batch  1299] avg loss: 0.497451
[epoch 10, batch  1399] avg loss: 0.490397
[epoch 10, batch  1499] avg loss: 0.513330
[epoch 10, batch  1599] avg loss: 0.491772
[epoch 10, batch  1699] avg loss: 0.499123
[epoch 10, batch  1799] avg loss: 0.511225
[epoch 10, batch  1899] avg loss: 0.499682
[epoch 10, batch  1999] avg loss: 0.494223
[epoch 10, batch  2099] avg loss: 0.485271
[epoch 10, batch  2199] avg loss: 0.492274
[epoch 10, batch  2299] avg loss: 0.494773
[epoch 10, batch  2399] avg loss: 0.501036
[epoch 11, batch    99] avg loss: 0.495684
[epoch 11, batch   199] avg loss: 0.518342
[epoch 11, batch   299] avg loss: 0.480001
[epoch 11, batch   399] avg loss: 0.482468
[epoch 11, batch   499] avg loss: 0.501918
[epoch 11, batch   599] avg loss: 0.495708
[epoch 11, batch   699] avg loss: 0.529323
[epoch 11, batch   799] avg loss: 0.499354
[epoch 11, batch   899] avg loss: 0.497282
[epoch 11, batch   999] avg loss: 0.510287
[epoch 11, batch  1099] avg loss: 0.495698
[epoch 11, batch  1199] avg loss: 0.489304
[epoch 11, batch  1299] avg loss: 0.510020
[epoch 11, batch  1399] avg loss: 0.479087
[epoch 11, batch  1499] avg loss: 0.492946
[epoch 11, batch  1599] avg loss: 0.484479
[epoch 11, batch  1699] avg loss: 0.503107
[epoch 11, batch  1799] avg loss: 0.500577
[epoch 11, batch  1899] avg loss: 0.496442
[epoch 11, batch  1999] avg loss: 0.502597
[epoch 11, batch  2099] avg loss: 0.470451
[epoch 11, batch  2199] avg loss: 0.482467
[epoch 11, batch  2299] avg loss: 0.479593
[epoch 11, batch  2399] avg loss: 0.500873
[epoch 12, batch    99] avg loss: 0.475545
[epoch 12, batch   199] avg loss: 0.512180
[epoch 12, batch   299] avg loss: 0.490056
[epoch 12, batch   399] avg loss: 0.465961
[epoch 12, batch   499] avg loss: 0.480062
[epoch 12, batch   599] avg loss: 0.487435
[epoch 12, batch   699] avg loss: 0.502902
[epoch 12, batch   799] avg loss: 0.485811
[epoch 12, batch   899] avg loss: 0.474703
[epoch 12, batch   999] avg loss: 0.474062
[epoch 12, batch  1099] avg loss: 0.498561
[epoch 12, batch  1199] avg loss: 0.509247
[epoch 12, batch  1299] avg loss: 0.502869
[epoch 12, batch  1399] avg loss: 0.481203
[epoch 12, batch  1499] avg loss: 0.504357
[epoch 12, batch  1599] avg loss: 0.481986
[epoch 12, batch  1699] avg loss: 0.485823
[epoch 12, batch  1799] avg loss: 0.479170
[epoch 12, batch  1899] avg loss: 0.476608
[epoch 12, batch  1999] avg loss: 0.473844
[epoch 12, batch  2099] avg loss: 0.530638
[epoch 12, batch  2199] avg loss: 0.481130
[epoch 12, batch  2299] avg loss: 0.471877
[epoch 12, batch  2399] avg loss: 0.486866
[epoch 13, batch    99] avg loss: 0.475799
[epoch 13, batch   199] avg loss: 0.483494
[epoch 13, batch   299] avg loss: 0.479031
[epoch 13, batch   399] avg loss: 0.467375
[epoch 13, batch   499] avg loss: 0.458111
[epoch 13, batch   599] avg loss: 0.469063
[epoch 13, batch   699] avg loss: 0.495428
[epoch 13, batch   799] avg loss: 0.518954
[epoch 13, batch   899] avg loss: 0.500263
[epoch 13, batch   999] avg loss: 0.474811
[epoch 13, batch  1099] avg loss: 0.478864
[epoch 13, batch  1199] avg loss: 0.492275
[epoch 13, batch  1299] avg loss: 0.482951
[epoch 13, batch  1399] avg loss: 0.474687
[epoch 13, batch  1499] avg loss: 0.506478
[epoch 13, batch  1599] avg loss: 0.479966
[epoch 13, batch  1699] avg loss: 0.468000
[epoch 13, batch  1799] avg loss: 0.465225
[epoch 13, batch  1899] avg loss: 0.475166
[epoch 13, batch  1999] avg loss: 0.477265
[epoch 13, batch  2099] avg loss: 0.520054
[epoch 13, batch  2199] avg loss: 0.471184
[epoch 13, batch  2299] avg loss: 0.480550
[epoch 13, batch  2399] avg loss: 0.465047
[epoch 14, batch    99] avg loss: 0.484788
[epoch 14, batch   199] avg loss: 0.492546
[epoch 14, batch   299] avg loss: 0.473999
[epoch 14, batch   399] avg loss: 0.478502
[epoch 14, batch   499] avg loss: 0.466200
[epoch 14, batch   599] avg loss: 0.454562
[epoch 14, batch   699] avg loss: 0.496857
[epoch 14, batch   799] avg loss: 0.478230
[epoch 14, batch   899] avg loss: 0.462815
[epoch 14, batch   999] avg loss: 0.462035
[epoch 14, batch  1099] avg loss: 0.474577
[epoch 14, batch  1199] avg loss: 0.530631
[epoch 14, batch  1299] avg loss: 0.511295
[epoch 14, batch  1399] avg loss: 0.464200
[epoch 14, batch  1499] avg loss: 0.490764
[epoch 14, batch  1599] avg loss: 0.469570
[epoch 14, batch  1699] avg loss: 0.480795
[epoch 14, batch  1799] avg loss: 0.470238
[epoch 14, batch  1899] avg loss: 0.510098
[epoch 14, batch  1999] avg loss: 0.482752
[epoch 14, batch  2099] avg loss: 0.486036
[epoch 14, batch  2199] avg loss: 0.477086
[epoch 14, batch  2299] avg loss: 0.453654
[epoch 14, batch  2399] avg loss: 0.457535
[epoch 15, batch    99] avg loss: 0.467247
[epoch 15, batch   199] avg loss: 0.455340
[epoch 15, batch   299] avg loss: 0.542466
[epoch 15, batch   399] avg loss: 0.490538
[epoch 15, batch   499] avg loss: 0.480652
[epoch 15, batch   599] avg loss: 0.478333
[epoch 15, batch   699] avg loss: 0.460110
[epoch 15, batch   799] avg loss: 0.506412
[epoch 15, batch   899] avg loss: 0.452316
[epoch 15, batch   999] avg loss: 0.467473
[epoch 15, batch  1099] avg loss: 0.472590
[epoch 15, batch  1199] avg loss: 0.484578
[epoch 15, batch  1299] avg loss: 0.476781
[epoch 15, batch  1399] avg loss: 0.456272
[epoch 15, batch  1499] avg loss: 0.462935
[epoch 15, batch  1599] avg loss: 0.468486
[epoch 15, batch  1699] avg loss: 0.519968
[epoch 15, batch  1799] avg loss: 0.464980
[epoch 15, batch  1899] avg loss: 0.476070
[epoch 15, batch  1999] avg loss: 0.485402
[epoch 15, batch  2099] avg loss: 0.456278
[epoch 15, batch  2199] avg loss: 0.480441
[epoch 15, batch  2299] avg loss: 0.457263
[epoch 15, batch  2399] avg loss: 0.471648
[epoch 16, batch    99] avg loss: 0.465097
[epoch 16, batch   199] avg loss: 0.481934
[epoch 16, batch   299] avg loss: 0.449770
[epoch 16, batch   399] avg loss: 0.503088
[epoch 16, batch   499] avg loss: 0.469728
[epoch 16, batch   599] avg loss: 0.456983
[epoch 16, batch   699] avg loss: 0.471140
[epoch 16, batch   799] avg loss: 0.464308
[epoch 16, batch   899] avg loss: 0.458765
[epoch 16, batch   999] avg loss: 0.484548
[epoch 16, batch  1099] avg loss: 0.460920
[epoch 16, batch  1199] avg loss: 0.459484
[epoch 16, batch  1299] avg loss: 0.489612
[epoch 16, batch  1399] avg loss: 0.461630
[epoch 16, batch  1499] avg loss: 0.479290
[epoch 16, batch  1599] avg loss: 0.468578
[epoch 16, batch  1699] avg loss: 0.468366
[epoch 16, batch  1799] avg loss: 0.466720
[epoch 16, batch  1899] avg loss: 0.460232
[epoch 16, batch  1999] avg loss: 0.447696
[epoch 16, batch  2099] avg loss: 0.459765
[epoch 16, batch  2199] avg loss: 0.479222
[epoch 16, batch  2299] avg loss: 0.450652
[epoch 16, batch  2399] avg loss: 0.486323
[epoch 17, batch    99] avg loss: 0.473440
[epoch 17, batch   199] avg loss: 0.447496
[epoch 17, batch   299] avg loss: 0.482405
[epoch 17, batch   399] avg loss: 0.464620
[epoch 17, batch   499] avg loss: 0.479876
[epoch 17, batch   599] avg loss: 0.442541
[epoch 17, batch   699] avg loss: 0.469347
[epoch 17, batch   799] avg loss: 0.449893
[epoch 17, batch   899] avg loss: 0.453103
[epoch 17, batch   999] avg loss: 0.491724
[epoch 17, batch  1099] avg loss: 0.468158
[epoch 17, batch  1199] avg loss: 0.460787
[epoch 17, batch  1299] avg loss: 0.475639
[epoch 17, batch  1399] avg loss: 0.471399
[epoch 17, batch  1499] avg loss: 0.462119
[epoch 17, batch  1599] avg loss: 0.460169
[epoch 17, batch  1699] avg loss: 0.474283
[epoch 17, batch  1799] avg loss: 0.446821
[epoch 17, batch  1899] avg loss: 0.456963
[epoch 17, batch  1999] avg loss: 0.464661
[epoch 17, batch  2099] avg loss: 0.450362
[epoch 17, batch  2199] avg loss: 0.453764
[epoch 17, batch  2299] avg loss: 0.455063
[epoch 17, batch  2399] avg loss: 0.437377
[epoch 18, batch    99] avg loss: 0.453146
[epoch 18, batch   199] avg loss: 0.451397
[epoch 18, batch   299] avg loss: 0.460148
[epoch 18, batch   399] avg loss: 0.456566
[epoch 18, batch   499] avg loss: 0.455989
[epoch 18, batch   599] avg loss: 0.475925
[epoch 18, batch   699] avg loss: 0.444623
[epoch 18, batch   799] avg loss: 0.466914
[epoch 18, batch   899] avg loss: 0.463971
[epoch 18, batch   999] avg loss: 0.478869
[epoch 18, batch  1099] avg loss: 0.452194
[epoch 18, batch  1199] avg loss: 0.448333
[epoch 18, batch  1299] avg loss: 0.460559
[epoch 18, batch  1399] avg loss: 0.453023
[epoch 18, batch  1499] avg loss: 0.439134
[epoch 18, batch  1599] avg loss: 0.477181
[epoch 18, batch  1699] avg loss: 0.470058
[epoch 18, batch  1799] avg loss: 0.450292
[epoch 18, batch  1899] avg loss: 0.447966
[epoch 18, batch  1999] avg loss: 0.440794
[epoch 18, batch  2099] avg loss: 0.493925
[epoch 18, batch  2199] avg loss: 0.472925
[epoch 18, batch  2299] avg loss: 0.460038
[epoch 18, batch  2399] avg loss: 0.448122
[epoch 19, batch    99] avg loss: 0.467857
[epoch 19, batch   199] avg loss: 0.452215
[epoch 19, batch   299] avg loss: 0.438898
[epoch 19, batch   399] avg loss: 0.438664
[epoch 19, batch   499] avg loss: 0.456562
[epoch 19, batch   599] avg loss: 0.470897
[epoch 19, batch   699] avg loss: 0.452134
[epoch 19, batch   799] avg loss: 0.453655
[epoch 19, batch   899] avg loss: 0.431675
[epoch 19, batch   999] avg loss: 0.443073
[epoch 19, batch  1099] avg loss: 0.436341
[epoch 19, batch  1199] avg loss: 0.449454
[epoch 19, batch  1299] avg loss: 0.485518
[epoch 19, batch  1399] avg loss: 0.449698
[epoch 19, batch  1499] avg loss: 0.436934
[epoch 19, batch  1599] avg loss: 0.439360
[epoch 19, batch  1699] avg loss: 0.467500
[epoch 19, batch  1799] avg loss: 0.448382
[epoch 19, batch  1899] avg loss: 0.460356
[epoch 19, batch  1999] avg loss: 0.483891
[epoch 19, batch  2099] avg loss: 0.475937
[epoch 19, batch  2199] avg loss: 0.471864
[epoch 19, batch  2299] avg loss: 0.457064
[epoch 19, batch  2399] avg loss: 0.461773
Model saved to model/20200502-045916.pth.
accuracy/TriangPrismIsosc : 0.608
n_examples/TriangPrismIsosc : 500.0
accuracy/parallelepiped : 0.548
n_examples/parallelepiped : 500.0
accuracy/sphere : 1.0
n_examples/sphere : 102.0
accuracy/wire : 0.715
n_examples/wire : 200.0
accuracy/avg_geom : 0.6321044546850998
loss/validation_geom : 0.8037068547191708
accuracy/Au : 1.0
n_examples/Au : 1302.0
accuracy/SiN : 0.0
n_examples/SiN : 0.0
accuracy/SiO2 : 0.0
n_examples/SiO2 : 0.0
accuracy/avg_mat : 1.0
loss/validation_mat : 1.5253512709144744e-07
MSE/ShortestDim : 1.5862667945123488
MAE/ShortestDim : 0.8661869227794642
MSE/MiddleDim : 14.369634859756024
MAE/MiddleDim : 2.4775794977050407
MSE/LongDim : 62.83738687400994
MAE/LongDim : 4.288853353802144
MSE/log Area/Vol : 1.9256109305058025
MAE/log Area/Vol : 1.0094358939362744
loss/validation_dim : 80.71889945878411
loss/validation : 81.52260646603841
Metrics saved to model/20200502-045916_metrics.csv.
Parsed 2604 rows from data/sim_train_spectrum_SiN.
Parsed 2604 rows from data/sim_train_labels_SiN.
Parsed 9765 rows from data/gen_spectrum_SiN_00-of-16.
Parsed 9765 rows from data/gen_labels_SiN_00-of-16.
Parsed 9765 rows from data/gen_spectrum_SiN_01-of-16.
Parsed 9765 rows from data/gen_labels_SiN_01-of-16.
Parsed 9765 rows from data/gen_spectrum_SiN_02-of-16.
Parsed 9765 rows from data/gen_labels_SiN_02-of-16.
Parsed 9765 rows from data/gen_spectrum_SiN_03-of-16.
Parsed 9765 rows from data/gen_labels_SiN_03-of-16.
Parsed 9765 rows from data/gen_spectrum_SiN_04-of-16.
Parsed 9765 rows from data/gen_labels_SiN_04-of-16.
Parsed 9765 rows from data/gen_spectrum_SiN_05-of-16.
Parsed 9765 rows from data/gen_labels_SiN_05-of-16.
Parsed 9765 rows from data/gen_spectrum_SiN_06-of-16.
Parsed 9765 rows from data/gen_labels_SiN_06-of-16.
Parsed 9765 rows from data/gen_spectrum_SiN_07-of-16.
Parsed 9765 rows from data/gen_labels_SiN_07-of-16.
Parsed 9765 rows from data/gen_spectrum_SiN_08-of-16.
Parsed 9765 rows from data/gen_labels_SiN_08-of-16.
Parsed 9765 rows from data/gen_spectrum_SiN_09-of-16.
Parsed 9765 rows from data/gen_labels_SiN_09-of-16.
Parsed 9765 rows from data/gen_spectrum_SiN_10-of-16.
Parsed 9765 rows from data/gen_labels_SiN_10-of-16.
Parsed 9765 rows from data/gen_spectrum_SiN_11-of-16.
Parsed 9765 rows from data/gen_labels_SiN_11-of-16.
Parsed 9765 rows from data/gen_spectrum_SiN_12-of-16.
Parsed 9765 rows from data/gen_labels_SiN_12-of-16.
Parsed 9765 rows from data/gen_spectrum_SiN_13-of-16.
Parsed 9765 rows from data/gen_labels_SiN_13-of-16.
Parsed 9765 rows from data/gen_spectrum_SiN_14-of-16.
Parsed 9765 rows from data/gen_labels_SiN_14-of-16.
Parsed 9765 rows from data/gen_spectrum_SiN_15-of-16.
Parsed 9765 rows from data/gen_labels_SiN_15-of-16.
Parsed 1302 rows from data/sim_validation_spectrum_SiN.
Parsed 1302 rows from data/sim_validation_labels_SiN.
Logging training progress to tensorboard dir runs/alexnet-SiN-lr_0.000500-trainsize_158844-05_02_2020_05:00-multistage-joint.
[epoch 0, batch    99] avg loss: 0.395272
[epoch 0, batch   199] avg loss: 0.225181
[epoch 0, batch   299] avg loss: 0.213428
[epoch 0, batch   399] avg loss: 0.202638
[epoch 0, batch   499] avg loss: 0.171314
[epoch 0, batch   599] avg loss: 0.143703
[epoch 0, batch   699] avg loss: 0.138631
[epoch 0, batch   799] avg loss: 0.130019
[epoch 0, batch   899] avg loss: 0.127156
[epoch 0, batch   999] avg loss: 0.120043
[epoch 0, batch  1099] avg loss: 0.121398
[epoch 0, batch  1199] avg loss: 0.113952
[epoch 0, batch  1299] avg loss: 0.112633
[epoch 0, batch  1399] avg loss: 0.094739
[epoch 0, batch  1499] avg loss: 0.102870
[epoch 0, batch  1599] avg loss: 0.102124
[epoch 0, batch  1699] avg loss: 0.115666
[epoch 0, batch  1799] avg loss: 0.104886
[epoch 0, batch  1899] avg loss: 0.106975
[epoch 0, batch  1999] avg loss: 0.103147
[epoch 0, batch  2099] avg loss: 0.110298
[epoch 0, batch  2199] avg loss: 0.102770
[epoch 0, batch  2299] avg loss: 0.101906
[epoch 0, batch  2399] avg loss: 0.105561
[epoch 1, batch    99] avg loss: 0.100299
[epoch 1, batch   199] avg loss: 0.100811
[epoch 1, batch   299] avg loss: 0.096128
[epoch 1, batch   399] avg loss: 0.095540
[epoch 1, batch   499] avg loss: 0.090546
[epoch 1, batch   599] avg loss: 0.095675
[epoch 1, batch   699] avg loss: 0.098716
[epoch 1, batch   799] avg loss: 0.101897
[epoch 1, batch   899] avg loss: 0.088776
[epoch 1, batch   999] avg loss: 0.098435
[epoch 1, batch  1099] avg loss: 0.095607
[epoch 1, batch  1199] avg loss: 0.075806
[epoch 1, batch  1299] avg loss: 0.097633
[epoch 1, batch  1399] avg loss: 0.095471
[epoch 1, batch  1499] avg loss: 0.091089
[epoch 1, batch  1599] avg loss: 0.091626
[epoch 1, batch  1699] avg loss: 0.092027
[epoch 1, batch  1799] avg loss: 0.091926
[epoch 1, batch  1899] avg loss: 0.094511
[epoch 1, batch  1999] avg loss: 0.080281
[epoch 1, batch  2099] avg loss: 0.090845
[epoch 1, batch  2199] avg loss: 0.086445
[epoch 1, batch  2299] avg loss: 0.094899
[epoch 1, batch  2399] avg loss: 0.082758
[epoch 2, batch    99] avg loss: 0.077408
[epoch 2, batch   199] avg loss: 0.086456
[epoch 2, batch   299] avg loss: 0.082219
[epoch 2, batch   399] avg loss: 0.084656
[epoch 2, batch   499] avg loss: 0.074740
[epoch 2, batch   599] avg loss: 0.081308
[epoch 2, batch   699] avg loss: 0.088811
[epoch 2, batch   799] avg loss: 0.079644
[epoch 2, batch   899] avg loss: 0.078439
[epoch 2, batch   999] avg loss: 0.094867
[epoch 2, batch  1099] avg loss: 0.079279
[epoch 2, batch  1199] avg loss: 0.080290
[epoch 2, batch  1299] avg loss: 0.072356
[epoch 2, batch  1399] avg loss: 0.080069
[epoch 2, batch  1499] avg loss: 0.082816
[epoch 2, batch  1599] avg loss: 0.079940
[epoch 2, batch  1699] avg loss: 0.075254
[epoch 2, batch  1799] avg loss: 0.095023
[epoch 2, batch  1899] avg loss: 0.073906
[epoch 2, batch  1999] avg loss: 0.082413
[epoch 2, batch  2099] avg loss: 0.077130
[epoch 2, batch  2199] avg loss: 0.090539
[epoch 2, batch  2299] avg loss: 0.073202
[epoch 2, batch  2399] avg loss: 0.081216
[epoch 3, batch    99] avg loss: 0.076596
[epoch 3, batch   199] avg loss: 0.080788
[epoch 3, batch   299] avg loss: 0.070871
[epoch 3, batch   399] avg loss: 0.073103
[epoch 3, batch   499] avg loss: 0.079797
[epoch 3, batch   599] avg loss: 0.096514
[epoch 3, batch   699] avg loss: 0.080441
[epoch 3, batch   799] avg loss: 0.067462
[epoch 3, batch   899] avg loss: 0.081157
[epoch 3, batch   999] avg loss: 0.081725
[epoch 3, batch  1099] avg loss: 0.078633
[epoch 3, batch  1199] avg loss: 0.078012
[epoch 3, batch  1299] avg loss: 0.069661
[epoch 3, batch  1399] avg loss: 0.078209
[epoch 3, batch  1499] avg loss: 0.066341
[epoch 3, batch  1599] avg loss: 0.065844
[epoch 3, batch  1699] avg loss: 0.070840
[epoch 3, batch  1799] avg loss: 0.081156
[epoch 3, batch  1899] avg loss: 0.071518
[epoch 3, batch  1999] avg loss: 0.076377
[epoch 3, batch  2099] avg loss: 0.061258
[epoch 3, batch  2199] avg loss: 0.067968
[epoch 3, batch  2299] avg loss: 0.078483
[epoch 3, batch  2399] avg loss: 0.085365
[epoch 4, batch    99] avg loss: 0.079504
[epoch 4, batch   199] avg loss: 0.074700
[epoch 4, batch   299] avg loss: 0.079811
[epoch 4, batch   399] avg loss: 0.063450
[epoch 4, batch   499] avg loss: 0.071655
[epoch 4, batch   599] avg loss: 0.081315
[epoch 4, batch   699] avg loss: 0.072916
[epoch 4, batch   799] avg loss: 0.075979
[epoch 4, batch   899] avg loss: 0.080201
[epoch 4, batch   999] avg loss: 0.073898
[epoch 4, batch  1099] avg loss: 0.079657
[epoch 4, batch  1199] avg loss: 0.073196
[epoch 4, batch  1299] avg loss: 0.071146
[epoch 4, batch  1399] avg loss: 0.069463
[epoch 4, batch  1499] avg loss: 0.080499
[epoch 4, batch  1599] avg loss: 0.057966
[epoch 4, batch  1699] avg loss: 0.075868
[epoch 4, batch  1799] avg loss: 0.068838
[epoch 4, batch  1899] avg loss: 0.071351
[epoch 4, batch  1999] avg loss: 0.065158
[epoch 4, batch  2099] avg loss: 0.074673
[epoch 4, batch  2199] avg loss: 0.072773
[epoch 4, batch  2299] avg loss: 0.062706
[epoch 4, batch  2399] avg loss: 0.074678
[epoch 5, batch    99] avg loss: 0.070676
[epoch 5, batch   199] avg loss: 0.061372
[epoch 5, batch   299] avg loss: 0.061099
[epoch 5, batch   399] avg loss: 0.078390
[epoch 5, batch   499] avg loss: 0.067107
[epoch 5, batch   599] avg loss: 0.074377
[epoch 5, batch   699] avg loss: 0.070569
[epoch 5, batch   799] avg loss: 0.065573
[epoch 5, batch   899] avg loss: 0.068350
[epoch 5, batch   999] avg loss: 0.083433
[epoch 5, batch  1099] avg loss: 0.073261
[epoch 5, batch  1199] avg loss: 0.071272
[epoch 5, batch  1299] avg loss: 0.069604
[epoch 5, batch  1399] avg loss: 0.072021
[epoch 5, batch  1499] avg loss: 0.074732
[epoch 5, batch  1599] avg loss: 0.078346
[epoch 5, batch  1699] avg loss: 0.066038
[epoch 5, batch  1799] avg loss: 0.061498
[epoch 5, batch  1899] avg loss: 0.074102
[epoch 5, batch  1999] avg loss: 0.070153
[epoch 5, batch  2099] avg loss: 0.063798
[epoch 5, batch  2199] avg loss: 0.070652
[epoch 5, batch  2299] avg loss: 0.077486
[epoch 5, batch  2399] avg loss: 0.070102
[epoch 6, batch    99] avg loss: 0.071259
[epoch 6, batch   199] avg loss: 0.065999
[epoch 6, batch   299] avg loss: 0.072174
[epoch 6, batch   399] avg loss: 0.059845
[epoch 6, batch   499] avg loss: 0.075772
[epoch 6, batch   599] avg loss: 0.069573
[epoch 6, batch   699] avg loss: 0.059456
[epoch 6, batch   799] avg loss: 0.077715
[epoch 6, batch   899] avg loss: 0.077676
[epoch 6, batch   999] avg loss: 0.064134
[epoch 6, batch  1099] avg loss: 0.070057
[epoch 6, batch  1199] avg loss: 0.067385
[epoch 6, batch  1299] avg loss: 0.075567
[epoch 6, batch  1399] avg loss: 0.077130
[epoch 6, batch  1499] avg loss: 0.073122
[epoch 6, batch  1599] avg loss: 0.070516
[epoch 6, batch  1699] avg loss: 0.070063
[epoch 6, batch  1799] avg loss: 0.063767
[epoch 6, batch  1899] avg loss: 0.065350
[epoch 6, batch  1999] avg loss: 0.067780
[epoch 6, batch  2099] avg loss: 0.066739
[epoch 6, batch  2199] avg loss: 0.068475
[epoch 6, batch  2299] avg loss: 0.066475
[epoch 6, batch  2399] avg loss: 0.076265
[epoch 7, batch    99] avg loss: 0.058083
[epoch 7, batch   199] avg loss: 0.058656
[epoch 7, batch   299] avg loss: 0.068581
[epoch 7, batch   399] avg loss: 0.056745
[epoch 7, batch   499] avg loss: 0.069732
[epoch 7, batch   599] avg loss: 0.067218
[epoch 7, batch   699] avg loss: 0.068216
[epoch 7, batch   799] avg loss: 0.070912
[epoch 7, batch   899] avg loss: 0.072677
[epoch 7, batch   999] avg loss: 0.084931
[epoch 7, batch  1099] avg loss: 0.077003
[epoch 7, batch  1199] avg loss: 0.067489
[epoch 7, batch  1299] avg loss: 0.067893
[epoch 7, batch  1399] avg loss: 0.071230
[epoch 7, batch  1499] avg loss: 0.066757
[epoch 7, batch  1599] avg loss: 0.061061
[epoch 7, batch  1699] avg loss: 0.060287
[epoch 7, batch  1799] avg loss: 0.078691
[epoch 7, batch  1899] avg loss: 0.073592
[epoch 7, batch  1999] avg loss: 0.065652
[epoch 7, batch  2099] avg loss: 0.070533
[epoch 7, batch  2199] avg loss: 0.070552
[epoch 7, batch  2299] avg loss: 0.074468
[epoch 7, batch  2399] avg loss: 0.068068
[epoch 8, batch    99] avg loss: 0.068922
[epoch 8, batch   199] avg loss: 0.076005
[epoch 8, batch   299] avg loss: 0.070178
[epoch 8, batch   399] avg loss: 0.055569
[epoch 8, batch   499] avg loss: 0.070551
[epoch 8, batch   599] avg loss: 0.068865
[epoch 8, batch   699] avg loss: 0.062733
[epoch 8, batch   799] avg loss: 0.068969
[epoch 8, batch   899] avg loss: 0.068127
[epoch 8, batch   999] avg loss: 0.071598
[epoch 8, batch  1099] avg loss: 0.069959
[epoch 8, batch  1199] avg loss: 0.068063
[epoch 8, batch  1299] avg loss: 0.070332
[epoch 8, batch  1399] avg loss: 0.062510
[epoch 8, batch  1499] avg loss: 0.070261
[epoch 8, batch  1599] avg loss: 0.065671
[epoch 8, batch  1699] avg loss: 0.070664
[epoch 8, batch  1799] avg loss: 0.065465
[epoch 8, batch  1899] avg loss: 0.060844
[epoch 8, batch  1999] avg loss: 0.067645
[epoch 8, batch  2099] avg loss: 0.068500
[epoch 8, batch  2199] avg loss: 0.068701
[epoch 8, batch  2299] avg loss: 0.067029
[epoch 8, batch  2399] avg loss: 0.058491
[epoch 9, batch    99] avg loss: 0.058603
[epoch 9, batch   199] avg loss: 0.057914
[epoch 9, batch   299] avg loss: 0.071396
[epoch 9, batch   399] avg loss: 0.075883
[epoch 9, batch   499] avg loss: 0.059961
[epoch 9, batch   599] avg loss: 0.073145
[epoch 9, batch   699] avg loss: 0.078628
[epoch 9, batch   799] avg loss: 0.066002
[epoch 9, batch   899] avg loss: 0.066535
[epoch 9, batch   999] avg loss: 0.071131
[epoch 9, batch  1099] avg loss: 0.075617
[epoch 9, batch  1199] avg loss: 0.075918
[epoch 9, batch  1299] avg loss: 0.075982
[epoch 9, batch  1399] avg loss: 0.062331
[epoch 9, batch  1499] avg loss: 0.067979
[epoch 9, batch  1599] avg loss: 0.060579
[epoch 9, batch  1699] avg loss: 0.065400
[epoch 9, batch  1799] avg loss: 0.060303
[epoch 9, batch  1899] avg loss: 0.066570
[epoch 9, batch  1999] avg loss: 0.064525
[epoch 9, batch  2099] avg loss: 0.063390
[epoch 9, batch  2199] avg loss: 0.058320
[epoch 9, batch  2299] avg loss: 0.064479
[epoch 9, batch  2399] avg loss: 0.056799
[epoch 10, batch    99] avg loss: 0.062323
[epoch 10, batch   199] avg loss: 0.066359
[epoch 10, batch   299] avg loss: 0.067025
[epoch 10, batch   399] avg loss: 0.068913
[epoch 10, batch   499] avg loss: 0.060673
[epoch 10, batch   599] avg loss: 0.061708
[epoch 10, batch   699] avg loss: 0.066747
[epoch 10, batch   799] avg loss: 0.067189
[epoch 10, batch   899] avg loss: 0.065660
[epoch 10, batch   999] avg loss: 0.064808
[epoch 10, batch  1099] avg loss: 0.065912
[epoch 10, batch  1199] avg loss: 0.065278
[epoch 10, batch  1299] avg loss: 0.064544
[epoch 10, batch  1399] avg loss: 0.061226
[epoch 10, batch  1499] avg loss: 0.065265
[epoch 10, batch  1599] avg loss: 0.066719
[epoch 10, batch  1699] avg loss: 0.069958
[epoch 10, batch  1799] avg loss: 0.067257
[epoch 10, batch  1899] avg loss: 0.065845
[epoch 10, batch  1999] avg loss: 0.071880
[epoch 10, batch  2099] avg loss: 0.072076
[epoch 10, batch  2199] avg loss: 0.066157
[epoch 10, batch  2299] avg loss: 0.068829
[epoch 10, batch  2399] avg loss: 0.056218
[epoch 11, batch    99] avg loss: 0.061782
[epoch 11, batch   199] avg loss: 0.061689
[epoch 11, batch   299] avg loss: 0.069588
[epoch 11, batch   399] avg loss: 0.063387
[epoch 11, batch   499] avg loss: 0.061351
[epoch 11, batch   599] avg loss: 0.060536
[epoch 11, batch   699] avg loss: 0.058397
[epoch 11, batch   799] avg loss: 0.063297
[epoch 11, batch   899] avg loss: 0.069959
[epoch 11, batch   999] avg loss: 0.055751
[epoch 11, batch  1099] avg loss: 0.063320
[epoch 11, batch  1199] avg loss: 0.058240
[epoch 11, batch  1299] avg loss: 0.061449
[epoch 11, batch  1399] avg loss: 0.070753
[epoch 11, batch  1499] avg loss: 0.064240
[epoch 11, batch  1599] avg loss: 0.062929
[epoch 11, batch  1699] avg loss: 0.072832
[epoch 11, batch  1799] avg loss: 0.067873
[epoch 11, batch  1899] avg loss: 0.061895
[epoch 11, batch  1999] avg loss: 0.063192
[epoch 11, batch  2099] avg loss: 0.073937
[epoch 11, batch  2199] avg loss: 0.070088
[epoch 11, batch  2299] avg loss: 0.075919
[epoch 11, batch  2399] avg loss: 0.056979
[epoch 12, batch    99] avg loss: 0.064880
[epoch 12, batch   199] avg loss: 0.070433
[epoch 12, batch   299] avg loss: 0.060173
[epoch 12, batch   399] avg loss: 0.064859
[epoch 12, batch   499] avg loss: 0.059471
[epoch 12, batch   599] avg loss: 0.063332
[epoch 12, batch   699] avg loss: 0.056423
[epoch 12, batch   799] avg loss: 0.062949
[epoch 12, batch   899] avg loss: 0.064508
[epoch 12, batch   999] avg loss: 0.059417
[epoch 12, batch  1099] avg loss: 0.064809
[epoch 12, batch  1199] avg loss: 0.067584
[epoch 12, batch  1299] avg loss: 0.066236
[epoch 12, batch  1399] avg loss: 0.064795
[epoch 12, batch  1499] avg loss: 0.062923
[epoch 12, batch  1599] avg loss: 0.072459
[epoch 12, batch  1699] avg loss: 0.068553
[epoch 12, batch  1799] avg loss: 0.073781
[epoch 12, batch  1899] avg loss: 0.063012
[epoch 12, batch  1999] avg loss: 0.063557
[epoch 12, batch  2099] avg loss: 0.076336
[epoch 12, batch  2199] avg loss: 0.054822
[epoch 12, batch  2299] avg loss: 0.065445
[epoch 12, batch  2399] avg loss: 0.058045
[epoch 13, batch    99] avg loss: 0.061196
[epoch 13, batch   199] avg loss: 0.057610
[epoch 13, batch   299] avg loss: 0.059177
[epoch 13, batch   399] avg loss: 0.061234
[epoch 13, batch   499] avg loss: 0.062411
[epoch 13, batch   599] avg loss: 0.062773
[epoch 13, batch   699] avg loss: 0.054274
[epoch 13, batch   799] avg loss: 0.069921
[epoch 13, batch   899] avg loss: 0.070998
[epoch 13, batch   999] avg loss: 0.056848
[epoch 13, batch  1099] avg loss: 0.060854
[epoch 13, batch  1199] avg loss: 0.060379
[epoch 13, batch  1299] avg loss: 0.058337
[epoch 13, batch  1399] avg loss: 0.066323
[epoch 13, batch  1499] avg loss: 0.070327
[epoch 13, batch  1599] avg loss: 0.068435
[epoch 13, batch  1699] avg loss: 0.054913
[epoch 13, batch  1799] avg loss: 0.057575
[epoch 13, batch  1899] avg loss: 0.064435
[epoch 13, batch  1999] avg loss: 0.070585
[epoch 13, batch  2099] avg loss: 0.069732
[epoch 13, batch  2199] avg loss: 0.063748
[epoch 13, batch  2299] avg loss: 0.063747
[epoch 13, batch  2399] avg loss: 0.072855
[epoch 14, batch    99] avg loss: 0.071317
[epoch 14, batch   199] avg loss: 0.057842
[epoch 14, batch   299] avg loss: 0.066911
[epoch 14, batch   399] avg loss: 0.062742
[epoch 14, batch   499] avg loss: 0.061534
[epoch 14, batch   599] avg loss: 0.063312
[epoch 14, batch   699] avg loss: 0.054620
[epoch 14, batch   799] avg loss: 0.056234
[epoch 14, batch   899] avg loss: 0.066062
[epoch 14, batch   999] avg loss: 0.066409
[epoch 14, batch  1099] avg loss: 0.058371
[epoch 14, batch  1199] avg loss: 0.059008
[epoch 14, batch  1299] avg loss: 0.062684
[epoch 14, batch  1399] avg loss: 0.073767
[epoch 14, batch  1499] avg loss: 0.070832
[epoch 14, batch  1599] avg loss: 0.051765
[epoch 14, batch  1699] avg loss: 0.063827
[epoch 14, batch  1799] avg loss: 0.062729
[epoch 14, batch  1899] avg loss: 0.074400
[epoch 14, batch  1999] avg loss: 0.055754
[epoch 14, batch  2099] avg loss: 0.071297
[epoch 14, batch  2199] avg loss: 0.060887
[epoch 14, batch  2299] avg loss: 0.055441
[epoch 14, batch  2399] avg loss: 0.059119
[epoch 15, batch    99] avg loss: 0.069160
[epoch 15, batch   199] avg loss: 0.058038
[epoch 15, batch   299] avg loss: 0.060404
[epoch 15, batch   399] avg loss: 0.059577
[epoch 15, batch   499] avg loss: 0.065261
[epoch 15, batch   599] avg loss: 0.065856
[epoch 15, batch   699] avg loss: 0.061777
[epoch 15, batch   799] avg loss: 0.063612
[epoch 15, batch   899] avg loss: 0.070606
[epoch 15, batch   999] avg loss: 0.065829
[epoch 15, batch  1099] avg loss: 0.063915
[epoch 15, batch  1199] avg loss: 0.055945
[epoch 15, batch  1299] avg loss: 0.054440
[epoch 15, batch  1399] avg loss: 0.062374
[epoch 15, batch  1499] avg loss: 0.061040
[epoch 15, batch  1599] avg loss: 0.062965
[epoch 15, batch  1699] avg loss: 0.060225
[epoch 15, batch  1799] avg loss: 0.058471
[epoch 15, batch  1899] avg loss: 0.066575
[epoch 15, batch  1999] avg loss: 0.073366
[epoch 15, batch  2099] avg loss: 0.061936
[epoch 15, batch  2199] avg loss: 0.056364
[epoch 15, batch  2299] avg loss: 0.064565
[epoch 15, batch  2399] avg loss: 0.054329
[epoch 16, batch    99] avg loss: 0.073197
[epoch 16, batch   199] avg loss: 0.075622
[epoch 16, batch   299] avg loss: 0.061392
[epoch 16, batch   399] avg loss: 0.060599
[epoch 16, batch   499] avg loss: 0.055388
[epoch 16, batch   599] avg loss: 0.059618
[epoch 16, batch   699] avg loss: 0.064188
[epoch 16, batch   799] avg loss: 0.061032
[epoch 16, batch   899] avg loss: 0.062787
[epoch 16, batch   999] avg loss: 0.059360
[epoch 16, batch  1099] avg loss: 0.062939
[epoch 16, batch  1199] avg loss: 0.062225
[epoch 16, batch  1299] avg loss: 0.064635
[epoch 16, batch  1399] avg loss: 0.069117
[epoch 16, batch  1499] avg loss: 0.054703
[epoch 16, batch  1599] avg loss: 0.056481
[epoch 16, batch  1699] avg loss: 0.052897
[epoch 16, batch  1799] avg loss: 0.074317
[epoch 16, batch  1899] avg loss: 0.061004
[epoch 16, batch  1999] avg loss: 0.061678
[epoch 16, batch  2099] avg loss: 0.060479
[epoch 16, batch  2199] avg loss: 0.056645
[epoch 16, batch  2299] avg loss: 0.061232
[epoch 16, batch  2399] avg loss: 0.060987
[epoch 17, batch    99] avg loss: 0.063246
[epoch 17, batch   199] avg loss: 0.070397
[epoch 17, batch   299] avg loss: 0.062324
[epoch 17, batch   399] avg loss: 0.056971
[epoch 17, batch   499] avg loss: 0.064717
[epoch 17, batch   599] avg loss: 0.064657
[epoch 17, batch   699] avg loss: 0.062894
[epoch 17, batch   799] avg loss: 0.049911
[epoch 17, batch   899] avg loss: 0.061665
[epoch 17, batch   999] avg loss: 0.065571
[epoch 17, batch  1099] avg loss: 0.063690
[epoch 17, batch  1199] avg loss: 0.054799
[epoch 17, batch  1299] avg loss: 0.055663
[epoch 17, batch  1399] avg loss: 0.061939
[epoch 17, batch  1499] avg loss: 0.066906
[epoch 17, batch  1599] avg loss: 0.073675
[epoch 17, batch  1699] avg loss: 0.061142
[epoch 17, batch  1799] avg loss: 0.070060
[epoch 17, batch  1899] avg loss: 0.066337
[epoch 17, batch  1999] avg loss: 0.057240
[epoch 17, batch  2099] avg loss: 0.065931
[epoch 17, batch  2199] avg loss: 0.063499
[epoch 17, batch  2299] avg loss: 0.058888
[epoch 17, batch  2399] avg loss: 0.052075
[epoch 18, batch    99] avg loss: 0.063554
[epoch 18, batch   199] avg loss: 0.061251
[epoch 18, batch   299] avg loss: 0.059713
[epoch 18, batch   399] avg loss: 0.061383
[epoch 18, batch   499] avg loss: 0.061894
[epoch 18, batch   599] avg loss: 0.068908
[epoch 18, batch   699] avg loss: 0.071636
[epoch 18, batch   799] avg loss: 0.067491
[epoch 18, batch   899] avg loss: 0.057846
[epoch 18, batch   999] avg loss: 0.064493
[epoch 18, batch  1099] avg loss: 0.056127
[epoch 18, batch  1199] avg loss: 0.057999
[epoch 18, batch  1299] avg loss: 0.058369
[epoch 18, batch  1399] avg loss: 0.061144
[epoch 18, batch  1499] avg loss: 0.054585
[epoch 18, batch  1599] avg loss: 0.069469
[epoch 18, batch  1699] avg loss: 0.063981
[epoch 18, batch  1799] avg loss: 0.055317
[epoch 18, batch  1899] avg loss: 0.052374
[epoch 18, batch  1999] avg loss: 0.061524
[epoch 18, batch  2099] avg loss: 0.058184
[epoch 18, batch  2199] avg loss: 0.062669
[epoch 18, batch  2299] avg loss: 0.058597
[epoch 18, batch  2399] avg loss: 0.066332
[epoch 19, batch    99] avg loss: 0.056529
[epoch 19, batch   199] avg loss: 0.062797
[epoch 19, batch   299] avg loss: 0.054225
[epoch 19, batch   399] avg loss: 0.065060
[epoch 19, batch   499] avg loss: 0.072798
[epoch 19, batch   599] avg loss: 0.056038
[epoch 19, batch   699] avg loss: 0.058914
[epoch 19, batch   799] avg loss: 0.059355
[epoch 19, batch   899] avg loss: 0.055632
[epoch 19, batch   999] avg loss: 0.056969
[epoch 19, batch  1099] avg loss: 0.061160
[epoch 19, batch  1199] avg loss: 0.064058
[epoch 19, batch  1299] avg loss: 0.057138
[epoch 19, batch  1399] avg loss: 0.059598
[epoch 19, batch  1499] avg loss: 0.055296
[epoch 19, batch  1599] avg loss: 0.065625
[epoch 19, batch  1699] avg loss: 0.062736
[epoch 19, batch  1799] avg loss: 0.064494
[epoch 19, batch  1899] avg loss: 0.066834
[epoch 19, batch  1999] avg loss: 0.062441
[epoch 19, batch  2099] avg loss: 0.060177
[epoch 19, batch  2199] avg loss: 0.067228
[epoch 19, batch  2299] avg loss: 0.064889
[epoch 19, batch  2399] avg loss: 0.059819
Model saved to model/20200502-051627.pth.
accuracy/TriangPrismIsosc : 0.004
n_examples/TriangPrismIsosc : 500.0
accuracy/parallelepiped : 0.0
n_examples/parallelepiped : 500.0
accuracy/sphere : 0.9215686274509803
n_examples/sphere : 102.0
accuracy/wire : 0.02
n_examples/wire : 200.0
accuracy/avg_geom : 0.07680491551459294
loss/validation_geom : 1.5513135321129303
accuracy/Au : 0.0
n_examples/Au : 0.0
accuracy/SiN : 1.0
n_examples/SiN : 1302.0
accuracy/SiO2 : 0.0
n_examples/SiO2 : 0.0
accuracy/avg_mat : 1.0
loss/validation_mat : 0.0
MSE/ShortestDim : 0.2503676139813964
MAE/ShortestDim : 0.2660687361627863
MSE/MiddleDim : 0.5903767015893705
MAE/MiddleDim : 0.4963219030295283
MSE/LongDim : 31.197262062271985
MAE/LongDim : 2.9290179327336325
MSE/log Area/Vol : 0.16717541034686767
MAE/log Area/Vol : 0.2990954306817824
loss/validation_dim : 32.20518178818962
loss/validation : 33.75649532030255
Metrics saved to model/20200502-051627_metrics.csv.
[epoch 0, batch    99] avg loss: 1.013760
[epoch 0, batch   199] avg loss: 0.808649
[epoch 0, batch   299] avg loss: 0.764960
[epoch 0, batch   399] avg loss: 0.723216
[epoch 0, batch   499] avg loss: 0.695149
[epoch 0, batch   599] avg loss: 0.664839
[epoch 0, batch   699] avg loss: 0.652045
[epoch 0, batch   799] avg loss: 0.623288
[epoch 0, batch   899] avg loss: 0.627330
[epoch 0, batch   999] avg loss: 0.617461
[epoch 0, batch  1099] avg loss: 0.614492
[epoch 0, batch  1199] avg loss: 0.604757
[epoch 0, batch  1299] avg loss: 0.579343
[epoch 0, batch  1399] avg loss: 0.589674
[epoch 0, batch  1499] avg loss: 0.598627
[epoch 0, batch  1599] avg loss: 0.576770
[epoch 0, batch  1699] avg loss: 0.587338
[epoch 0, batch  1799] avg loss: 0.553305
[epoch 0, batch  1899] avg loss: 0.576838
[epoch 0, batch  1999] avg loss: 0.572899
[epoch 0, batch  2099] avg loss: 0.555165
[epoch 0, batch  2199] avg loss: 0.567847
[epoch 0, batch  2299] avg loss: 0.554860
[epoch 0, batch  2399] avg loss: 0.565484
[epoch 1, batch    99] avg loss: 0.551124
[epoch 1, batch   199] avg loss: 0.557174
[epoch 1, batch   299] avg loss: 0.540785
[epoch 1, batch   399] avg loss: 0.538958
[epoch 1, batch   499] avg loss: 0.544543
[epoch 1, batch   599] avg loss: 0.557240
[epoch 1, batch   699] avg loss: 0.530933
[epoch 1, batch   799] avg loss: 0.535722
[epoch 1, batch   899] avg loss: 0.533188
[epoch 1, batch   999] avg loss: 0.523516
[epoch 1, batch  1099] avg loss: 0.517328
[epoch 1, batch  1199] avg loss: 0.516392
[epoch 1, batch  1299] avg loss: 0.531692
[epoch 1, batch  1399] avg loss: 0.526619
[epoch 1, batch  1499] avg loss: 0.523112
[epoch 1, batch  1599] avg loss: 0.538341
[epoch 1, batch  1699] avg loss: 0.524758
[epoch 1, batch  1799] avg loss: 0.510822
[epoch 1, batch  1899] avg loss: 0.538063
[epoch 1, batch  1999] avg loss: 0.526158
[epoch 1, batch  2099] avg loss: 0.521307
[epoch 1, batch  2199] avg loss: 0.503810
[epoch 1, batch  2299] avg loss: 0.519543
[epoch 1, batch  2399] avg loss: 0.519637
[epoch 2, batch    99] avg loss: 0.520742
[epoch 2, batch   199] avg loss: 0.497751
[epoch 2, batch   299] avg loss: 0.516319
[epoch 2, batch   399] avg loss: 0.506525
[epoch 2, batch   499] avg loss: 0.517009
[epoch 2, batch   599] avg loss: 0.510010
[epoch 2, batch   699] avg loss: 0.503823
[epoch 2, batch   799] avg loss: 0.514774
[epoch 2, batch   899] avg loss: 0.503591
[epoch 2, batch   999] avg loss: 0.501029
[epoch 2, batch  1099] avg loss: 0.488117
[epoch 2, batch  1199] avg loss: 0.499452
[epoch 2, batch  1299] avg loss: 0.523163
[epoch 2, batch  1399] avg loss: 0.501156
[epoch 2, batch  1499] avg loss: 0.500450
[epoch 2, batch  1599] avg loss: 0.491005
[epoch 2, batch  1699] avg loss: 0.513420
[epoch 2, batch  1799] avg loss: 0.481553
[epoch 2, batch  1899] avg loss: 0.522872
[epoch 2, batch  1999] avg loss: 0.497735
[epoch 2, batch  2099] avg loss: 0.498113
[epoch 2, batch  2199] avg loss: 0.504689
[epoch 2, batch  2299] avg loss: 0.492545
[epoch 2, batch  2399] avg loss: 0.469892
[epoch 3, batch    99] avg loss: 0.479476
[epoch 3, batch   199] avg loss: 0.487103
[epoch 3, batch   299] avg loss: 0.493539
[epoch 3, batch   399] avg loss: 0.491166
[epoch 3, batch   499] avg loss: 0.485872
[epoch 3, batch   599] avg loss: 0.487201
[epoch 3, batch   699] avg loss: 0.494701
[epoch 3, batch   799] avg loss: 0.482717
[epoch 3, batch   899] avg loss: 0.500925
[epoch 3, batch   999] avg loss: 0.494473
[epoch 3, batch  1099] avg loss: 0.472203
[epoch 3, batch  1199] avg loss: 0.502899
[epoch 3, batch  1299] avg loss: 0.470127
[epoch 3, batch  1399] avg loss: 0.485942
[epoch 3, batch  1499] avg loss: 0.496410
[epoch 3, batch  1599] avg loss: 0.482646
[epoch 3, batch  1699] avg loss: 0.473716
[epoch 3, batch  1799] avg loss: 0.495777
[epoch 3, batch  1899] avg loss: 0.481932
[epoch 3, batch  1999] avg loss: 0.481713
[epoch 3, batch  2099] avg loss: 0.491579
[epoch 3, batch  2199] avg loss: 0.462712
[epoch 3, batch  2299] avg loss: 0.481389
[epoch 3, batch  2399] avg loss: 0.476298
[epoch 4, batch    99] avg loss: 0.482221
[epoch 4, batch   199] avg loss: 0.484305
[epoch 4, batch   299] avg loss: 0.468673
[epoch 4, batch   399] avg loss: 0.473468
[epoch 4, batch   499] avg loss: 0.483601
[epoch 4, batch   599] avg loss: 0.476050
[epoch 4, batch   699] avg loss: 0.480034
[epoch 4, batch   799] avg loss: 0.459299
[epoch 4, batch   899] avg loss: 0.462948
[epoch 4, batch   999] avg loss: 0.483627
[epoch 4, batch  1099] avg loss: 0.471880
[epoch 4, batch  1199] avg loss: 0.472435
[epoch 4, batch  1299] avg loss: 0.474988
[epoch 4, batch  1399] avg loss: 0.459839
[epoch 4, batch  1499] avg loss: 0.473936
[epoch 4, batch  1599] avg loss: 0.474817
[epoch 4, batch  1699] avg loss: 0.458918
[epoch 4, batch  1799] avg loss: 0.467250
[epoch 4, batch  1899] avg loss: 0.463059
[epoch 4, batch  1999] avg loss: 0.468083
[epoch 4, batch  2099] avg loss: 0.463475
[epoch 4, batch  2199] avg loss: 0.466679
[epoch 4, batch  2299] avg loss: 0.474923
[epoch 4, batch  2399] avg loss: 0.482305
[epoch 5, batch    99] avg loss: 0.471226
[epoch 5, batch   199] avg loss: 0.461094
[epoch 5, batch   299] avg loss: 0.462015
[epoch 5, batch   399] avg loss: 0.465199
[epoch 5, batch   499] avg loss: 0.468380
[epoch 5, batch   599] avg loss: 0.486570
[epoch 5, batch   699] avg loss: 0.457617
[epoch 5, batch   799] avg loss: 0.467274
[epoch 5, batch   899] avg loss: 0.448435
[epoch 5, batch   999] avg loss: 0.483569
[epoch 5, batch  1099] avg loss: 0.465573
[epoch 5, batch  1199] avg loss: 0.459941
[epoch 5, batch  1299] avg loss: 0.471639
[epoch 5, batch  1399] avg loss: 0.468703
[epoch 5, batch  1499] avg loss: 0.463672
[epoch 5, batch  1599] avg loss: 0.456947
[epoch 5, batch  1699] avg loss: 0.501941
[epoch 5, batch  1799] avg loss: 0.463128
[epoch 5, batch  1899] avg loss: 0.449900
[epoch 5, batch  1999] avg loss: 0.454444
[epoch 5, batch  2099] avg loss: 0.470509
[epoch 5, batch  2199] avg loss: 0.452239
[epoch 5, batch  2299] avg loss: 0.456254
[epoch 5, batch  2399] avg loss: 0.454576
[epoch 6, batch    99] avg loss: 0.474917
[epoch 6, batch   199] avg loss: 0.468271
[epoch 6, batch   299] avg loss: 0.458248
[epoch 6, batch   399] avg loss: 0.464264
[epoch 6, batch   499] avg loss: 0.464311
[epoch 6, batch   599] avg loss: 0.466874
[epoch 6, batch   699] avg loss: 0.461094
[epoch 6, batch   799] avg loss: 0.448119
[epoch 6, batch   899] avg loss: 0.476524
[epoch 6, batch   999] avg loss: 0.446542
[epoch 6, batch  1099] avg loss: 0.455309
[epoch 6, batch  1199] avg loss: 0.468928
[epoch 6, batch  1299] avg loss: 0.457493
[epoch 6, batch  1399] avg loss: 0.455840
[epoch 6, batch  1499] avg loss: 0.463452
[epoch 6, batch  1599] avg loss: 0.450630
[epoch 6, batch  1699] avg loss: 0.447533
[epoch 6, batch  1799] avg loss: 0.451699
[epoch 6, batch  1899] avg loss: 0.442877
[epoch 6, batch  1999] avg loss: 0.464336
[epoch 6, batch  2099] avg loss: 0.444207
[epoch 6, batch  2199] avg loss: 0.470726
[epoch 6, batch  2299] avg loss: 0.445423
[epoch 6, batch  2399] avg loss: 0.437079
[epoch 7, batch    99] avg loss: 0.453383
[epoch 7, batch   199] avg loss: 0.455283
[epoch 7, batch   299] avg loss: 0.445942
[epoch 7, batch   399] avg loss: 0.454410
[epoch 7, batch   499] avg loss: 0.451663
[epoch 7, batch   599] avg loss: 0.450232
[epoch 7, batch   699] avg loss: 0.439855
[epoch 7, batch   799] avg loss: 0.441922
[epoch 7, batch   899] avg loss: 0.462510
[epoch 7, batch   999] avg loss: 0.443200
[epoch 7, batch  1099] avg loss: 0.469780
[epoch 7, batch  1199] avg loss: 0.446548
[epoch 7, batch  1299] avg loss: 0.430301
[epoch 7, batch  1399] avg loss: 0.443790
[epoch 7, batch  1499] avg loss: 0.449665
[epoch 7, batch  1599] avg loss: 0.448826
[epoch 7, batch  1699] avg loss: 0.459233
[epoch 7, batch  1799] avg loss: 0.441318
[epoch 7, batch  1899] avg loss: 0.448136
[epoch 7, batch  1999] avg loss: 0.424011
[epoch 7, batch  2099] avg loss: 0.443367
[epoch 7, batch  2199] avg loss: 0.451922
[epoch 7, batch  2299] avg loss: 0.451348
[epoch 7, batch  2399] avg loss: 0.449679
[epoch 8, batch    99] avg loss: 0.470496
[epoch 8, batch   199] avg loss: 0.462704
[epoch 8, batch   299] avg loss: 0.435110
[epoch 8, batch   399] avg loss: 0.438342
[epoch 8, batch   499] avg loss: 0.441152
[epoch 8, batch   599] avg loss: 0.446828
[epoch 8, batch   699] avg loss: 0.446822
[epoch 8, batch   799] avg loss: 0.456260
[epoch 8, batch   899] avg loss: 0.437499
[epoch 8, batch   999] avg loss: 0.445363
[epoch 8, batch  1099] avg loss: 0.440062
[epoch 8, batch  1199] avg loss: 0.452365
[epoch 8, batch  1299] avg loss: 0.443417
[epoch 8, batch  1399] avg loss: 0.448248
[epoch 8, batch  1499] avg loss: 0.452193
[epoch 8, batch  1599] avg loss: 0.429690
[epoch 8, batch  1699] avg loss: 0.445512
[epoch 8, batch  1799] avg loss: 0.449598
[epoch 8, batch  1899] avg loss: 0.430480
[epoch 8, batch  1999] avg loss: 0.427746
[epoch 8, batch  2099] avg loss: 0.459903
[epoch 8, batch  2199] avg loss: 0.438810
[epoch 8, batch  2299] avg loss: 0.451643
[epoch 8, batch  2399] avg loss: 0.438565
[epoch 9, batch    99] avg loss: 0.445595
[epoch 9, batch   199] avg loss: 0.455808
[epoch 9, batch   299] avg loss: 0.425873
[epoch 9, batch   399] avg loss: 0.442406
[epoch 9, batch   499] avg loss: 0.437933
[epoch 9, batch   599] avg loss: 0.425523
[epoch 9, batch   699] avg loss: 0.433195
[epoch 9, batch   799] avg loss: 0.428096
[epoch 9, batch   899] avg loss: 0.436839
[epoch 9, batch   999] avg loss: 0.439464
[epoch 9, batch  1099] avg loss: 0.436227
[epoch 9, batch  1199] avg loss: 0.437644
[epoch 9, batch  1299] avg loss: 0.433859
[epoch 9, batch  1399] avg loss: 0.440702
[epoch 9, batch  1499] avg loss: 0.433388
[epoch 9, batch  1599] avg loss: 0.424685
[epoch 9, batch  1699] avg loss: 0.436786
[epoch 9, batch  1799] avg loss: 0.457425
[epoch 9, batch  1899] avg loss: 0.438677
[epoch 9, batch  1999] avg loss: 0.441494
[epoch 9, batch  2099] avg loss: 0.447369
[epoch 9, batch  2199] avg loss: 0.451616
[epoch 9, batch  2299] avg loss: 0.436774
[epoch 9, batch  2399] avg loss: 0.430258
[epoch 10, batch    99] avg loss: 0.450613
[epoch 10, batch   199] avg loss: 0.420374
[epoch 10, batch   299] avg loss: 0.463509
[epoch 10, batch   399] avg loss: 0.426759
[epoch 10, batch   499] avg loss: 0.444475
[epoch 10, batch   599] avg loss: 0.434187
[epoch 10, batch   699] avg loss: 0.439032
[epoch 10, batch   799] avg loss: 0.419236
[epoch 10, batch   899] avg loss: 0.435912
[epoch 10, batch   999] avg loss: 0.449992
[epoch 10, batch  1099] avg loss: 0.448647
[epoch 10, batch  1199] avg loss: 0.438305
[epoch 10, batch  1299] avg loss: 0.416463
[epoch 10, batch  1399] avg loss: 0.433021
[epoch 10, batch  1499] avg loss: 0.447714
[epoch 10, batch  1599] avg loss: 0.434252
[epoch 10, batch  1699] avg loss: 0.434084
[epoch 10, batch  1799] avg loss: 0.441121
[epoch 10, batch  1899] avg loss: 0.415356
[epoch 10, batch  1999] avg loss: 0.428614
[epoch 10, batch  2099] avg loss: 0.426138
[epoch 10, batch  2199] avg loss: 0.438214
[epoch 10, batch  2299] avg loss: 0.441979
[epoch 10, batch  2399] avg loss: 0.433244
[epoch 11, batch    99] avg loss: 0.425094
[epoch 11, batch   199] avg loss: 0.438395
[epoch 11, batch   299] avg loss: 0.450240
[epoch 11, batch   399] avg loss: 0.428867
[epoch 11, batch   499] avg loss: 0.417134
[epoch 11, batch   599] avg loss: 0.414327
[epoch 11, batch   699] avg loss: 0.431158
[epoch 11, batch   799] avg loss: 0.430440
[epoch 11, batch   899] avg loss: 0.429740
[epoch 11, batch   999] avg loss: 0.441162
[epoch 11, batch  1099] avg loss: 0.435992
[epoch 11, batch  1199] avg loss: 0.451208
[epoch 11, batch  1299] avg loss: 0.427343
[epoch 11, batch  1399] avg loss: 0.422445
[epoch 11, batch  1499] avg loss: 0.429508
[epoch 11, batch  1599] avg loss: 0.425030
[epoch 11, batch  1699] avg loss: 0.422605
[epoch 11, batch  1799] avg loss: 0.444869
[epoch 11, batch  1899] avg loss: 0.435666
[epoch 11, batch  1999] avg loss: 0.437836
[epoch 11, batch  2099] avg loss: 0.436307
[epoch 11, batch  2199] avg loss: 0.433349
[epoch 11, batch  2299] avg loss: 0.431556
[epoch 11, batch  2399] avg loss: 0.423624
[epoch 12, batch    99] avg loss: 0.433429
[epoch 12, batch   199] avg loss: 0.437537
[epoch 12, batch   299] avg loss: 0.421262
[epoch 12, batch   399] avg loss: 0.433500
[epoch 12, batch   499] avg loss: 0.459984
[epoch 12, batch   599] avg loss: 0.433666
[epoch 12, batch   699] avg loss: 0.427892
[epoch 12, batch   799] avg loss: 0.422925
[epoch 12, batch   899] avg loss: 0.431059
[epoch 12, batch   999] avg loss: 0.441330
[epoch 12, batch  1099] avg loss: 0.446602
[epoch 12, batch  1199] avg loss: 0.437258
[epoch 12, batch  1299] avg loss: 0.443927
[epoch 12, batch  1399] avg loss: 0.423999
[epoch 12, batch  1499] avg loss: 0.429567
[epoch 12, batch  1599] avg loss: 0.423610
[epoch 12, batch  1699] avg loss: 0.421530
[epoch 12, batch  1799] avg loss: 0.433048
[epoch 12, batch  1899] avg loss: 0.439028
[epoch 12, batch  1999] avg loss: 0.418300
[epoch 12, batch  2099] avg loss: 0.428532
[epoch 12, batch  2199] avg loss: 0.431393
[epoch 12, batch  2299] avg loss: 0.424640
[epoch 12, batch  2399] avg loss: 0.423055
[epoch 13, batch    99] avg loss: 0.427364
[epoch 13, batch   199] avg loss: 0.436568
[epoch 13, batch   299] avg loss: 0.442885
[epoch 13, batch   399] avg loss: 0.426443
[epoch 13, batch   499] avg loss: 0.410879
[epoch 13, batch   599] avg loss: 0.424903
[epoch 13, batch   699] avg loss: 0.427486
[epoch 13, batch   799] avg loss: 0.420730
[epoch 13, batch   899] avg loss: 0.418684
[epoch 13, batch   999] avg loss: 0.418300
[epoch 13, batch  1099] avg loss: 0.433648
[epoch 13, batch  1199] avg loss: 0.418561
[epoch 13, batch  1299] avg loss: 0.418918
[epoch 13, batch  1399] avg loss: 0.423731
[epoch 13, batch  1499] avg loss: 0.412859
[epoch 13, batch  1599] avg loss: 0.465411
[epoch 13, batch  1699] avg loss: 0.424887
[epoch 13, batch  1799] avg loss: 0.429066
[epoch 13, batch  1899] avg loss: 0.406809
[epoch 13, batch  1999] avg loss: 0.423526
[epoch 13, batch  2099] avg loss: 0.433241
[epoch 13, batch  2199] avg loss: 0.454208
[epoch 13, batch  2299] avg loss: 0.421735
[epoch 13, batch  2399] avg loss: 0.414237
[epoch 14, batch    99] avg loss: 0.422865
[epoch 14, batch   199] avg loss: 0.413659
[epoch 14, batch   299] avg loss: 0.412211
[epoch 14, batch   399] avg loss: 0.431287
[epoch 14, batch   499] avg loss: 0.425468
[epoch 14, batch   599] avg loss: 0.428053
[epoch 14, batch   699] avg loss: 0.436899
[epoch 14, batch   799] avg loss: 0.435364
[epoch 14, batch   899] avg loss: 0.436284
[epoch 14, batch   999] avg loss: 0.415119
[epoch 14, batch  1099] avg loss: 0.419752
[epoch 14, batch  1199] avg loss: 0.420442
[epoch 14, batch  1299] avg loss: 0.413791
[epoch 14, batch  1399] avg loss: 0.419869
[epoch 14, batch  1499] avg loss: 0.408136
[epoch 14, batch  1599] avg loss: 0.416876
[epoch 14, batch  1699] avg loss: 0.418855
[epoch 14, batch  1799] avg loss: 0.401837
[epoch 14, batch  1899] avg loss: 0.432424
[epoch 14, batch  1999] avg loss: 0.443281
[epoch 14, batch  2099] avg loss: 0.421505
[epoch 14, batch  2199] avg loss: 0.433550
[epoch 14, batch  2299] avg loss: 0.422543
[epoch 14, batch  2399] avg loss: 0.425277
[epoch 15, batch    99] avg loss: 0.421687
[epoch 15, batch   199] avg loss: 0.412896
[epoch 15, batch   299] avg loss: 0.416615
[epoch 15, batch   399] avg loss: 0.421821
[epoch 15, batch   499] avg loss: 0.413778
[epoch 15, batch   599] avg loss: 0.422680
[epoch 15, batch   699] avg loss: 0.432962
[epoch 15, batch   799] avg loss: 0.394336
[epoch 15, batch   899] avg loss: 0.410401
[epoch 15, batch   999] avg loss: 0.414949
[epoch 15, batch  1099] avg loss: 0.427158
[epoch 15, batch  1199] avg loss: 0.419456
[epoch 15, batch  1299] avg loss: 0.419900
[epoch 15, batch  1399] avg loss: 0.416755
[epoch 15, batch  1499] avg loss: 0.418305
[epoch 15, batch  1599] avg loss: 0.432432
[epoch 15, batch  1699] avg loss: 0.424950
[epoch 15, batch  1799] avg loss: 0.412293
[epoch 15, batch  1899] avg loss: 0.420843
[epoch 15, batch  1999] avg loss: 0.423792
[epoch 15, batch  2099] avg loss: 0.421601
[epoch 15, batch  2199] avg loss: 0.417623
[epoch 15, batch  2299] avg loss: 0.426043
[epoch 15, batch  2399] avg loss: 0.423967
[epoch 16, batch    99] avg loss: 0.421044
[epoch 16, batch   199] avg loss: 0.418241
[epoch 16, batch   299] avg loss: 0.413822
[epoch 16, batch   399] avg loss: 0.431748
[epoch 16, batch   499] avg loss: 0.414784
[epoch 16, batch   599] avg loss: 0.406840
[epoch 16, batch   699] avg loss: 0.435383
[epoch 16, batch   799] avg loss: 0.430998
[epoch 16, batch   899] avg loss: 0.411026
[epoch 16, batch   999] avg loss: 0.419524
[epoch 16, batch  1099] avg loss: 0.400184
[epoch 16, batch  1199] avg loss: 0.416254
[epoch 16, batch  1299] avg loss: 0.419916
[epoch 16, batch  1399] avg loss: 0.404979
[epoch 16, batch  1499] avg loss: 0.415365
[epoch 16, batch  1599] avg loss: 0.432256
[epoch 16, batch  1699] avg loss: 0.426134
[epoch 16, batch  1799] avg loss: 0.412652
[epoch 16, batch  1899] avg loss: 0.418654
[epoch 16, batch  1999] avg loss: 0.420042
[epoch 16, batch  2099] avg loss: 0.406835
[epoch 16, batch  2199] avg loss: 0.411444
[epoch 16, batch  2299] avg loss: 0.425089
[epoch 16, batch  2399] avg loss: 0.418066
[epoch 17, batch    99] avg loss: 0.414080
[epoch 17, batch   199] avg loss: 0.405913
[epoch 17, batch   299] avg loss: 0.418684
[epoch 17, batch   399] avg loss: 0.437595
[epoch 17, batch   499] avg loss: 0.427283
[epoch 17, batch   599] avg loss: 0.414320
[epoch 17, batch   699] avg loss: 0.421457
[epoch 17, batch   799] avg loss: 0.429119
[epoch 17, batch   899] avg loss: 0.408388
[epoch 17, batch   999] avg loss: 0.400711
[epoch 17, batch  1099] avg loss: 0.430080
[epoch 17, batch  1199] avg loss: 0.406984
[epoch 17, batch  1299] avg loss: 0.445401
[epoch 17, batch  1399] avg loss: 0.417636
[epoch 17, batch  1499] avg loss: 0.422893
[epoch 17, batch  1599] avg loss: 0.417773
[epoch 17, batch  1699] avg loss: 0.428664
[epoch 17, batch  1799] avg loss: 0.407963
[epoch 17, batch  1899] avg loss: 0.405516
[epoch 17, batch  1999] avg loss: 0.402728
[epoch 17, batch  2099] avg loss: 0.409521
[epoch 17, batch  2199] avg loss: 0.430377
[epoch 17, batch  2299] avg loss: 0.413024
[epoch 17, batch  2399] avg loss: 0.397193
[epoch 18, batch    99] avg loss: 0.418820
[epoch 18, batch   199] avg loss: 0.415951
[epoch 18, batch   299] avg loss: 0.406390
[epoch 18, batch   399] avg loss: 0.424423
[epoch 18, batch   499] avg loss: 0.411265
[epoch 18, batch   599] avg loss: 0.408734
[epoch 18, batch   699] avg loss: 0.399333
[epoch 18, batch   799] avg loss: 0.415423
[epoch 18, batch   899] avg loss: 0.409551
[epoch 18, batch   999] avg loss: 0.422379
[epoch 18, batch  1099] avg loss: 0.414705
[epoch 18, batch  1199] avg loss: 0.423895
[epoch 18, batch  1299] avg loss: 0.409185
[epoch 18, batch  1399] avg loss: 0.395369
[epoch 18, batch  1499] avg loss: 0.405489
[epoch 18, batch  1599] avg loss: 0.424110
[epoch 18, batch  1699] avg loss: 0.402891
[epoch 18, batch  1799] avg loss: 0.422380
[epoch 18, batch  1899] avg loss: 0.423723
[epoch 18, batch  1999] avg loss: 0.400081
[epoch 18, batch  2099] avg loss: 0.406028
[epoch 18, batch  2199] avg loss: 0.402060
[epoch 18, batch  2299] avg loss: 0.424358
[epoch 18, batch  2399] avg loss: 0.417079
[epoch 19, batch    99] avg loss: 0.416010
[epoch 19, batch   199] avg loss: 0.399874
[epoch 19, batch   299] avg loss: 0.418918
[epoch 19, batch   399] avg loss: 0.417809
[epoch 19, batch   499] avg loss: 0.420051
[epoch 19, batch   599] avg loss: 0.407177
[epoch 19, batch   699] avg loss: 0.401027
[epoch 19, batch   799] avg loss: 0.404537
[epoch 19, batch   899] avg loss: 0.417357
[epoch 19, batch   999] avg loss: 0.430359
[epoch 19, batch  1099] avg loss: 0.393674
[epoch 19, batch  1199] avg loss: 0.403130
[epoch 19, batch  1299] avg loss: 0.411837
[epoch 19, batch  1399] avg loss: 0.402376
[epoch 19, batch  1499] avg loss: 0.417919
[epoch 19, batch  1599] avg loss: 0.406205
[epoch 19, batch  1699] avg loss: 0.397851
[epoch 19, batch  1799] avg loss: 0.404986
[epoch 19, batch  1899] avg loss: 0.398079
[epoch 19, batch  1999] avg loss: 0.408619
[epoch 19, batch  2099] avg loss: 0.409491
[epoch 19, batch  2199] avg loss: 0.417772
[epoch 19, batch  2299] avg loss: 0.411559
[epoch 19, batch  2399] avg loss: 0.406094
Model saved to model/20200502-053237.pth.
accuracy/TriangPrismIsosc : 0.632
n_examples/TriangPrismIsosc : 500.0
accuracy/parallelepiped : 0.362
n_examples/parallelepiped : 500.0
accuracy/sphere : 1.0
n_examples/sphere : 102.0
accuracy/wire : 0.985
n_examples/wire : 200.0
accuracy/avg_geom : 0.6113671274961597
loss/validation_geom : 0.9322038663697133
accuracy/Au : 0.0
n_examples/Au : 0.0
accuracy/SiN : 1.0
n_examples/SiN : 1302.0
accuracy/SiO2 : 0.0
n_examples/SiO2 : 0.0
accuracy/avg_mat : 1.0
loss/validation_mat : 0.0024663650798403907
MSE/ShortestDim : 20.222156512938703
MAE/ShortestDim : 1.8025738837715302
MSE/MiddleDim : 13.961439821211057
MAE/MiddleDim : 2.823766922254899
MSE/LongDim : 109.3219701998428
MAE/LongDim : 6.145564458703482
MSE/log Area/Vol : 4.9482731738581265
MAE/log Area/Vol : 1.5167408048099453
loss/validation_dim : 148.45383970785068
loss/validation : 149.38850993930023
Metrics saved to model/20200502-053237_metrics.csv.
Parsed 2604 rows from data/sim_train_spectrum_SiO2.
Parsed 2604 rows from data/sim_train_labels_SiO2.
Parsed 9765 rows from data/gen_spectrum_SiO2_00-of-16.
Parsed 9765 rows from data/gen_labels_SiO2_00-of-16.
Parsed 9765 rows from data/gen_spectrum_SiO2_01-of-16.
Parsed 9765 rows from data/gen_labels_SiO2_01-of-16.
Parsed 9765 rows from data/gen_spectrum_SiO2_02-of-16.
Parsed 9765 rows from data/gen_labels_SiO2_02-of-16.
Parsed 9765 rows from data/gen_spectrum_SiO2_03-of-16.
Parsed 9765 rows from data/gen_labels_SiO2_03-of-16.
Parsed 9765 rows from data/gen_spectrum_SiO2_04-of-16.
Parsed 9765 rows from data/gen_labels_SiO2_04-of-16.
Parsed 9765 rows from data/gen_spectrum_SiO2_05-of-16.
Parsed 9765 rows from data/gen_labels_SiO2_05-of-16.
Parsed 9765 rows from data/gen_spectrum_SiO2_06-of-16.
Parsed 9765 rows from data/gen_labels_SiO2_06-of-16.
Parsed 9765 rows from data/gen_spectrum_SiO2_07-of-16.
Parsed 9765 rows from data/gen_labels_SiO2_07-of-16.
Parsed 9765 rows from data/gen_spectrum_SiO2_08-of-16.
Parsed 9765 rows from data/gen_labels_SiO2_08-of-16.
Parsed 9765 rows from data/gen_spectrum_SiO2_09-of-16.
Parsed 9765 rows from data/gen_labels_SiO2_09-of-16.
Parsed 9765 rows from data/gen_spectrum_SiO2_10-of-16.
Parsed 9765 rows from data/gen_labels_SiO2_10-of-16.
Parsed 9765 rows from data/gen_spectrum_SiO2_11-of-16.
Parsed 9765 rows from data/gen_labels_SiO2_11-of-16.
Parsed 9765 rows from data/gen_spectrum_SiO2_12-of-16.
Parsed 9765 rows from data/gen_labels_SiO2_12-of-16.
Parsed 9765 rows from data/gen_spectrum_SiO2_13-of-16.
Parsed 9765 rows from data/gen_labels_SiO2_13-of-16.
Parsed 9765 rows from data/gen_spectrum_SiO2_14-of-16.
Parsed 9765 rows from data/gen_labels_SiO2_14-of-16.
Parsed 9765 rows from data/gen_spectrum_SiO2_15-of-16.
Parsed 9765 rows from data/gen_labels_SiO2_15-of-16.
Parsed 1302 rows from data/sim_validation_spectrum_SiO2.
Parsed 1302 rows from data/sim_validation_labels_SiO2.
Logging training progress to tensorboard dir runs/alexnet-SiO2-lr_0.000500-trainsize_158844-05_02_2020_05:33-multistage-joint.
[epoch 0, batch    99] avg loss: 0.328277
[epoch 0, batch   199] avg loss: 0.203635
[epoch 0, batch   299] avg loss: 0.190962
[epoch 0, batch   399] avg loss: 0.203182
[epoch 0, batch   499] avg loss: 0.189000
[epoch 0, batch   599] avg loss: 0.167272
[epoch 0, batch   699] avg loss: 0.161481
[epoch 0, batch   799] avg loss: 0.154644
[epoch 0, batch   899] avg loss: 0.134150
[epoch 0, batch   999] avg loss: 0.139916
[epoch 0, batch  1099] avg loss: 0.140642
[epoch 0, batch  1199] avg loss: 0.118377
[epoch 0, batch  1299] avg loss: 0.116300
[epoch 0, batch  1399] avg loss: 0.118250
[epoch 0, batch  1499] avg loss: 0.113951
[epoch 0, batch  1599] avg loss: 0.116968
[epoch 0, batch  1699] avg loss: 0.106218
[epoch 0, batch  1799] avg loss: 0.111355
[epoch 0, batch  1899] avg loss: 0.108704
[epoch 0, batch  1999] avg loss: 0.104232
[epoch 0, batch  2099] avg loss: 0.101140
[epoch 0, batch  2199] avg loss: 0.098176
[epoch 0, batch  2299] avg loss: 0.100936
[epoch 0, batch  2399] avg loss: 0.100051
[epoch 1, batch    99] avg loss: 0.106576
[epoch 1, batch   199] avg loss: 0.102607
[epoch 1, batch   299] avg loss: 0.092860
[epoch 1, batch   399] avg loss: 0.085785
[epoch 1, batch   499] avg loss: 0.109669
[epoch 1, batch   599] avg loss: 0.096376
[epoch 1, batch   699] avg loss: 0.088441
[epoch 1, batch   799] avg loss: 0.086368
[epoch 1, batch   899] avg loss: 0.106739
[epoch 1, batch   999] avg loss: 0.107440
[epoch 1, batch  1099] avg loss: 0.087941
[epoch 1, batch  1199] avg loss: 0.099991
[epoch 1, batch  1299] avg loss: 0.092843
[epoch 1, batch  1399] avg loss: 0.088699
[epoch 1, batch  1499] avg loss: 0.083108
[epoch 1, batch  1599] avg loss: 0.092975
[epoch 1, batch  1699] avg loss: 0.082743
[epoch 1, batch  1799] avg loss: 0.084604
[epoch 1, batch  1899] avg loss: 0.087909
[epoch 1, batch  1999] avg loss: 0.091872
[epoch 1, batch  2099] avg loss: 0.088526
[epoch 1, batch  2199] avg loss: 0.090690
[epoch 1, batch  2299] avg loss: 0.088393
[epoch 1, batch  2399] avg loss: 0.083627
[epoch 2, batch    99] avg loss: 0.086242
[epoch 2, batch   199] avg loss: 0.085066
[epoch 2, batch   299] avg loss: 0.092215
[epoch 2, batch   399] avg loss: 0.092970
[epoch 2, batch   499] avg loss: 0.085453
[epoch 2, batch   599] avg loss: 0.085490
[epoch 2, batch   699] avg loss: 0.090097
[epoch 2, batch   799] avg loss: 0.085121
[epoch 2, batch   899] avg loss: 0.095093
[epoch 2, batch   999] avg loss: 0.080953
[epoch 2, batch  1099] avg loss: 0.084768
[epoch 2, batch  1199] avg loss: 0.075330
[epoch 2, batch  1299] avg loss: 0.086918
[epoch 2, batch  1399] avg loss: 0.081295
[epoch 2, batch  1499] avg loss: 0.107053
[epoch 2, batch  1599] avg loss: 0.086656
[epoch 2, batch  1699] avg loss: 0.097048
[epoch 2, batch  1799] avg loss: 0.093852
[epoch 2, batch  1899] avg loss: 0.069963
[epoch 2, batch  1999] avg loss: 0.086931
[epoch 2, batch  2099] avg loss: 0.083057
[epoch 2, batch  2199] avg loss: 0.080720
[epoch 2, batch  2299] avg loss: 0.075772
[epoch 2, batch  2399] avg loss: 0.087657
[epoch 3, batch    99] avg loss: 0.078307
[epoch 3, batch   199] avg loss: 0.083541
[epoch 3, batch   299] avg loss: 0.083998
[epoch 3, batch   399] avg loss: 0.085251
[epoch 3, batch   499] avg loss: 0.075690
[epoch 3, batch   599] avg loss: 0.077946
[epoch 3, batch   699] avg loss: 0.096284
[epoch 3, batch   799] avg loss: 0.079768
[epoch 3, batch   899] avg loss: 0.080060
[epoch 3, batch   999] avg loss: 0.085331
[epoch 3, batch  1099] avg loss: 0.075030
[epoch 3, batch  1199] avg loss: 0.103158
[epoch 3, batch  1299] avg loss: 0.077983
[epoch 3, batch  1399] avg loss: 0.077977
[epoch 3, batch  1499] avg loss: 0.083440
[epoch 3, batch  1599] avg loss: 0.079864
[epoch 3, batch  1699] avg loss: 0.094594
[epoch 3, batch  1799] avg loss: 0.095994
[epoch 3, batch  1899] avg loss: 0.082438
[epoch 3, batch  1999] avg loss: 0.076545
[epoch 3, batch  2099] avg loss: 0.084841
[epoch 3, batch  2199] avg loss: 0.087863
[epoch 3, batch  2299] avg loss: 0.080107
[epoch 3, batch  2399] avg loss: 0.081106
[epoch 4, batch    99] avg loss: 0.075472
[epoch 4, batch   199] avg loss: 0.089336
[epoch 4, batch   299] avg loss: 0.076292
[epoch 4, batch   399] avg loss: 0.075998
[epoch 4, batch   499] avg loss: 0.073745
[epoch 4, batch   599] avg loss: 0.074395
[epoch 4, batch   699] avg loss: 0.086609
[epoch 4, batch   799] avg loss: 0.078026
[epoch 4, batch   899] avg loss: 0.077770
[epoch 4, batch   999] avg loss: 0.087555
[epoch 4, batch  1099] avg loss: 0.077195
[epoch 4, batch  1199] avg loss: 0.087895
[epoch 4, batch  1299] avg loss: 0.078237
[epoch 4, batch  1399] avg loss: 0.087269
[epoch 4, batch  1499] avg loss: 0.095472
[epoch 4, batch  1599] avg loss: 0.076610
[epoch 4, batch  1699] avg loss: 0.085487
[epoch 4, batch  1799] avg loss: 0.089714
[epoch 4, batch  1899] avg loss: 0.070932
[epoch 4, batch  1999] avg loss: 0.080066
[epoch 4, batch  2099] avg loss: 0.083096
[epoch 4, batch  2199] avg loss: 0.083902
[epoch 4, batch  2299] avg loss: 0.072676
[epoch 4, batch  2399] avg loss: 0.089063
[epoch 5, batch    99] avg loss: 0.080817
[epoch 5, batch   199] avg loss: 0.079292
[epoch 5, batch   299] avg loss: 0.073022
[epoch 5, batch   399] avg loss: 0.079982
[epoch 5, batch   499] avg loss: 0.072189
[epoch 5, batch   599] avg loss: 0.084269
[epoch 5, batch   699] avg loss: 0.085890
[epoch 5, batch   799] avg loss: 0.073547
[epoch 5, batch   899] avg loss: 0.078760
[epoch 5, batch   999] avg loss: 0.084532
[epoch 5, batch  1099] avg loss: 0.078683
[epoch 5, batch  1199] avg loss: 0.064846
[epoch 5, batch  1299] avg loss: 0.087355
[epoch 5, batch  1399] avg loss: 0.085083
[epoch 5, batch  1499] avg loss: 0.085471
[epoch 5, batch  1599] avg loss: 0.081405
[epoch 5, batch  1699] avg loss: 0.079122
[epoch 5, batch  1799] avg loss: 0.080488
[epoch 5, batch  1899] avg loss: 0.083286
[epoch 5, batch  1999] avg loss: 0.078039
[epoch 5, batch  2099] avg loss: 0.071539
[epoch 5, batch  2199] avg loss: 0.074213
[epoch 5, batch  2299] avg loss: 0.086643
[epoch 5, batch  2399] avg loss: 0.075638
[epoch 6, batch    99] avg loss: 0.085608
[epoch 6, batch   199] avg loss: 0.080947
[epoch 6, batch   299] avg loss: 0.079126
[epoch 6, batch   399] avg loss: 0.080393
[epoch 6, batch   499] avg loss: 0.081932
[epoch 6, batch   599] avg loss: 0.075422
[epoch 6, batch   699] avg loss: 0.085839
[epoch 6, batch   799] avg loss: 0.069092
[epoch 6, batch   899] avg loss: 0.079134
[epoch 6, batch   999] avg loss: 0.083921
[epoch 6, batch  1099] avg loss: 0.076618
[epoch 6, batch  1199] avg loss: 0.070883
[epoch 6, batch  1299] avg loss: 0.073629
[epoch 6, batch  1399] avg loss: 0.082778
[epoch 6, batch  1499] avg loss: 0.078130
[epoch 6, batch  1599] avg loss: 0.073467
[epoch 6, batch  1699] avg loss: 0.080236
[epoch 6, batch  1799] avg loss: 0.079735
[epoch 6, batch  1899] avg loss: 0.075328
[epoch 6, batch  1999] avg loss: 0.072472
[epoch 6, batch  2099] avg loss: 0.080159
[epoch 6, batch  2199] avg loss: 0.082115
[epoch 6, batch  2299] avg loss: 0.072128
[epoch 6, batch  2399] avg loss: 0.066354
[epoch 7, batch    99] avg loss: 0.073611
[epoch 7, batch   199] avg loss: 0.069055
[epoch 7, batch   299] avg loss: 0.070527
[epoch 7, batch   399] avg loss: 0.083028
[epoch 7, batch   499] avg loss: 0.070138
[epoch 7, batch   599] avg loss: 0.076081
[epoch 7, batch   699] avg loss: 0.073945
[epoch 7, batch   799] avg loss: 0.075505
[epoch 7, batch   899] avg loss: 0.083450
[epoch 7, batch   999] avg loss: 0.061422
[epoch 7, batch  1099] avg loss: 0.080056
[epoch 7, batch  1199] avg loss: 0.079156
[epoch 7, batch  1299] avg loss: 0.078333
[epoch 7, batch  1399] avg loss: 0.083069
[epoch 7, batch  1499] avg loss: 0.071132
[epoch 7, batch  1599] avg loss: 0.082735
[epoch 7, batch  1699] avg loss: 0.075646
[epoch 7, batch  1799] avg loss: 0.091167
[epoch 7, batch  1899] avg loss: 0.065363
[epoch 7, batch  1999] avg loss: 0.080896
[epoch 7, batch  2099] avg loss: 0.068410
[epoch 7, batch  2199] avg loss: 0.086324
[epoch 7, batch  2299] avg loss: 0.069740
[epoch 7, batch  2399] avg loss: 0.076521
[epoch 8, batch    99] avg loss: 0.076359
[epoch 8, batch   199] avg loss: 0.075352
[epoch 8, batch   299] avg loss: 0.073810
[epoch 8, batch   399] avg loss: 0.067976
[epoch 8, batch   499] avg loss: 0.071366
[epoch 8, batch   599] avg loss: 0.083569
[epoch 8, batch   699] avg loss: 0.082351
[epoch 8, batch   799] avg loss: 0.068367
[epoch 8, batch   899] avg loss: 0.075061
[epoch 8, batch   999] avg loss: 0.074256
[epoch 8, batch  1099] avg loss: 0.060338
[epoch 8, batch  1199] avg loss: 0.068534
[epoch 8, batch  1299] avg loss: 0.076342
[epoch 8, batch  1399] avg loss: 0.081041
[epoch 8, batch  1499] avg loss: 0.074511
[epoch 8, batch  1599] avg loss: 0.082449
[epoch 8, batch  1699] avg loss: 0.079316
[epoch 8, batch  1799] avg loss: 0.081151
[epoch 8, batch  1899] avg loss: 0.072583
[epoch 8, batch  1999] avg loss: 0.075255
[epoch 8, batch  2099] avg loss: 0.067538
[epoch 8, batch  2199] avg loss: 0.086961
[epoch 8, batch  2299] avg loss: 0.083160
[epoch 8, batch  2399] avg loss: 0.076230
[epoch 9, batch    99] avg loss: 0.088431
[epoch 9, batch   199] avg loss: 0.064433
[epoch 9, batch   299] avg loss: 0.074545
[epoch 9, batch   399] avg loss: 0.072355
[epoch 9, batch   499] avg loss: 0.083000
[epoch 9, batch   599] avg loss: 0.082995
[epoch 9, batch   699] avg loss: 0.074366
[epoch 9, batch   799] avg loss: 0.072721
[epoch 9, batch   899] avg loss: 0.083846
[epoch 9, batch   999] avg loss: 0.074179
[epoch 9, batch  1099] avg loss: 0.076523
[epoch 9, batch  1199] avg loss: 0.067141
[epoch 9, batch  1299] avg loss: 0.072869
[epoch 9, batch  1399] avg loss: 0.075859
[epoch 9, batch  1499] avg loss: 0.072504
[epoch 9, batch  1599] avg loss: 0.073481
[epoch 9, batch  1699] avg loss: 0.074722
[epoch 9, batch  1799] avg loss: 0.071088
[epoch 9, batch  1899] avg loss: 0.061877
[epoch 9, batch  1999] avg loss: 0.070777
[epoch 9, batch  2099] avg loss: 0.067695
[epoch 9, batch  2199] avg loss: 0.073548
[epoch 9, batch  2299] avg loss: 0.078429
[epoch 9, batch  2399] avg loss: 0.068924
[epoch 10, batch    99] avg loss: 0.066847
[epoch 10, batch   199] avg loss: 0.075219
[epoch 10, batch   299] avg loss: 0.082647
[epoch 10, batch   399] avg loss: 0.070237
[epoch 10, batch   499] avg loss: 0.081244
[epoch 10, batch   599] avg loss: 0.076755
[epoch 10, batch   699] avg loss: 0.081036
[epoch 10, batch   799] avg loss: 0.066666
[epoch 10, batch   899] avg loss: 0.072026
[epoch 10, batch   999] avg loss: 0.071985
[epoch 10, batch  1099] avg loss: 0.067302
[epoch 10, batch  1199] avg loss: 0.086408
[epoch 10, batch  1299] avg loss: 0.080575
[epoch 10, batch  1399] avg loss: 0.075655
[epoch 10, batch  1499] avg loss: 0.067444
[epoch 10, batch  1599] avg loss: 0.070256
[epoch 10, batch  1699] avg loss: 0.073547
[epoch 10, batch  1799] avg loss: 0.063434
[epoch 10, batch  1899] avg loss: 0.069154
[epoch 10, batch  1999] avg loss: 0.075817
[epoch 10, batch  2099] avg loss: 0.068800
[epoch 10, batch  2199] avg loss: 0.071291
[epoch 10, batch  2299] avg loss: 0.073770
[epoch 10, batch  2399] avg loss: 0.074496
[epoch 11, batch    99] avg loss: 0.070462
[epoch 11, batch   199] avg loss: 0.067622
[epoch 11, batch   299] avg loss: 0.070595
[epoch 11, batch   399] avg loss: 0.070803
[epoch 11, batch   499] avg loss: 0.077646
[epoch 11, batch   599] avg loss: 0.071926
[epoch 11, batch   699] avg loss: 0.072452
[epoch 11, batch   799] avg loss: 0.075581
[epoch 11, batch   899] avg loss: 0.074771
[epoch 11, batch   999] avg loss: 0.076934
[epoch 11, batch  1099] avg loss: 0.078091
[epoch 11, batch  1199] avg loss: 0.065659
[epoch 11, batch  1299] avg loss: 0.065340
[epoch 11, batch  1399] avg loss: 0.075773
[epoch 11, batch  1499] avg loss: 0.069396
[epoch 11, batch  1599] avg loss: 0.070105
[epoch 11, batch  1699] avg loss: 0.076411
[epoch 11, batch  1799] avg loss: 0.072677
[epoch 11, batch  1899] avg loss: 0.066017
[epoch 11, batch  1999] avg loss: 0.077366
[epoch 11, batch  2099] avg loss: 0.073405
[epoch 11, batch  2199] avg loss: 0.078947
[epoch 11, batch  2299] avg loss: 0.069302
[epoch 11, batch  2399] avg loss: 0.074790
[epoch 12, batch    99] avg loss: 0.066130
[epoch 12, batch   199] avg loss: 0.071093
[epoch 12, batch   299] avg loss: 0.075689
[epoch 12, batch   399] avg loss: 0.073816
[epoch 12, batch   499] avg loss: 0.076294
[epoch 12, batch   599] avg loss: 0.068144
[epoch 12, batch   699] avg loss: 0.066770
[epoch 12, batch   799] avg loss: 0.075984
[epoch 12, batch   899] avg loss: 0.073648
[epoch 12, batch   999] avg loss: 0.072179
[epoch 12, batch  1099] avg loss: 0.076155
[epoch 12, batch  1199] avg loss: 0.076645
[epoch 12, batch  1299] avg loss: 0.073115
[epoch 12, batch  1399] avg loss: 0.071186
[epoch 12, batch  1499] avg loss: 0.070786
[epoch 12, batch  1599] avg loss: 0.064346
[epoch 12, batch  1699] avg loss: 0.063069
[epoch 12, batch  1799] avg loss: 0.064436
[epoch 12, batch  1899] avg loss: 0.071167
[epoch 12, batch  1999] avg loss: 0.086404
[epoch 12, batch  2099] avg loss: 0.068024
[epoch 12, batch  2199] avg loss: 0.068805
[epoch 12, batch  2299] avg loss: 0.074590
[epoch 12, batch  2399] avg loss: 0.069412
[epoch 13, batch    99] avg loss: 0.066071
[epoch 13, batch   199] avg loss: 0.077101
[epoch 13, batch   299] avg loss: 0.068629
[epoch 13, batch   399] avg loss: 0.072811
[epoch 13, batch   499] avg loss: 0.080745
[epoch 13, batch   599] avg loss: 0.075393
[epoch 13, batch   699] avg loss: 0.068321
[epoch 13, batch   799] avg loss: 0.066449
[epoch 13, batch   899] avg loss: 0.069152
[epoch 13, batch   999] avg loss: 0.064700
[epoch 13, batch  1099] avg loss: 0.070606
[epoch 13, batch  1199] avg loss: 0.071643
[epoch 13, batch  1299] avg loss: 0.071246
[epoch 13, batch  1399] avg loss: 0.071015
[epoch 13, batch  1499] avg loss: 0.072811
[epoch 13, batch  1599] avg loss: 0.069721
[epoch 13, batch  1699] avg loss: 0.064831
[epoch 13, batch  1799] avg loss: 0.072720
[epoch 13, batch  1899] avg loss: 0.078749
[epoch 13, batch  1999] avg loss: 0.081388
[epoch 13, batch  2099] avg loss: 0.080574
[epoch 13, batch  2199] avg loss: 0.071528
[epoch 13, batch  2299] avg loss: 0.065674
[epoch 13, batch  2399] avg loss: 0.066686
[epoch 14, batch    99] avg loss: 0.070766
[epoch 14, batch   199] avg loss: 0.082231
[epoch 14, batch   299] avg loss: 0.076237
[epoch 14, batch   399] avg loss: 0.069313
[epoch 14, batch   499] avg loss: 0.070793
[epoch 14, batch   599] avg loss: 0.070602
[epoch 14, batch   699] avg loss: 0.067525
[epoch 14, batch   799] avg loss: 0.069906
[epoch 14, batch   899] avg loss: 0.069070
[epoch 14, batch   999] avg loss: 0.071213
[epoch 14, batch  1099] avg loss: 0.066740
[epoch 14, batch  1199] avg loss: 0.066545
[epoch 14, batch  1299] avg loss: 0.060500
[epoch 14, batch  1399] avg loss: 0.068632
[epoch 14, batch  1499] avg loss: 0.069042
[epoch 14, batch  1599] avg loss: 0.075237
[epoch 14, batch  1699] avg loss: 0.075069
[epoch 14, batch  1799] avg loss: 0.070949
[epoch 14, batch  1899] avg loss: 0.071962
[epoch 14, batch  1999] avg loss: 0.075502
[epoch 14, batch  2099] avg loss: 0.065443
[epoch 14, batch  2199] avg loss: 0.065489
[epoch 14, batch  2299] avg loss: 0.075483
[epoch 14, batch  2399] avg loss: 0.083539
[epoch 15, batch    99] avg loss: 0.071747
[epoch 15, batch   199] avg loss: 0.065339
[epoch 15, batch   299] avg loss: 0.069694
[epoch 15, batch   399] avg loss: 0.082619
[epoch 15, batch   499] avg loss: 0.071425
[epoch 15, batch   599] avg loss: 0.071290
[epoch 15, batch   699] avg loss: 0.058821
[epoch 15, batch   799] avg loss: 0.076222
[epoch 15, batch   899] avg loss: 0.077585
[epoch 15, batch   999] avg loss: 0.071949
[epoch 15, batch  1099] avg loss: 0.071306
[epoch 15, batch  1199] avg loss: 0.073605
[epoch 15, batch  1299] avg loss: 0.074826
[epoch 15, batch  1399] avg loss: 0.061995
[epoch 15, batch  1499] avg loss: 0.078013
[epoch 15, batch  1599] avg loss: 0.070264
[epoch 15, batch  1699] avg loss: 0.071583
[epoch 15, batch  1799] avg loss: 0.065523
[epoch 15, batch  1899] avg loss: 0.071149
[epoch 15, batch  1999] avg loss: 0.066614
[epoch 15, batch  2099] avg loss: 0.074041
[epoch 15, batch  2199] avg loss: 0.061672
[epoch 15, batch  2299] avg loss: 0.071264
[epoch 15, batch  2399] avg loss: 0.070071
[epoch 16, batch    99] avg loss: 0.068952
[epoch 16, batch   199] avg loss: 0.071798
[epoch 16, batch   299] avg loss: 0.066041
[epoch 16, batch   399] avg loss: 0.072256
[epoch 16, batch   499] avg loss: 0.061002
[epoch 16, batch   599] avg loss: 0.081123
[epoch 16, batch   699] avg loss: 0.072092
[epoch 16, batch   799] avg loss: 0.069167
[epoch 16, batch   899] avg loss: 0.086281
[epoch 16, batch   999] avg loss: 0.070183
[epoch 16, batch  1099] avg loss: 0.067529
[epoch 16, batch  1199] avg loss: 0.064627
[epoch 16, batch  1299] avg loss: 0.068217
[epoch 16, batch  1399] avg loss: 0.068130
[epoch 16, batch  1499] avg loss: 0.067193
[epoch 16, batch  1599] avg loss: 0.068031
[epoch 16, batch  1699] avg loss: 0.072563
[epoch 16, batch  1799] avg loss: 0.074202
[epoch 16, batch  1899] avg loss: 0.062102
[epoch 16, batch  1999] avg loss: 0.066043
[epoch 16, batch  2099] avg loss: 0.076973
[epoch 16, batch  2199] avg loss: 0.066511
[epoch 16, batch  2299] avg loss: 0.070954
[epoch 16, batch  2399] avg loss: 0.064826
[epoch 17, batch    99] avg loss: 0.062578
[epoch 17, batch   199] avg loss: 0.070289
[epoch 17, batch   299] avg loss: 0.070174
[epoch 17, batch   399] avg loss: 0.065592
[epoch 17, batch   499] avg loss: 0.057738
[epoch 17, batch   599] avg loss: 0.072486
[epoch 17, batch   699] avg loss: 0.069738
[epoch 17, batch   799] avg loss: 0.065766
[epoch 17, batch   899] avg loss: 0.078281
[epoch 17, batch   999] avg loss: 0.069882
[epoch 17, batch  1099] avg loss: 0.073800
[epoch 17, batch  1199] avg loss: 0.060304
[epoch 17, batch  1299] avg loss: 0.080969
[epoch 17, batch  1399] avg loss: 0.069658
[epoch 17, batch  1499] avg loss: 0.069154
[epoch 17, batch  1599] avg loss: 0.071526
[epoch 17, batch  1699] avg loss: 0.062056
[epoch 17, batch  1799] avg loss: 0.070385
[epoch 17, batch  1899] avg loss: 0.076116
[epoch 17, batch  1999] avg loss: 0.075801
[epoch 17, batch  2099] avg loss: 0.071607
[epoch 17, batch  2199] avg loss: 0.069853
[epoch 17, batch  2299] avg loss: 0.063254
[epoch 17, batch  2399] avg loss: 0.070676
[epoch 18, batch    99] avg loss: 0.066276
[epoch 18, batch   199] avg loss: 0.072213
[epoch 18, batch   299] avg loss: 0.065943
[epoch 18, batch   399] avg loss: 0.067323
[epoch 18, batch   499] avg loss: 0.063252
[epoch 18, batch   599] avg loss: 0.060724
[epoch 18, batch   699] avg loss: 0.073871
[epoch 18, batch   799] avg loss: 0.071545
[epoch 18, batch   899] avg loss: 0.069918
[epoch 18, batch   999] avg loss: 0.069369
[epoch 18, batch  1099] avg loss: 0.065459
[epoch 18, batch  1199] avg loss: 0.070538
[epoch 18, batch  1299] avg loss: 0.071147
[epoch 18, batch  1399] avg loss: 0.077051
[epoch 18, batch  1499] avg loss: 0.067299
[epoch 18, batch  1599] avg loss: 0.076355
[epoch 18, batch  1699] avg loss: 0.064657
[epoch 18, batch  1799] avg loss: 0.060816
[epoch 18, batch  1899] avg loss: 0.069526
[epoch 18, batch  1999] avg loss: 0.064953
[epoch 18, batch  2099] avg loss: 0.066163
[epoch 18, batch  2199] avg loss: 0.072721
[epoch 18, batch  2299] avg loss: 0.081023
[epoch 18, batch  2399] avg loss: 0.071033
[epoch 19, batch    99] avg loss: 0.069990
[epoch 19, batch   199] avg loss: 0.070439
[epoch 19, batch   299] avg loss: 0.065062
[epoch 19, batch   399] avg loss: 0.069307
[epoch 19, batch   499] avg loss: 0.067213
[epoch 19, batch   599] avg loss: 0.067825
[epoch 19, batch   699] avg loss: 0.079396
[epoch 19, batch   799] avg loss: 0.076168
[epoch 19, batch   899] avg loss: 0.061730
[epoch 19, batch   999] avg loss: 0.070620
[epoch 19, batch  1099] avg loss: 0.077901
[epoch 19, batch  1199] avg loss: 0.070254
[epoch 19, batch  1299] avg loss: 0.064367
[epoch 19, batch  1399] avg loss: 0.062496
[epoch 19, batch  1499] avg loss: 0.076807
[epoch 19, batch  1599] avg loss: 0.071437
[epoch 19, batch  1699] avg loss: 0.062056
[epoch 19, batch  1799] avg loss: 0.069394
[epoch 19, batch  1899] avg loss: 0.073578
[epoch 19, batch  1999] avg loss: 0.070391
[epoch 19, batch  2099] avg loss: 0.074107
[epoch 19, batch  2199] avg loss: 0.057570
[epoch 19, batch  2299] avg loss: 0.073956
[epoch 19, batch  2399] avg loss: 0.060445
Model saved to model/20200502-054848.pth.
accuracy/TriangPrismIsosc : 0.8
n_examples/TriangPrismIsosc : 500.0
accuracy/parallelepiped : 0.014
n_examples/parallelepiped : 500.0
accuracy/sphere : 0.049019607843137254
n_examples/sphere : 102.0
accuracy/wire : 0.165
n_examples/wire : 200.0
accuracy/avg_geom : 0.34178187403993854
loss/validation_geom : 1.4346195636989518
accuracy/Au : 0.0
n_examples/Au : 0.0
accuracy/SiN : 0.0
n_examples/SiN : 0.0
accuracy/SiO2 : 1.0
n_examples/SiO2 : 1302.0
accuracy/avg_mat : 1.0
loss/validation_mat : 0.0
MSE/ShortestDim : 0.13224254857559908
MAE/ShortestDim : 0.2153634699075819
MSE/MiddleDim : 0.576010590874105
MAE/MiddleDim : 0.44535013570946663
MSE/LongDim : 16.373176797377532
MAE/LongDim : 2.1260879504882064
MSE/log Area/Vol : 0.12199101332695253
MAE/log Area/Vol : 0.25967304021715204
loss/validation_dim : 17.203420950154186
loss/validation : 18.638040513853138
Metrics saved to model/20200502-054848_metrics.csv.
[epoch 0, batch    99] avg loss: 0.962054
[epoch 0, batch   199] avg loss: 0.657013
[epoch 0, batch   299] avg loss: 0.627440
[epoch 0, batch   399] avg loss: 0.583968
[epoch 0, batch   499] avg loss: 0.560940
[epoch 0, batch   599] avg loss: 0.551898
[epoch 0, batch   699] avg loss: 0.558631
[epoch 0, batch   799] avg loss: 0.544386
[epoch 0, batch   899] avg loss: 0.545722
[epoch 0, batch   999] avg loss: 0.542752
[epoch 0, batch  1099] avg loss: 0.518418
[epoch 0, batch  1199] avg loss: 0.526916
[epoch 0, batch  1299] avg loss: 0.501017
[epoch 0, batch  1399] avg loss: 0.526319
[epoch 0, batch  1499] avg loss: 0.515731
[epoch 0, batch  1599] avg loss: 0.496374
[epoch 0, batch  1699] avg loss: 0.497715
[epoch 0, batch  1799] avg loss: 0.506363
[epoch 0, batch  1899] avg loss: 0.503193
[epoch 0, batch  1999] avg loss: 0.501255
[epoch 0, batch  2099] avg loss: 0.488749
[epoch 0, batch  2199] avg loss: 0.505963
[epoch 0, batch  2299] avg loss: 0.503267
[epoch 0, batch  2399] avg loss: 0.486425
[epoch 1, batch    99] avg loss: 0.495750
[epoch 1, batch   199] avg loss: 0.482177
[epoch 1, batch   299] avg loss: 0.487203
[epoch 1, batch   399] avg loss: 0.490087
[epoch 1, batch   499] avg loss: 0.487265
[epoch 1, batch   599] avg loss: 0.468101
[epoch 1, batch   699] avg loss: 0.479978
[epoch 1, batch   799] avg loss: 0.484047
[epoch 1, batch   899] avg loss: 0.460823
[epoch 1, batch   999] avg loss: 0.474148
[epoch 1, batch  1099] avg loss: 0.474631
[epoch 1, batch  1199] avg loss: 0.485046
[epoch 1, batch  1299] avg loss: 0.479010
[epoch 1, batch  1399] avg loss: 0.487104
[epoch 1, batch  1499] avg loss: 0.474148
[epoch 1, batch  1599] avg loss: 0.476848
[epoch 1, batch  1699] avg loss: 0.480682
[epoch 1, batch  1799] avg loss: 0.463648
[epoch 1, batch  1899] avg loss: 0.484900
[epoch 1, batch  1999] avg loss: 0.460898
[epoch 1, batch  2099] avg loss: 0.467831
[epoch 1, batch  2199] avg loss: 0.456356
[epoch 1, batch  2299] avg loss: 0.475898
[epoch 1, batch  2399] avg loss: 0.461412
[epoch 2, batch    99] avg loss: 0.482424
[epoch 2, batch   199] avg loss: 0.459710
[epoch 2, batch   299] avg loss: 0.460877
[epoch 2, batch   399] avg loss: 0.448517
[epoch 2, batch   499] avg loss: 0.453156
[epoch 2, batch   599] avg loss: 0.449188
[epoch 2, batch   699] avg loss: 0.461641
[epoch 2, batch   799] avg loss: 0.448478
[epoch 2, batch   899] avg loss: 0.452765
[epoch 2, batch   999] avg loss: 0.470287
[epoch 2, batch  1099] avg loss: 0.456033
[epoch 2, batch  1199] avg loss: 0.451403
[epoch 2, batch  1299] avg loss: 0.468766
[epoch 2, batch  1399] avg loss: 0.445847
[epoch 2, batch  1499] avg loss: 0.448593
[epoch 2, batch  1599] avg loss: 0.462410
[epoch 2, batch  1699] avg loss: 0.457109
[epoch 2, batch  1799] avg loss: 0.453193
[epoch 2, batch  1899] avg loss: 0.461122
[epoch 2, batch  1999] avg loss: 0.448128
[epoch 2, batch  2099] avg loss: 0.448971
[epoch 2, batch  2199] avg loss: 0.449142
[epoch 2, batch  2299] avg loss: 0.454804
[epoch 2, batch  2399] avg loss: 0.446877
[epoch 3, batch    99] avg loss: 0.445105
[epoch 3, batch   199] avg loss: 0.445953
[epoch 3, batch   299] avg loss: 0.443085
[epoch 3, batch   399] avg loss: 0.438768
[epoch 3, batch   499] avg loss: 0.447642
[epoch 3, batch   599] avg loss: 0.452386
[epoch 3, batch   699] avg loss: 0.426821
[epoch 3, batch   799] avg loss: 0.440761
[epoch 3, batch   899] avg loss: 0.437917
[epoch 3, batch   999] avg loss: 0.450022
[epoch 3, batch  1099] avg loss: 0.438965
[epoch 3, batch  1199] avg loss: 0.434389
[epoch 3, batch  1299] avg loss: 0.442028
[epoch 3, batch  1399] avg loss: 0.434644
[epoch 3, batch  1499] avg loss: 0.417077
[epoch 3, batch  1599] avg loss: 0.439509
[epoch 3, batch  1699] avg loss: 0.442838
[epoch 3, batch  1799] avg loss: 0.434819
[epoch 3, batch  1899] avg loss: 0.445029
[epoch 3, batch  1999] avg loss: 0.437342
[epoch 3, batch  2099] avg loss: 0.436333
[epoch 3, batch  2199] avg loss: 0.424869
[epoch 3, batch  2299] avg loss: 0.429403
[epoch 3, batch  2399] avg loss: 0.435008
[epoch 4, batch    99] avg loss: 0.436982
[epoch 4, batch   199] avg loss: 0.420797
[epoch 4, batch   299] avg loss: 0.430564
[epoch 4, batch   399] avg loss: 0.436836
[epoch 4, batch   499] avg loss: 0.420786
[epoch 4, batch   599] avg loss: 0.428591
[epoch 4, batch   699] avg loss: 0.434373
[epoch 4, batch   799] avg loss: 0.425799
[epoch 4, batch   899] avg loss: 0.424724
[epoch 4, batch   999] avg loss: 0.437171
[epoch 4, batch  1099] avg loss: 0.418531
[epoch 4, batch  1199] avg loss: 0.426780
[epoch 4, batch  1299] avg loss: 0.434904
[epoch 4, batch  1399] avg loss: 0.416052
[epoch 4, batch  1499] avg loss: 0.424009
[epoch 4, batch  1599] avg loss: 0.423721
[epoch 4, batch  1699] avg loss: 0.412798
[epoch 4, batch  1799] avg loss: 0.432247
[epoch 4, batch  1899] avg loss: 0.422406
[epoch 4, batch  1999] avg loss: 0.427864
[epoch 4, batch  2099] avg loss: 0.432755
[epoch 4, batch  2199] avg loss: 0.424732
[epoch 4, batch  2299] avg loss: 0.425297
[epoch 4, batch  2399] avg loss: 0.416967
[epoch 5, batch    99] avg loss: 0.417914
[epoch 5, batch   199] avg loss: 0.416524
[epoch 5, batch   299] avg loss: 0.423093
[epoch 5, batch   399] avg loss: 0.416516
[epoch 5, batch   499] avg loss: 0.414384
[epoch 5, batch   599] avg loss: 0.425022
[epoch 5, batch   699] avg loss: 0.412973
[epoch 5, batch   799] avg loss: 0.406726
[epoch 5, batch   899] avg loss: 0.416066
[epoch 5, batch   999] avg loss: 0.422174
[epoch 5, batch  1099] avg loss: 0.419264
[epoch 5, batch  1199] avg loss: 0.430038
[epoch 5, batch  1299] avg loss: 0.408784
[epoch 5, batch  1399] avg loss: 0.420074
[epoch 5, batch  1499] avg loss: 0.402067
[epoch 5, batch  1599] avg loss: 0.418715
[epoch 5, batch  1699] avg loss: 0.441447
[epoch 5, batch  1799] avg loss: 0.425558
[epoch 5, batch  1899] avg loss: 0.407858
[epoch 5, batch  1999] avg loss: 0.423661
[epoch 5, batch  2099] avg loss: 0.396656
[epoch 5, batch  2199] avg loss: 0.417763
[epoch 5, batch  2299] avg loss: 0.399212
[epoch 5, batch  2399] avg loss: 0.425802
[epoch 6, batch    99] avg loss: 0.410109
[epoch 6, batch   199] avg loss: 0.401501
[epoch 6, batch   299] avg loss: 0.411007
[epoch 6, batch   399] avg loss: 0.414012
[epoch 6, batch   499] avg loss: 0.401789
[epoch 6, batch   599] avg loss: 0.408006
[epoch 6, batch   699] avg loss: 0.415123
[epoch 6, batch   799] avg loss: 0.406522
[epoch 6, batch   899] avg loss: 0.401057
[epoch 6, batch   999] avg loss: 0.401495
[epoch 6, batch  1099] avg loss: 0.413853
[epoch 6, batch  1199] avg loss: 0.404736
[epoch 6, batch  1299] avg loss: 0.413966
[epoch 6, batch  1399] avg loss: 0.404245
[epoch 6, batch  1499] avg loss: 0.389166
[epoch 6, batch  1599] avg loss: 0.398185
[epoch 6, batch  1699] avg loss: 0.401954
[epoch 6, batch  1799] avg loss: 0.396425
[epoch 6, batch  1899] avg loss: 0.407848
[epoch 6, batch  1999] avg loss: 0.392360
[epoch 6, batch  2099] avg loss: 0.390650
[epoch 6, batch  2199] avg loss: 0.405458
[epoch 6, batch  2299] avg loss: 0.398777
[epoch 6, batch  2399] avg loss: 0.395806
[epoch 7, batch    99] avg loss: 0.400359
[epoch 7, batch   199] avg loss: 0.398630
[epoch 7, batch   299] avg loss: 0.405834
[epoch 7, batch   399] avg loss: 0.394967
[epoch 7, batch   499] avg loss: 0.376597
[epoch 7, batch   599] avg loss: 0.396643
[epoch 7, batch   699] avg loss: 0.383114
[epoch 7, batch   799] avg loss: 0.395381
[epoch 7, batch   899] avg loss: 0.386540
[epoch 7, batch   999] avg loss: 0.394665
[epoch 7, batch  1099] avg loss: 0.395974
[epoch 7, batch  1199] avg loss: 0.391458
[epoch 7, batch  1299] avg loss: 0.391588
[epoch 7, batch  1399] avg loss: 0.381171
[epoch 7, batch  1499] avg loss: 0.387248
[epoch 7, batch  1599] avg loss: 0.391895
[epoch 7, batch  1699] avg loss: 0.410914
[epoch 7, batch  1799] avg loss: 0.397516
[epoch 7, batch  1899] avg loss: 0.382997
[epoch 7, batch  1999] avg loss: 0.401565
[epoch 7, batch  2099] avg loss: 0.391836
[epoch 7, batch  2199] avg loss: 0.408656
[epoch 7, batch  2299] avg loss: 0.395231
[epoch 7, batch  2399] avg loss: 0.397014
[epoch 8, batch    99] avg loss: 0.385905
[epoch 8, batch   199] avg loss: 0.402129
[epoch 8, batch   299] avg loss: 0.391866
[epoch 8, batch   399] avg loss: 0.376083
[epoch 8, batch   499] avg loss: 0.377035
[epoch 8, batch   599] avg loss: 0.389655
[epoch 8, batch   699] avg loss: 0.379702
[epoch 8, batch   799] avg loss: 0.389960
[epoch 8, batch   899] avg loss: 0.393212
[epoch 8, batch   999] avg loss: 0.382433
[epoch 8, batch  1099] avg loss: 0.385104
[epoch 8, batch  1199] avg loss: 0.385599
[epoch 8, batch  1299] avg loss: 0.382128
[epoch 8, batch  1399] avg loss: 0.388494
[epoch 8, batch  1499] avg loss: 0.381371
[epoch 8, batch  1599] avg loss: 0.394859
[epoch 8, batch  1699] avg loss: 0.383344
[epoch 8, batch  1799] avg loss: 0.370716
[epoch 8, batch  1899] avg loss: 0.391172
[epoch 8, batch  1999] avg loss: 0.387827
[epoch 8, batch  2099] avg loss: 0.375678
[epoch 8, batch  2199] avg loss: 0.373882
[epoch 8, batch  2299] avg loss: 0.385702
[epoch 8, batch  2399] avg loss: 0.386324
[epoch 9, batch    99] avg loss: 0.396670
[epoch 9, batch   199] avg loss: 0.384352
[epoch 9, batch   299] avg loss: 0.371417
[epoch 9, batch   399] avg loss: 0.387805
[epoch 9, batch   499] avg loss: 0.367514
[epoch 9, batch   599] avg loss: 0.369286
[epoch 9, batch   699] avg loss: 0.402069
[epoch 9, batch   799] avg loss: 0.375643
[epoch 9, batch   899] avg loss: 0.379776
[epoch 9, batch   999] avg loss: 0.375439
[epoch 9, batch  1099] avg loss: 0.374336
[epoch 9, batch  1199] avg loss: 0.386639
[epoch 9, batch  1299] avg loss: 0.377210
[epoch 9, batch  1399] avg loss: 0.371115
[epoch 9, batch  1499] avg loss: 0.375294
[epoch 9, batch  1599] avg loss: 0.375111
[epoch 9, batch  1699] avg loss: 0.400307
[epoch 9, batch  1799] avg loss: 0.367997
[epoch 9, batch  1899] avg loss: 0.362724
[epoch 9, batch  1999] avg loss: 0.382381
[epoch 9, batch  2099] avg loss: 0.389959
[epoch 9, batch  2199] avg loss: 0.382079
[epoch 9, batch  2299] avg loss: 0.384095
[epoch 9, batch  2399] avg loss: 0.378738
[epoch 10, batch    99] avg loss: 0.376074
[epoch 10, batch   199] avg loss: 0.375789
[epoch 10, batch   299] avg loss: 0.369029
[epoch 10, batch   399] avg loss: 0.385137
[epoch 10, batch   499] avg loss: 0.370642
[epoch 10, batch   599] avg loss: 0.381009
[epoch 10, batch   699] avg loss: 0.377422
[epoch 10, batch   799] avg loss: 0.369790
[epoch 10, batch   899] avg loss: 0.374684
[epoch 10, batch   999] avg loss: 0.390521
[epoch 10, batch  1099] avg loss: 0.366206
[epoch 10, batch  1199] avg loss: 0.368901
[epoch 10, batch  1299] avg loss: 0.373493
[epoch 10, batch  1399] avg loss: 0.365875
[epoch 10, batch  1499] avg loss: 0.394990
[epoch 10, batch  1599] avg loss: 0.365490
[epoch 10, batch  1699] avg loss: 0.369883
[epoch 10, batch  1799] avg loss: 0.370715
[epoch 10, batch  1899] avg loss: 0.363994
[epoch 10, batch  1999] avg loss: 0.345884
[epoch 10, batch  2099] avg loss: 0.366279
[epoch 10, batch  2199] avg loss: 0.370749
[epoch 10, batch  2299] avg loss: 0.357492
[epoch 10, batch  2399] avg loss: 0.369816
[epoch 11, batch    99] avg loss: 0.377603
[epoch 11, batch   199] avg loss: 0.365783
[epoch 11, batch   299] avg loss: 0.374742
[epoch 11, batch   399] avg loss: 0.360354
[epoch 11, batch   499] avg loss: 0.343555
[epoch 11, batch   599] avg loss: 0.376144
[epoch 11, batch   699] avg loss: 0.366091
[epoch 11, batch   799] avg loss: 0.368180
[epoch 11, batch   899] avg loss: 0.356832
[epoch 11, batch   999] avg loss: 0.361675
[epoch 11, batch  1099] avg loss: 0.357401
[epoch 11, batch  1199] avg loss: 0.377449
[epoch 11, batch  1299] avg loss: 0.358159
[epoch 11, batch  1399] avg loss: 0.362604
[epoch 11, batch  1499] avg loss: 0.372587
[epoch 11, batch  1599] avg loss: 0.359975
[epoch 11, batch  1699] avg loss: 0.367823
[epoch 11, batch  1799] avg loss: 0.363108
[epoch 11, batch  1899] avg loss: 0.346259
[epoch 11, batch  1999] avg loss: 0.372427
[epoch 11, batch  2099] avg loss: 0.360797
[epoch 11, batch  2199] avg loss: 0.357919
[epoch 11, batch  2299] avg loss: 0.371635
[epoch 11, batch  2399] avg loss: 0.362120
[epoch 12, batch    99] avg loss: 0.363168
[epoch 12, batch   199] avg loss: 0.378861
[epoch 12, batch   299] avg loss: 0.355505
[epoch 12, batch   399] avg loss: 0.354320
[epoch 12, batch   499] avg loss: 0.358873
[epoch 12, batch   599] avg loss: 0.358569
[epoch 12, batch   699] avg loss: 0.371990
[epoch 12, batch   799] avg loss: 0.349350
[epoch 12, batch   899] avg loss: 0.360219
[epoch 12, batch   999] avg loss: 0.366043
[epoch 12, batch  1099] avg loss: 0.357809
[epoch 12, batch  1199] avg loss: 0.361692
[epoch 12, batch  1299] avg loss: 0.349657
[epoch 12, batch  1399] avg loss: 0.359509
[epoch 12, batch  1499] avg loss: 0.357815
[epoch 12, batch  1599] avg loss: 0.372238
[epoch 12, batch  1699] avg loss: 0.347321
[epoch 12, batch  1799] avg loss: 0.368862
[epoch 12, batch  1899] avg loss: 0.364778
[epoch 12, batch  1999] avg loss: 0.352903
[epoch 12, batch  2099] avg loss: 0.363577
[epoch 12, batch  2199] avg loss: 0.349215
[epoch 12, batch  2299] avg loss: 0.345309
[epoch 12, batch  2399] avg loss: 0.350277
[epoch 13, batch    99] avg loss: 0.349127
[epoch 13, batch   199] avg loss: 0.354989
[epoch 13, batch   299] avg loss: 0.362773
[epoch 13, batch   399] avg loss: 0.354270
[epoch 13, batch   499] avg loss: 0.362994
[epoch 13, batch   599] avg loss: 0.353557
[epoch 13, batch   699] avg loss: 0.359419
[epoch 13, batch   799] avg loss: 0.357976
[epoch 13, batch   899] avg loss: 0.355064
[epoch 13, batch   999] avg loss: 0.344475
[epoch 13, batch  1099] avg loss: 0.340881
[epoch 13, batch  1199] avg loss: 0.346067
[epoch 13, batch  1299] avg loss: 0.354234
[epoch 13, batch  1399] avg loss: 0.351570
[epoch 13, batch  1499] avg loss: 0.350071
[epoch 13, batch  1599] avg loss: 0.352982
[epoch 13, batch  1699] avg loss: 0.352173
[epoch 13, batch  1799] avg loss: 0.340419
[epoch 13, batch  1899] avg loss: 0.364707
[epoch 13, batch  1999] avg loss: 0.363166
[epoch 13, batch  2099] avg loss: 0.343400
[epoch 13, batch  2199] avg loss: 0.359287
[epoch 13, batch  2299] avg loss: 0.343957
[epoch 13, batch  2399] avg loss: 0.353245
[epoch 14, batch    99] avg loss: 0.350326
[epoch 14, batch   199] avg loss: 0.352969
[epoch 14, batch   299] avg loss: 0.345431
[epoch 14, batch   399] avg loss: 0.360841
[epoch 14, batch   499] avg loss: 0.359260
[epoch 14, batch   599] avg loss: 0.330793
[epoch 14, batch   699] avg loss: 0.345277
[epoch 14, batch   799] avg loss: 0.351352
[epoch 14, batch   899] avg loss: 0.330178
[epoch 14, batch   999] avg loss: 0.348108
[epoch 14, batch  1099] avg loss: 0.359832
[epoch 14, batch  1199] avg loss: 0.361776
[epoch 14, batch  1299] avg loss: 0.351659
[epoch 14, batch  1399] avg loss: 0.338815
[epoch 14, batch  1499] avg loss: 0.349455
[epoch 14, batch  1599] avg loss: 0.351209
[epoch 14, batch  1699] avg loss: 0.337931
[epoch 14, batch  1799] avg loss: 0.353338
[epoch 14, batch  1899] avg loss: 0.340495
[epoch 14, batch  1999] avg loss: 0.334372
[epoch 14, batch  2099] avg loss: 0.360114
[epoch 14, batch  2199] avg loss: 0.357486
[epoch 14, batch  2299] avg loss: 0.336165
[epoch 14, batch  2399] avg loss: 0.340523
[epoch 15, batch    99] avg loss: 0.332999
[epoch 15, batch   199] avg loss: 0.351342
[epoch 15, batch   299] avg loss: 0.339457
[epoch 15, batch   399] avg loss: 0.348261
[epoch 15, batch   499] avg loss: 0.343716
[epoch 15, batch   599] avg loss: 0.364036
[epoch 15, batch   699] avg loss: 0.343890
[epoch 15, batch   799] avg loss: 0.334353
[epoch 15, batch   899] avg loss: 0.345520
[epoch 15, batch   999] avg loss: 0.349916
[epoch 15, batch  1099] avg loss: 0.361013
[epoch 15, batch  1199] avg loss: 0.342012
[epoch 15, batch  1299] avg loss: 0.343728
[epoch 15, batch  1399] avg loss: 0.343179
[epoch 15, batch  1499] avg loss: 0.339899
[epoch 15, batch  1599] avg loss: 0.346629
[epoch 15, batch  1699] avg loss: 0.336635
[epoch 15, batch  1799] avg loss: 0.348826
[epoch 15, batch  1899] avg loss: 0.338545
[epoch 15, batch  1999] avg loss: 0.326837
[epoch 15, batch  2099] avg loss: 0.348210
[epoch 15, batch  2199] avg loss: 0.335925
[epoch 15, batch  2299] avg loss: 0.348888
[epoch 15, batch  2399] avg loss: 0.337348
[epoch 16, batch    99] avg loss: 0.338207
[epoch 16, batch   199] avg loss: 0.350192
[epoch 16, batch   299] avg loss: 0.339927
[epoch 16, batch   399] avg loss: 0.339103
[epoch 16, batch   499] avg loss: 0.341683
[epoch 16, batch   599] avg loss: 0.337808
[epoch 16, batch   699] avg loss: 0.360669
[epoch 16, batch   799] avg loss: 0.331320
[epoch 16, batch   899] avg loss: 0.339717
[epoch 16, batch   999] avg loss: 0.342011
[epoch 16, batch  1099] avg loss: 0.340233
[epoch 16, batch  1199] avg loss: 0.338191
[epoch 16, batch  1299] avg loss: 0.337228
[epoch 16, batch  1399] avg loss: 0.332077
[epoch 16, batch  1499] avg loss: 0.325545
[epoch 16, batch  1599] avg loss: 0.326407
[epoch 16, batch  1699] avg loss: 0.343835
[epoch 16, batch  1799] avg loss: 0.340875
[epoch 16, batch  1899] avg loss: 0.340335
[epoch 16, batch  1999] avg loss: 0.335452
[epoch 16, batch  2099] avg loss: 0.343894
[epoch 16, batch  2199] avg loss: 0.330218
[epoch 16, batch  2299] avg loss: 0.340900
[epoch 16, batch  2399] avg loss: 0.330179
[epoch 17, batch    99] avg loss: 0.314201
[epoch 17, batch   199] avg loss: 0.334950
[epoch 17, batch   299] avg loss: 0.324464
[epoch 17, batch   399] avg loss: 0.336106
[epoch 17, batch   499] avg loss: 0.334630
[epoch 17, batch   599] avg loss: 0.334548
[epoch 17, batch   699] avg loss: 0.337776
[epoch 17, batch   799] avg loss: 0.337428
[epoch 17, batch   899] avg loss: 0.328864
[epoch 17, batch   999] avg loss: 0.321144
[epoch 17, batch  1099] avg loss: 0.321154
[epoch 17, batch  1199] avg loss: 0.345046
[epoch 17, batch  1299] avg loss: 0.331503
[epoch 17, batch  1399] avg loss: 0.343837
[epoch 17, batch  1499] avg loss: 0.336008
[epoch 17, batch  1599] avg loss: 0.335928
[epoch 17, batch  1699] avg loss: 0.319216
[epoch 17, batch  1799] avg loss: 0.342911
[epoch 17, batch  1899] avg loss: 0.335391
[epoch 17, batch  1999] avg loss: 0.331058
[epoch 17, batch  2099] avg loss: 0.331586
[epoch 17, batch  2199] avg loss: 0.329599
[epoch 17, batch  2299] avg loss: 0.326090
[epoch 17, batch  2399] avg loss: 0.338736
[epoch 18, batch    99] avg loss: 0.338080
[epoch 18, batch   199] avg loss: 0.322573
[epoch 18, batch   299] avg loss: 0.352485
[epoch 18, batch   399] avg loss: 0.316890
[epoch 18, batch   499] avg loss: 0.324575
[epoch 18, batch   599] avg loss: 0.317056
[epoch 18, batch   699] avg loss: 0.334494
[epoch 18, batch   799] avg loss: 0.320476
[epoch 18, batch   899] avg loss: 0.340582
[epoch 18, batch   999] avg loss: 0.325421
[epoch 18, batch  1099] avg loss: 0.317490
[epoch 18, batch  1199] avg loss: 0.338804
[epoch 18, batch  1299] avg loss: 0.336445
[epoch 18, batch  1399] avg loss: 0.324139
[epoch 18, batch  1499] avg loss: 0.335836
[epoch 18, batch  1599] avg loss: 0.334379
[epoch 18, batch  1699] avg loss: 0.322136
[epoch 18, batch  1799] avg loss: 0.322786
[epoch 18, batch  1899] avg loss: 0.322592
[epoch 18, batch  1999] avg loss: 0.327326
[epoch 18, batch  2099] avg loss: 0.334465
[epoch 18, batch  2199] avg loss: 0.322281
[epoch 18, batch  2299] avg loss: 0.317982
[epoch 18, batch  2399] avg loss: 0.335401
[epoch 19, batch    99] avg loss: 0.329041
[epoch 19, batch   199] avg loss: 0.321912
[epoch 19, batch   299] avg loss: 0.328443
[epoch 19, batch   399] avg loss: 0.323854
[epoch 19, batch   499] avg loss: 0.325231
[epoch 19, batch   599] avg loss: 0.318997
[epoch 19, batch   699] avg loss: 0.336208
[epoch 19, batch   799] avg loss: 0.337112
[epoch 19, batch   899] avg loss: 0.321435
[epoch 19, batch   999] avg loss: 0.321340
[epoch 19, batch  1099] avg loss: 0.321460
[epoch 19, batch  1199] avg loss: 0.306196
[epoch 19, batch  1299] avg loss: 0.320450
[epoch 19, batch  1399] avg loss: 0.322848
[epoch 19, batch  1499] avg loss: 0.322302
[epoch 19, batch  1599] avg loss: 0.310796
[epoch 19, batch  1699] avg loss: 0.323508
[epoch 19, batch  1799] avg loss: 0.332237
[epoch 19, batch  1899] avg loss: 0.316844
[epoch 19, batch  1999] avg loss: 0.342584
[epoch 19, batch  2099] avg loss: 0.322304
[epoch 19, batch  2199] avg loss: 0.314219
[epoch 19, batch  2299] avg loss: 0.324534
[epoch 19, batch  2399] avg loss: 0.322745
Model saved to model/20200502-060542.pth.
accuracy/TriangPrismIsosc : 0.768
n_examples/TriangPrismIsosc : 500.0
accuracy/parallelepiped : 0.578
n_examples/parallelepiped : 500.0
accuracy/sphere : 1.0
n_examples/sphere : 102.0
accuracy/wire : 0.965
n_examples/wire : 200.0
accuracy/avg_geom : 0.7434715821812596
loss/validation_geom : 0.6894783768602597
accuracy/Au : 0.0
n_examples/Au : 0.0
accuracy/SiN : 0.0
n_examples/SiN : 0.0
accuracy/SiO2 : 1.0
n_examples/SiO2 : 1302.0
accuracy/avg_mat : 1.0
loss/validation_mat : 0.003256985148022984
MSE/ShortestDim : 13.053764085432718
MAE/ShortestDim : 1.9102491856353807
MSE/MiddleDim : 62.92768714541481
MAE/MiddleDim : 4.282730893605316
MSE/LongDim : 2137.2169187578006
MAE/LongDim : 19.958904951948174
MSE/log Area/Vol : 27.79576671031946
MAE/log Area/Vol : 3.0292810712541853
loss/validation_dim : 2240.994136698968
loss/validation : 2241.686872060976
Metrics saved to model/20200502-060542_metrics.csv.
Parsed 7812 rows from data/sim_train_spectrum_all.
Parsed 7812 rows from data/sim_train_labels_all.
Parsed 9765 rows from data/gen_spectrum_all_00-of-16.
Parsed 9765 rows from data/gen_labels_all_00-of-16.
Parsed 9765 rows from data/gen_spectrum_all_01-of-16.
Parsed 9765 rows from data/gen_labels_all_01-of-16.
Parsed 9765 rows from data/gen_spectrum_all_02-of-16.
Parsed 9765 rows from data/gen_labels_all_02-of-16.
Parsed 9765 rows from data/gen_spectrum_all_03-of-16.
Parsed 9765 rows from data/gen_labels_all_03-of-16.
Parsed 9765 rows from data/gen_spectrum_all_04-of-16.
Parsed 9765 rows from data/gen_labels_all_04-of-16.
Parsed 9765 rows from data/gen_spectrum_all_05-of-16.
Parsed 9765 rows from data/gen_labels_all_05-of-16.
Parsed 9765 rows from data/gen_spectrum_all_06-of-16.
Parsed 9765 rows from data/gen_labels_all_06-of-16.
Parsed 9765 rows from data/gen_spectrum_all_07-of-16.
Parsed 9765 rows from data/gen_labels_all_07-of-16.
Parsed 9765 rows from data/gen_spectrum_all_08-of-16.
Parsed 9765 rows from data/gen_labels_all_08-of-16.
Parsed 9765 rows from data/gen_spectrum_all_09-of-16.
Parsed 9765 rows from data/gen_labels_all_09-of-16.
Parsed 9765 rows from data/gen_spectrum_all_10-of-16.
Parsed 9765 rows from data/gen_labels_all_10-of-16.
Parsed 9765 rows from data/gen_spectrum_all_11-of-16.
Parsed 9765 rows from data/gen_labels_all_11-of-16.
Parsed 9765 rows from data/gen_spectrum_all_12-of-16.
Parsed 9765 rows from data/gen_labels_all_12-of-16.
Parsed 9765 rows from data/gen_spectrum_all_13-of-16.
Parsed 9765 rows from data/gen_labels_all_13-of-16.
Parsed 9765 rows from data/gen_spectrum_all_14-of-16.
Parsed 9765 rows from data/gen_labels_all_14-of-16.
Parsed 9765 rows from data/gen_spectrum_all_15-of-16.
Parsed 9765 rows from data/gen_labels_all_15-of-16.
Parsed 3906 rows from data/sim_validation_spectrum_all.
Parsed 3906 rows from data/sim_validation_labels_all.
Logging training progress to tensorboard dir runs/alexnet-all-lr_0.000500-trainsize_164052-05_02_2020_06:06-multistage.
[epoch 0, batch    99] avg loss: 0.560211
[epoch 0, batch   199] avg loss: 0.036682
[epoch 0, batch   299] avg loss: 0.013139
[epoch 0, batch   399] avg loss: 0.013316
[epoch 0, batch   499] avg loss: 0.007059
[epoch 0, batch   599] avg loss: 0.001158
[epoch 0, batch   699] avg loss: 0.001699
[epoch 0, batch   799] avg loss: 0.010512
[epoch 0, batch   899] avg loss: 0.008736
[epoch 0, batch   999] avg loss: 0.001399
[epoch 0, batch  1099] avg loss: 0.000717
[epoch 0, batch  1199] avg loss: 0.006461
[epoch 0, batch  1299] avg loss: 0.002785
[epoch 0, batch  1399] avg loss: 0.005064
[epoch 0, batch  1499] avg loss: 0.004647
[epoch 0, batch  1599] avg loss: 0.002024
[epoch 0, batch  1699] avg loss: 0.000162
[epoch 0, batch  1799] avg loss: 0.000103
[epoch 0, batch  1899] avg loss: 0.000062
[epoch 0, batch  1999] avg loss: 0.000058
[epoch 0, batch  2099] avg loss: 0.000031
[epoch 0, batch  2199] avg loss: 0.000025
[epoch 0, batch  2299] avg loss: 0.000023
[epoch 0, batch  2399] avg loss: 0.000032
[epoch 0, batch  2499] avg loss: 0.027180
[epoch 1, batch    99] avg loss: 0.001581
[epoch 1, batch   199] avg loss: 0.002357
[epoch 1, batch   299] avg loss: 0.000264
[epoch 1, batch   399] avg loss: 0.000085
[epoch 1, batch   499] avg loss: 0.000051
[epoch 1, batch   599] avg loss: 0.000069
[epoch 1, batch   699] avg loss: 0.004671
[epoch 1, batch   799] avg loss: 0.002075
[epoch 1, batch   899] avg loss: 0.000658
[epoch 1, batch   999] avg loss: 0.000223
[epoch 1, batch  1099] avg loss: 0.000070
[epoch 1, batch  1199] avg loss: 0.000049
[epoch 1, batch  1299] avg loss: 0.000069
[epoch 1, batch  1399] avg loss: 0.000059
[epoch 1, batch  1499] avg loss: 0.000037
[epoch 1, batch  1599] avg loss: 0.001723
[epoch 1, batch  1699] avg loss: 0.001957
[epoch 1, batch  1799] avg loss: 0.000207
[epoch 1, batch  1899] avg loss: 0.000002
[epoch 1, batch  1999] avg loss: 0.000007
[epoch 1, batch  2099] avg loss: 0.000004
[epoch 1, batch  2199] avg loss: 0.000002
[epoch 1, batch  2299] avg loss: 0.002092
[epoch 1, batch  2399] avg loss: 0.000382
[epoch 1, batch  2499] avg loss: 0.000135
[epoch 2, batch    99] avg loss: 0.000010
[epoch 2, batch   199] avg loss: 0.000008
[epoch 2, batch   299] avg loss: 0.000017
[epoch 2, batch   399] avg loss: 0.000006
[epoch 2, batch   499] avg loss: 0.000013
[epoch 2, batch   599] avg loss: 0.000008
[epoch 2, batch   699] avg loss: 0.000002
[epoch 2, batch   799] avg loss: 0.000009
[epoch 2, batch   899] avg loss: 0.000003
[epoch 2, batch   999] avg loss: 0.000010
[epoch 2, batch  1099] avg loss: 0.000004
[epoch 2, batch  1199] avg loss: 0.000003
[epoch 2, batch  1299] avg loss: 0.000003
[epoch 2, batch  1399] avg loss: 0.000018
[epoch 2, batch  1499] avg loss: 0.000001
[epoch 2, batch  1599] avg loss: 0.000005
[epoch 2, batch  1699] avg loss: 0.000000
[epoch 2, batch  1799] avg loss: 0.000004
[epoch 2, batch  1899] avg loss: 0.000001
[epoch 2, batch  1999] avg loss: 0.000001
[epoch 2, batch  2099] avg loss: 0.000002
[epoch 2, batch  2199] avg loss: 0.000001
[epoch 2, batch  2299] avg loss: 0.000003
[epoch 2, batch  2399] avg loss: 0.000028
[epoch 2, batch  2499] avg loss: 0.000011
[epoch 3, batch    99] avg loss: 0.000006
[epoch 3, batch   199] avg loss: 0.000000
[epoch 3, batch   299] avg loss: 0.000001
[epoch 3, batch   399] avg loss: 0.000000
[epoch 3, batch   499] avg loss: 0.000000
[epoch 3, batch   599] avg loss: 0.000000
[epoch 3, batch   699] avg loss: 0.000001
[epoch 3, batch   799] avg loss: 0.000001
[epoch 3, batch   899] avg loss: 0.000001
[epoch 3, batch   999] avg loss: 0.000003
[epoch 3, batch  1099] avg loss: 0.000003
[epoch 3, batch  1199] avg loss: 0.000000
[epoch 3, batch  1299] avg loss: 0.002371
[epoch 3, batch  1399] avg loss: 0.043263
[epoch 3, batch  1499] avg loss: 0.002085
[epoch 3, batch  1599] avg loss: 0.000870
[epoch 3, batch  1699] avg loss: 0.000160
[epoch 3, batch  1799] avg loss: 0.000053
[epoch 3, batch  1899] avg loss: 0.000032
[epoch 3, batch  1999] avg loss: 0.000025
[epoch 3, batch  2099] avg loss: 0.000026
[epoch 3, batch  2199] avg loss: 0.001582
[epoch 3, batch  2299] avg loss: 0.000068
[epoch 3, batch  2399] avg loss: 0.000025
[epoch 3, batch  2499] avg loss: 0.000021
[epoch 4, batch    99] avg loss: 0.000106
[epoch 4, batch   199] avg loss: 0.000032
[epoch 4, batch   299] avg loss: 0.000024
[epoch 4, batch   399] avg loss: 0.000028
[epoch 4, batch   499] avg loss: 0.000013
[epoch 4, batch   599] avg loss: 0.000008
[epoch 4, batch   699] avg loss: 0.000008
[epoch 4, batch   799] avg loss: 0.000007
[epoch 4, batch   899] avg loss: 0.000004
[epoch 4, batch   999] avg loss: 0.000003
[epoch 4, batch  1099] avg loss: 0.000003
[epoch 4, batch  1199] avg loss: 0.000005
[epoch 4, batch  1299] avg loss: 0.000003
[epoch 4, batch  1399] avg loss: 0.000004
[epoch 4, batch  1499] avg loss: 0.000002
[epoch 4, batch  1599] avg loss: 0.000003
[epoch 4, batch  1699] avg loss: 0.000001
[epoch 4, batch  1799] avg loss: 0.000002
[epoch 4, batch  1899] avg loss: 0.000002
[epoch 4, batch  1999] avg loss: 0.000003
[epoch 4, batch  2099] avg loss: 0.000001
[epoch 4, batch  2199] avg loss: 0.000005
[epoch 4, batch  2299] avg loss: 0.000001
[epoch 4, batch  2399] avg loss: 0.000003
[epoch 4, batch  2499] avg loss: 0.000001
Model saved to model/20200502-061126.pth.
accuracy/TriangPrismIsosc : 0.0
n_examples/TriangPrismIsosc : 1500.0
accuracy/parallelepiped : 0.5606666666666666
n_examples/parallelepiped : 1500.0
accuracy/sphere : 0.0
n_examples/sphere : 306.0
accuracy/wire : 0.45
n_examples/wire : 600.0
accuracy/avg_geom : 0.2844342037890425
loss/validation_geom : 7.0040287851860015
accuracy/Au : 1.0
n_examples/Au : 1302.0
accuracy/SiN : 1.0
n_examples/SiN : 1302.0
accuracy/SiO2 : 1.0
n_examples/SiO2 : 1302.0
accuracy/avg_mat : 1.0
loss/validation_mat : 1.0769258955141048e-06
MSE/ShortestDim : 317.4134668837556
MAE/ShortestDim : 11.254282955749794
MSE/MiddleDim : 244.99295161140313
MAE/MiddleDim : 9.628039712119701
MSE/LongDim : 215.29993170344153
MAE/LongDim : 9.779856154690384
MSE/log Area/Vol : 26.443266247457807
MAE/log Area/Vol : 4.00905861124526
loss/validation_dim : 804.1496164460581
loss/validation : 811.15364630817
Metrics saved to model/20200502-061126_metrics.csv.
[epoch 0, batch    99] avg loss: 0.303859
[epoch 0, batch   199] avg loss: 0.210302
[epoch 0, batch   299] avg loss: 0.166589
[epoch 0, batch   399] avg loss: 0.142536
[epoch 0, batch   499] avg loss: 0.155639
[epoch 0, batch   599] avg loss: 0.136220
[epoch 0, batch   699] avg loss: 0.126419
[epoch 0, batch   799] avg loss: 0.131262
[epoch 0, batch   899] avg loss: 0.120816
[epoch 0, batch   999] avg loss: 0.135716
[epoch 0, batch  1099] avg loss: 0.125741
[epoch 0, batch  1199] avg loss: 0.115621
[epoch 0, batch  1299] avg loss: 0.121296
[epoch 0, batch  1399] avg loss: 0.126794
[epoch 0, batch  1499] avg loss: 0.126463
[epoch 0, batch  1599] avg loss: 0.107473
[epoch 0, batch  1699] avg loss: 0.119261
[epoch 0, batch  1799] avg loss: 0.120541
[epoch 0, batch  1899] avg loss: 0.115457
[epoch 0, batch  1999] avg loss: 0.106727
[epoch 0, batch  2099] avg loss: 0.099085
[epoch 0, batch  2199] avg loss: 0.099886
[epoch 0, batch  2299] avg loss: 0.097035
[epoch 0, batch  2399] avg loss: 0.104500
[epoch 0, batch  2499] avg loss: 0.101001
[epoch 1, batch    99] avg loss: 0.107097
[epoch 1, batch   199] avg loss: 0.098642
[epoch 1, batch   299] avg loss: 0.101517
[epoch 1, batch   399] avg loss: 0.091621
[epoch 1, batch   499] avg loss: 0.096005
[epoch 1, batch   599] avg loss: 0.096108
[epoch 1, batch   699] avg loss: 0.094344
[epoch 1, batch   799] avg loss: 0.091227
[epoch 1, batch   899] avg loss: 0.090663
[epoch 1, batch   999] avg loss: 0.082851
[epoch 1, batch  1099] avg loss: 0.091115
[epoch 1, batch  1199] avg loss: 0.093395
[epoch 1, batch  1299] avg loss: 0.077349
[epoch 1, batch  1399] avg loss: 0.085395
[epoch 1, batch  1499] avg loss: 0.087231
[epoch 1, batch  1599] avg loss: 0.094090
[epoch 1, batch  1699] avg loss: 0.091924
[epoch 1, batch  1799] avg loss: 0.078283
[epoch 1, batch  1899] avg loss: 0.077953
[epoch 1, batch  1999] avg loss: 0.089979
[epoch 1, batch  2099] avg loss: 0.075177
[epoch 1, batch  2199] avg loss: 0.076786
[epoch 1, batch  2299] avg loss: 0.080732
[epoch 1, batch  2399] avg loss: 0.079740
[epoch 1, batch  2499] avg loss: 0.090774
[epoch 2, batch    99] avg loss: 0.081702
[epoch 2, batch   199] avg loss: 0.078625
[epoch 2, batch   299] avg loss: 0.073525
[epoch 2, batch   399] avg loss: 0.078550
[epoch 2, batch   499] avg loss: 0.080820
[epoch 2, batch   599] avg loss: 0.079236
[epoch 2, batch   699] avg loss: 0.080927
[epoch 2, batch   799] avg loss: 0.072068
[epoch 2, batch   899] avg loss: 0.092961
[epoch 2, batch   999] avg loss: 0.083923
[epoch 2, batch  1099] avg loss: 0.087557
[epoch 2, batch  1199] avg loss: 0.085027
[epoch 2, batch  1299] avg loss: 0.070942
[epoch 2, batch  1399] avg loss: 0.078922
[epoch 2, batch  1499] avg loss: 0.107417
[epoch 2, batch  1599] avg loss: 0.093256
[epoch 2, batch  1699] avg loss: 0.086376
[epoch 2, batch  1799] avg loss: 0.079237
[epoch 2, batch  1899] avg loss: 0.071512
[epoch 2, batch  1999] avg loss: 0.066965
[epoch 2, batch  2099] avg loss: 0.083433
[epoch 2, batch  2199] avg loss: 0.075355
[epoch 2, batch  2299] avg loss: 0.076631
[epoch 2, batch  2399] avg loss: 0.073164
[epoch 2, batch  2499] avg loss: 0.078318
[epoch 3, batch    99] avg loss: 0.072367
[epoch 3, batch   199] avg loss: 0.083689
[epoch 3, batch   299] avg loss: 0.072151
[epoch 3, batch   399] avg loss: 0.077718
[epoch 3, batch   499] avg loss: 0.078263
[epoch 3, batch   599] avg loss: 0.067147
[epoch 3, batch   699] avg loss: 0.075722
[epoch 3, batch   799] avg loss: 0.072937
[epoch 3, batch   899] avg loss: 0.075263
[epoch 3, batch   999] avg loss: 0.061667
[epoch 3, batch  1099] avg loss: 0.073601
[epoch 3, batch  1199] avg loss: 0.084361
[epoch 3, batch  1299] avg loss: 0.068056
[epoch 3, batch  1399] avg loss: 0.072506
[epoch 3, batch  1499] avg loss: 0.076273
[epoch 3, batch  1599] avg loss: 0.076941
[epoch 3, batch  1699] avg loss: 0.074616
[epoch 3, batch  1799] avg loss: 0.067208
[epoch 3, batch  1899] avg loss: 0.070283
[epoch 3, batch  1999] avg loss: 0.073704
[epoch 3, batch  2099] avg loss: 0.066277
[epoch 3, batch  2199] avg loss: 0.067115
[epoch 3, batch  2299] avg loss: 0.075040
[epoch 3, batch  2399] avg loss: 0.071368
[epoch 3, batch  2499] avg loss: 0.071869
[epoch 4, batch    99] avg loss: 0.088476
[epoch 4, batch   199] avg loss: 0.078148
[epoch 4, batch   299] avg loss: 0.071473
[epoch 4, batch   399] avg loss: 0.069074
[epoch 4, batch   499] avg loss: 0.063791
[epoch 4, batch   599] avg loss: 0.066371
[epoch 4, batch   699] avg loss: 0.070243
[epoch 4, batch   799] avg loss: 0.067587
[epoch 4, batch   899] avg loss: 0.074671
[epoch 4, batch   999] avg loss: 0.071999
[epoch 4, batch  1099] avg loss: 0.073951
[epoch 4, batch  1199] avg loss: 0.098756
[epoch 4, batch  1299] avg loss: 0.073882
[epoch 4, batch  1399] avg loss: 0.067939
[epoch 4, batch  1499] avg loss: 0.068626
[epoch 4, batch  1599] avg loss: 0.069498
[epoch 4, batch  1699] avg loss: 0.061551
[epoch 4, batch  1799] avg loss: 0.066267
[epoch 4, batch  1899] avg loss: 0.058844
[epoch 4, batch  1999] avg loss: 0.059583
[epoch 4, batch  2099] avg loss: 0.062549
[epoch 4, batch  2199] avg loss: 0.069084
[epoch 4, batch  2299] avg loss: 0.067680
[epoch 4, batch  2399] avg loss: 0.074569
[epoch 4, batch  2499] avg loss: 0.075527
[epoch 5, batch    99] avg loss: 0.063402
[epoch 5, batch   199] avg loss: 0.075038
[epoch 5, batch   299] avg loss: 0.071711
[epoch 5, batch   399] avg loss: 0.069658
[epoch 5, batch   499] avg loss: 0.064995
[epoch 5, batch   599] avg loss: 0.065110
[epoch 5, batch   699] avg loss: 0.073478
[epoch 5, batch   799] avg loss: 0.064424
[epoch 5, batch   899] avg loss: 0.053801
[epoch 5, batch   999] avg loss: 0.077068
[epoch 5, batch  1099] avg loss: 0.066361
[epoch 5, batch  1199] avg loss: 0.058085
[epoch 5, batch  1299] avg loss: 0.068048
[epoch 5, batch  1399] avg loss: 0.081517
[epoch 5, batch  1499] avg loss: 0.080548
[epoch 5, batch  1599] avg loss: 0.074477
[epoch 5, batch  1699] avg loss: 0.069655
[epoch 5, batch  1799] avg loss: 0.059397
[epoch 5, batch  1899] avg loss: 0.060498
[epoch 5, batch  1999] avg loss: 0.060521
[epoch 5, batch  2099] avg loss: 0.063518
[epoch 5, batch  2199] avg loss: 0.061279
[epoch 5, batch  2299] avg loss: 0.060104
[epoch 5, batch  2399] avg loss: 0.065805
[epoch 5, batch  2499] avg loss: 0.057220
[epoch 6, batch    99] avg loss: 0.066738
[epoch 6, batch   199] avg loss: 0.060823
[epoch 6, batch   299] avg loss: 0.070490
[epoch 6, batch   399] avg loss: 0.063086
[epoch 6, batch   499] avg loss: 0.054365
[epoch 6, batch   599] avg loss: 0.063380
[epoch 6, batch   699] avg loss: 0.071454
[epoch 6, batch   799] avg loss: 0.063099
[epoch 6, batch   899] avg loss: 0.056653
[epoch 6, batch   999] avg loss: 0.065540
[epoch 6, batch  1099] avg loss: 0.065746
[epoch 6, batch  1199] avg loss: 0.059675
[epoch 6, batch  1299] avg loss: 0.064167
[epoch 6, batch  1399] avg loss: 0.076836
[epoch 6, batch  1499] avg loss: 0.075174
[epoch 6, batch  1599] avg loss: 0.069573
[epoch 6, batch  1699] avg loss: 0.061673
[epoch 6, batch  1799] avg loss: 0.066588
[epoch 6, batch  1899] avg loss: 0.061892
[epoch 6, batch  1999] avg loss: 0.068087
[epoch 6, batch  2099] avg loss: 0.061425
[epoch 6, batch  2199] avg loss: 0.051992
[epoch 6, batch  2299] avg loss: 0.056799
[epoch 6, batch  2399] avg loss: 0.051817
[epoch 6, batch  2499] avg loss: 0.066931
[epoch 7, batch    99] avg loss: 0.054959
[epoch 7, batch   199] avg loss: 0.072083
[epoch 7, batch   299] avg loss: 0.063510
[epoch 7, batch   399] avg loss: 0.069004
[epoch 7, batch   499] avg loss: 0.081809
[epoch 7, batch   599] avg loss: 0.066591
[epoch 7, batch   699] avg loss: 0.069110
[epoch 7, batch   799] avg loss: 0.066598
[epoch 7, batch   899] avg loss: 0.068173
[epoch 7, batch   999] avg loss: 0.062553
[epoch 7, batch  1099] avg loss: 0.056910
[epoch 7, batch  1199] avg loss: 0.056468
[epoch 7, batch  1299] avg loss: 0.068391
[epoch 7, batch  1399] avg loss: 0.058774
[epoch 7, batch  1499] avg loss: 0.061348
[epoch 7, batch  1599] avg loss: 0.061499
[epoch 7, batch  1699] avg loss: 0.062387
[epoch 7, batch  1799] avg loss: 0.052314
[epoch 7, batch  1899] avg loss: 0.062778
[epoch 7, batch  1999] avg loss: 0.073316
[epoch 7, batch  2099] avg loss: 0.066813
[epoch 7, batch  2199] avg loss: 0.061422
[epoch 7, batch  2299] avg loss: 0.056028
[epoch 7, batch  2399] avg loss: 0.061483
[epoch 7, batch  2499] avg loss: 0.057921
[epoch 8, batch    99] avg loss: 0.061359
[epoch 8, batch   199] avg loss: 0.055296
[epoch 8, batch   299] avg loss: 0.066186
[epoch 8, batch   399] avg loss: 0.047197
[epoch 8, batch   499] avg loss: 0.066031
[epoch 8, batch   599] avg loss: 0.066457
[epoch 8, batch   699] avg loss: 0.060568
[epoch 8, batch   799] avg loss: 0.057547
[epoch 8, batch   899] avg loss: 0.064088
[epoch 8, batch   999] avg loss: 0.067563
[epoch 8, batch  1099] avg loss: 0.066546
[epoch 8, batch  1199] avg loss: 0.053772
[epoch 8, batch  1299] avg loss: 0.061662
[epoch 8, batch  1399] avg loss: 0.067265
[epoch 8, batch  1499] avg loss: 0.059735
[epoch 8, batch  1599] avg loss: 0.058218
[epoch 8, batch  1699] avg loss: 0.071836
[epoch 8, batch  1799] avg loss: 0.055538
[epoch 8, batch  1899] avg loss: 0.064792
[epoch 8, batch  1999] avg loss: 0.054632
[epoch 8, batch  2099] avg loss: 0.052074
[epoch 8, batch  2199] avg loss: 0.058996
[epoch 8, batch  2299] avg loss: 0.060422
[epoch 8, batch  2399] avg loss: 0.069259
[epoch 8, batch  2499] avg loss: 0.059211
[epoch 9, batch    99] avg loss: 0.054073
[epoch 9, batch   199] avg loss: 0.052140
[epoch 9, batch   299] avg loss: 0.059840
[epoch 9, batch   399] avg loss: 0.055861
[epoch 9, batch   499] avg loss: 0.059613
[epoch 9, batch   599] avg loss: 0.055187
[epoch 9, batch   699] avg loss: 0.057912
[epoch 9, batch   799] avg loss: 0.062575
[epoch 9, batch   899] avg loss: 0.074040
[epoch 9, batch   999] avg loss: 0.051242
[epoch 9, batch  1099] avg loss: 0.051126
[epoch 9, batch  1199] avg loss: 0.063292
[epoch 9, batch  1299] avg loss: 0.057778
[epoch 9, batch  1399] avg loss: 0.067505
[epoch 9, batch  1499] avg loss: 0.057322
[epoch 9, batch  1599] avg loss: 0.068279
[epoch 9, batch  1699] avg loss: 0.061083
[epoch 9, batch  1799] avg loss: 0.061399
[epoch 9, batch  1899] avg loss: 0.054008
[epoch 9, batch  1999] avg loss: 0.059370
[epoch 9, batch  2099] avg loss: 0.061125
[epoch 9, batch  2199] avg loss: 0.058964
[epoch 9, batch  2299] avg loss: 0.056473
[epoch 9, batch  2399] avg loss: 0.061893
[epoch 9, batch  2499] avg loss: 0.064807
[epoch 10, batch    99] avg loss: 0.052487
[epoch 10, batch   199] avg loss: 0.062977
[epoch 10, batch   299] avg loss: 0.058821
[epoch 10, batch   399] avg loss: 0.056558
[epoch 10, batch   499] avg loss: 0.049725
[epoch 10, batch   599] avg loss: 0.061580
[epoch 10, batch   699] avg loss: 0.059491
[epoch 10, batch   799] avg loss: 0.064227
[epoch 10, batch   899] avg loss: 0.055696
[epoch 10, batch   999] avg loss: 0.061125
[epoch 10, batch  1099] avg loss: 0.057166
[epoch 10, batch  1199] avg loss: 0.061322
[epoch 10, batch  1299] avg loss: 0.062375
[epoch 10, batch  1399] avg loss: 0.061357
[epoch 10, batch  1499] avg loss: 0.063863
[epoch 10, batch  1599] avg loss: 0.060012
[epoch 10, batch  1699] avg loss: 0.059663
[epoch 10, batch  1799] avg loss: 0.066467
[epoch 10, batch  1899] avg loss: 0.066045
[epoch 10, batch  1999] avg loss: 0.053312
[epoch 10, batch  2099] avg loss: 0.060602
[epoch 10, batch  2199] avg loss: 0.056718
[epoch 10, batch  2299] avg loss: 0.060801
[epoch 10, batch  2399] avg loss: 0.053730
[epoch 10, batch  2499] avg loss: 0.056963
[epoch 11, batch    99] avg loss: 0.058779
[epoch 11, batch   199] avg loss: 0.057319
[epoch 11, batch   299] avg loss: 0.048146
[epoch 11, batch   399] avg loss: 0.064412
[epoch 11, batch   499] avg loss: 0.055531
[epoch 11, batch   599] avg loss: 0.051250
[epoch 11, batch   699] avg loss: 0.055915
[epoch 11, batch   799] avg loss: 0.058262
[epoch 11, batch   899] avg loss: 0.060817
[epoch 11, batch   999] avg loss: 0.062404
[epoch 11, batch  1099] avg loss: 0.059184
[epoch 11, batch  1199] avg loss: 0.065423
[epoch 11, batch  1299] avg loss: 0.068897
[epoch 11, batch  1399] avg loss: 0.058832
[epoch 11, batch  1499] avg loss: 0.064946
[epoch 11, batch  1599] avg loss: 0.054673
[epoch 11, batch  1699] avg loss: 0.057476
[epoch 11, batch  1799] avg loss: 0.056571
[epoch 11, batch  1899] avg loss: 0.052704
[epoch 11, batch  1999] avg loss: 0.055916
[epoch 11, batch  2099] avg loss: 0.062217
[epoch 11, batch  2199] avg loss: 0.053594
[epoch 11, batch  2299] avg loss: 0.055108
[epoch 11, batch  2399] avg loss: 0.057102
[epoch 11, batch  2499] avg loss: 0.050794
[epoch 12, batch    99] avg loss: 0.053856
[epoch 12, batch   199] avg loss: 0.054825
[epoch 12, batch   299] avg loss: 0.052141
[epoch 12, batch   399] avg loss: 0.054223
[epoch 12, batch   499] avg loss: 0.056459
[epoch 12, batch   599] avg loss: 0.056235
[epoch 12, batch   699] avg loss: 0.056566
[epoch 12, batch   799] avg loss: 0.058440
[epoch 12, batch   899] avg loss: 0.061609
[epoch 12, batch   999] avg loss: 0.064987
[epoch 12, batch  1099] avg loss: 0.055392
[epoch 12, batch  1199] avg loss: 0.051618
[epoch 12, batch  1299] avg loss: 0.057443
[epoch 12, batch  1399] avg loss: 0.057354
[epoch 12, batch  1499] avg loss: 0.058333
[epoch 12, batch  1599] avg loss: 0.056489
[epoch 12, batch  1699] avg loss: 0.058416
[epoch 12, batch  1799] avg loss: 0.069494
[epoch 12, batch  1899] avg loss: 0.060001
[epoch 12, batch  1999] avg loss: 0.065680
[epoch 12, batch  2099] avg loss: 0.069761
[epoch 12, batch  2199] avg loss: 0.063500
[epoch 12, batch  2299] avg loss: 0.049766
[epoch 12, batch  2399] avg loss: 0.053531
[epoch 12, batch  2499] avg loss: 0.064025
[epoch 13, batch    99] avg loss: 0.061984
[epoch 13, batch   199] avg loss: 0.055040
[epoch 13, batch   299] avg loss: 0.049876
[epoch 13, batch   399] avg loss: 0.052617
[epoch 13, batch   499] avg loss: 0.056004
[epoch 13, batch   599] avg loss: 0.054070
[epoch 13, batch   699] avg loss: 0.061871
[epoch 13, batch   799] avg loss: 0.055502
[epoch 13, batch   899] avg loss: 0.053243
[epoch 13, batch   999] avg loss: 0.056595
[epoch 13, batch  1099] avg loss: 0.055279
[epoch 13, batch  1199] avg loss: 0.052602
[epoch 13, batch  1299] avg loss: 0.060609
[epoch 13, batch  1399] avg loss: 0.057302
[epoch 13, batch  1499] avg loss: 0.062928
[epoch 13, batch  1599] avg loss: 0.060594
[epoch 13, batch  1699] avg loss: 0.051449
[epoch 13, batch  1799] avg loss: 0.055098
[epoch 13, batch  1899] avg loss: 0.051565
[epoch 13, batch  1999] avg loss: 0.055158
[epoch 13, batch  2099] avg loss: 0.057399
[epoch 13, batch  2199] avg loss: 0.061975
[epoch 13, batch  2299] avg loss: 0.065242
[epoch 13, batch  2399] avg loss: 0.052179
[epoch 13, batch  2499] avg loss: 0.049176
[epoch 14, batch    99] avg loss: 0.054834
[epoch 14, batch   199] avg loss: 0.054751
[epoch 14, batch   299] avg loss: 0.063922
[epoch 14, batch   399] avg loss: 0.059106
[epoch 14, batch   499] avg loss: 0.051452
[epoch 14, batch   599] avg loss: 0.056154
[epoch 14, batch   699] avg loss: 0.057919
[epoch 14, batch   799] avg loss: 0.048790
[epoch 14, batch   899] avg loss: 0.061638
[epoch 14, batch   999] avg loss: 0.052394
[epoch 14, batch  1099] avg loss: 0.056117
[epoch 14, batch  1199] avg loss: 0.056132
[epoch 14, batch  1299] avg loss: 0.054234
[epoch 14, batch  1399] avg loss: 0.058603
[epoch 14, batch  1499] avg loss: 0.063330
[epoch 14, batch  1599] avg loss: 0.060374
[epoch 14, batch  1699] avg loss: 0.063220
[epoch 14, batch  1799] avg loss: 0.061736
[epoch 14, batch  1899] avg loss: 0.052734
[epoch 14, batch  1999] avg loss: 0.060189
[epoch 14, batch  2099] avg loss: 0.045868
[epoch 14, batch  2199] avg loss: 0.054748
[epoch 14, batch  2299] avg loss: 0.053258
[epoch 14, batch  2399] avg loss: 0.049514
[epoch 14, batch  2499] avg loss: 0.049929
[epoch 15, batch    99] avg loss: 0.056166
[epoch 15, batch   199] avg loss: 0.050667
[epoch 15, batch   299] avg loss: 0.054358
[epoch 15, batch   399] avg loss: 0.062467
[epoch 15, batch   499] avg loss: 0.051691
[epoch 15, batch   599] avg loss: 0.051386
[epoch 15, batch   699] avg loss: 0.057761
[epoch 15, batch   799] avg loss: 0.045633
[epoch 15, batch   899] avg loss: 0.053852
[epoch 15, batch   999] avg loss: 0.058520
[epoch 15, batch  1099] avg loss: 0.060560
[epoch 15, batch  1199] avg loss: 0.049610
[epoch 15, batch  1299] avg loss: 0.064592
[epoch 15, batch  1399] avg loss: 0.056193
[epoch 15, batch  1499] avg loss: 0.052964
[epoch 15, batch  1599] avg loss: 0.059542
[epoch 15, batch  1699] avg loss: 0.048621
[epoch 15, batch  1799] avg loss: 0.053359
[epoch 15, batch  1899] avg loss: 0.059248
[epoch 15, batch  1999] avg loss: 0.052946
[epoch 15, batch  2099] avg loss: 0.050803
[epoch 15, batch  2199] avg loss: 0.058460
[epoch 15, batch  2299] avg loss: 0.064556
[epoch 15, batch  2399] avg loss: 0.051958
[epoch 15, batch  2499] avg loss: 0.066580
[epoch 16, batch    99] avg loss: 0.050594
[epoch 16, batch   199] avg loss: 0.050534
[epoch 16, batch   299] avg loss: 0.060075
[epoch 16, batch   399] avg loss: 0.049426
[epoch 16, batch   499] avg loss: 0.059729
[epoch 16, batch   599] avg loss: 0.058466
[epoch 16, batch   699] avg loss: 0.056428
[epoch 16, batch   799] avg loss: 0.052506
[epoch 16, batch   899] avg loss: 0.055884
[epoch 16, batch   999] avg loss: 0.052439
[epoch 16, batch  1099] avg loss: 0.047887
[epoch 16, batch  1199] avg loss: 0.046727
[epoch 16, batch  1299] avg loss: 0.056928
[epoch 16, batch  1399] avg loss: 0.053079
[epoch 16, batch  1499] avg loss: 0.055461
[epoch 16, batch  1599] avg loss: 0.055521
[epoch 16, batch  1699] avg loss: 0.044988
[epoch 16, batch  1799] avg loss: 0.066526
[epoch 16, batch  1899] avg loss: 0.057148
[epoch 16, batch  1999] avg loss: 0.054627
[epoch 16, batch  2099] avg loss: 0.046950
[epoch 16, batch  2199] avg loss: 0.058095
[epoch 16, batch  2299] avg loss: 0.062755
[epoch 16, batch  2399] avg loss: 0.050715
[epoch 16, batch  2499] avg loss: 0.054864
[epoch 17, batch    99] avg loss: 0.053680
[epoch 17, batch   199] avg loss: 0.053411
[epoch 17, batch   299] avg loss: 0.057572
[epoch 17, batch   399] avg loss: 0.050446
[epoch 17, batch   499] avg loss: 0.048845
[epoch 17, batch   599] avg loss: 0.050171
[epoch 17, batch   699] avg loss: 0.047032
[epoch 17, batch   799] avg loss: 0.049395
[epoch 17, batch   899] avg loss: 0.055439
[epoch 17, batch   999] avg loss: 0.061956
[epoch 17, batch  1099] avg loss: 0.056417
[epoch 17, batch  1199] avg loss: 0.048226
[epoch 17, batch  1299] avg loss: 0.058445
[epoch 17, batch  1399] avg loss: 0.061401
[epoch 17, batch  1499] avg loss: 0.051664
[epoch 17, batch  1599] avg loss: 0.056816
[epoch 17, batch  1699] avg loss: 0.061208
[epoch 17, batch  1799] avg loss: 0.064063
[epoch 17, batch  1899] avg loss: 0.065057
[epoch 17, batch  1999] avg loss: 0.057386
[epoch 17, batch  2099] avg loss: 0.064843
[epoch 17, batch  2199] avg loss: 0.063136
[epoch 17, batch  2299] avg loss: 0.055216
[epoch 17, batch  2399] avg loss: 0.053460
[epoch 17, batch  2499] avg loss: 0.059607
[epoch 18, batch    99] avg loss: 0.051934
[epoch 18, batch   199] avg loss: 0.055379
[epoch 18, batch   299] avg loss: 0.052345
[epoch 18, batch   399] avg loss: 0.054795
[epoch 18, batch   499] avg loss: 0.048778
[epoch 18, batch   599] avg loss: 0.054795
[epoch 18, batch   699] avg loss: 0.051978
[epoch 18, batch   799] avg loss: 0.055064
[epoch 18, batch   899] avg loss: 0.053224
[epoch 18, batch   999] avg loss: 0.058623
[epoch 18, batch  1099] avg loss: 0.054679
[epoch 18, batch  1199] avg loss: 0.051570
[epoch 18, batch  1299] avg loss: 0.056556
[epoch 18, batch  1399] avg loss: 0.056892
[epoch 18, batch  1499] avg loss: 0.059441
[epoch 18, batch  1599] avg loss: 0.074641
[epoch 18, batch  1699] avg loss: 0.053655
[epoch 18, batch  1799] avg loss: 0.055156
[epoch 18, batch  1899] avg loss: 0.054740
[epoch 18, batch  1999] avg loss: 0.050728
[epoch 18, batch  2099] avg loss: 0.052662
[epoch 18, batch  2199] avg loss: 0.052425
[epoch 18, batch  2299] avg loss: 0.055493
[epoch 18, batch  2399] avg loss: 0.062659
[epoch 18, batch  2499] avg loss: 0.048046
[epoch 19, batch    99] avg loss: 0.052934
[epoch 19, batch   199] avg loss: 0.056041
[epoch 19, batch   299] avg loss: 0.061726
[epoch 19, batch   399] avg loss: 0.050919
[epoch 19, batch   499] avg loss: 0.056111
[epoch 19, batch   599] avg loss: 0.054272
[epoch 19, batch   699] avg loss: 0.051715
[epoch 19, batch   799] avg loss: 0.051257
[epoch 19, batch   899] avg loss: 0.046097
[epoch 19, batch   999] avg loss: 0.050598
[epoch 19, batch  1099] avg loss: 0.065492
[epoch 19, batch  1199] avg loss: 0.051242
[epoch 19, batch  1299] avg loss: 0.057141
[epoch 19, batch  1399] avg loss: 0.055776
[epoch 19, batch  1499] avg loss: 0.056104
[epoch 19, batch  1599] avg loss: 0.047745
[epoch 19, batch  1699] avg loss: 0.053030
[epoch 19, batch  1799] avg loss: 0.050274
[epoch 19, batch  1899] avg loss: 0.052627
[epoch 19, batch  1999] avg loss: 0.048992
[epoch 19, batch  2099] avg loss: 0.053391
[epoch 19, batch  2199] avg loss: 0.048355
[epoch 19, batch  2299] avg loss: 0.050956
[epoch 19, batch  2399] avg loss: 0.054883
[epoch 19, batch  2499] avg loss: 0.053658
Model saved to model/20200502-062810.pth.
accuracy/TriangPrismIsosc : 0.0
n_examples/TriangPrismIsosc : 1500.0
accuracy/parallelepiped : 0.6913333333333334
n_examples/parallelepiped : 1500.0
accuracy/sphere : 0.0
n_examples/sphere : 306.0
accuracy/wire : 0.325
n_examples/wire : 600.0
accuracy/avg_geom : 0.3154121863799283
loss/validation_geom : 1.7772066449117978
accuracy/Au : 0.999231950844854
n_examples/Au : 1302.0
accuracy/SiN : 1.0
n_examples/SiN : 1302.0
accuracy/SiO2 : 1.0
n_examples/SiO2 : 1302.0
accuracy/avg_mat : 0.9997439836149513
loss/validation_mat : 0.0003994385968934434
MSE/ShortestDim : 0.28664903109702455
MAE/ShortestDim : 0.31546416184380915
MSE/MiddleDim : 0.6375206887294742
MAE/MiddleDim : 0.5303926452635742
MSE/LongDim : 14.601143370636658
MAE/LongDim : 1.861221118395718
MSE/log Area/Vol : 0.2896513644085136
MAE/log Area/Vol : 0.4096298007799062
loss/validation_dim : 15.814964454871669
loss/validation : 17.59257053838036
Metrics saved to model/20200502-062810_metrics.csv.
[epoch 0, batch    99] avg loss: 1.181876
[epoch 0, batch   199] avg loss: 0.910108
[epoch 0, batch   299] avg loss: 0.848726
[epoch 0, batch   399] avg loss: 0.812077
[epoch 0, batch   499] avg loss: 0.804529
[epoch 0, batch   599] avg loss: 0.785452
[epoch 0, batch   699] avg loss: 0.770017
[epoch 0, batch   799] avg loss: 0.744595
[epoch 0, batch   899] avg loss: 0.750419
[epoch 0, batch   999] avg loss: 0.736160
[epoch 0, batch  1099] avg loss: 0.720555
[epoch 0, batch  1199] avg loss: 0.719757
[epoch 0, batch  1299] avg loss: 0.729106
[epoch 0, batch  1399] avg loss: 0.709909
[epoch 0, batch  1499] avg loss: 0.723237
[epoch 0, batch  1599] avg loss: 0.696345
[epoch 0, batch  1699] avg loss: 0.689137
[epoch 0, batch  1799] avg loss: 0.692051
[epoch 0, batch  1899] avg loss: 0.695718
[epoch 0, batch  1999] avg loss: 0.678524
[epoch 0, batch  2099] avg loss: 0.684089
[epoch 0, batch  2199] avg loss: 0.678846
[epoch 0, batch  2299] avg loss: 0.684396
[epoch 0, batch  2399] avg loss: 0.678811
[epoch 0, batch  2499] avg loss: 0.666114
[epoch 1, batch    99] avg loss: 0.652522
[epoch 1, batch   199] avg loss: 0.670820
[epoch 1, batch   299] avg loss: 0.674614
[epoch 1, batch   399] avg loss: 0.653439
[epoch 1, batch   499] avg loss: 0.666143
[epoch 1, batch   599] avg loss: 0.661889
[epoch 1, batch   699] avg loss: 0.655864
[epoch 1, batch   799] avg loss: 0.666228
[epoch 1, batch   899] avg loss: 0.642756
[epoch 1, batch   999] avg loss: 0.654712
[epoch 1, batch  1099] avg loss: 0.662259
[epoch 1, batch  1199] avg loss: 0.646510
[epoch 1, batch  1299] avg loss: 0.677138
[epoch 1, batch  1399] avg loss: 0.657515
[epoch 1, batch  1499] avg loss: 0.645479
[epoch 1, batch  1599] avg loss: 0.652489
[epoch 1, batch  1699] avg loss: 0.651195
[epoch 1, batch  1799] avg loss: 0.650831
[epoch 1, batch  1899] avg loss: 0.634137
[epoch 1, batch  1999] avg loss: 0.651737
[epoch 1, batch  2099] avg loss: 0.654314
[epoch 1, batch  2199] avg loss: 0.636759
[epoch 1, batch  2299] avg loss: 0.647921
[epoch 1, batch  2399] avg loss: 0.627846
[epoch 1, batch  2499] avg loss: 0.629849
[epoch 2, batch    99] avg loss: 0.647936
[epoch 2, batch   199] avg loss: 0.610069
[epoch 2, batch   299] avg loss: 0.628612
[epoch 2, batch   399] avg loss: 0.636741
[epoch 2, batch   499] avg loss: 0.652883
[epoch 2, batch   599] avg loss: 0.640368
[epoch 2, batch   699] avg loss: 0.617082
[epoch 2, batch   799] avg loss: 0.625602
[epoch 2, batch   899] avg loss: 0.609988
[epoch 2, batch   999] avg loss: 0.629098
[epoch 2, batch  1099] avg loss: 0.626662
[epoch 2, batch  1199] avg loss: 0.630618
[epoch 2, batch  1299] avg loss: 0.645968
[epoch 2, batch  1399] avg loss: 0.628766
[epoch 2, batch  1499] avg loss: 0.629422
[epoch 2, batch  1599] avg loss: 0.611169
[epoch 2, batch  1699] avg loss: 0.604337
[epoch 2, batch  1799] avg loss: 0.625641
[epoch 2, batch  1899] avg loss: 0.626356
[epoch 2, batch  1999] avg loss: 0.616131
[epoch 2, batch  2099] avg loss: 0.617532
[epoch 2, batch  2199] avg loss: 0.629840
[epoch 2, batch  2299] avg loss: 0.621558
[epoch 2, batch  2399] avg loss: 0.612245
[epoch 2, batch  2499] avg loss: 0.606298
[epoch 3, batch    99] avg loss: 0.629273
[epoch 3, batch   199] avg loss: 0.628129
[epoch 3, batch   299] avg loss: 0.622185
[epoch 3, batch   399] avg loss: 0.596327
[epoch 3, batch   499] avg loss: 0.624927
[epoch 3, batch   599] avg loss: 0.618971
[epoch 3, batch   699] avg loss: 0.616941
[epoch 3, batch   799] avg loss: 0.609279
[epoch 3, batch   899] avg loss: 0.598011
[epoch 3, batch   999] avg loss: 0.595902
[epoch 3, batch  1099] avg loss: 0.618657
[epoch 3, batch  1199] avg loss: 0.605035
[epoch 3, batch  1299] avg loss: 0.599559
[epoch 3, batch  1399] avg loss: 0.608102
[epoch 3, batch  1499] avg loss: 0.610936
[epoch 3, batch  1599] avg loss: 0.612356
[epoch 3, batch  1699] avg loss: 0.604556
[epoch 3, batch  1799] avg loss: 0.604956
[epoch 3, batch  1899] avg loss: 0.619419
[epoch 3, batch  1999] avg loss: 0.608233
[epoch 3, batch  2099] avg loss: 0.594118
[epoch 3, batch  2199] avg loss: 0.588193
[epoch 3, batch  2299] avg loss: 0.583190
[epoch 3, batch  2399] avg loss: 0.604524
[epoch 3, batch  2499] avg loss: 0.629774
[epoch 4, batch    99] avg loss: 0.596396
[epoch 4, batch   199] avg loss: 0.606504
[epoch 4, batch   299] avg loss: 0.583257
[epoch 4, batch   399] avg loss: 0.589268
[epoch 4, batch   499] avg loss: 0.597508
[epoch 4, batch   599] avg loss: 0.598245
[epoch 4, batch   699] avg loss: 0.568838
[epoch 4, batch   799] avg loss: 0.579656
[epoch 4, batch   899] avg loss: 0.601419
[epoch 4, batch   999] avg loss: 0.598623
[epoch 4, batch  1099] avg loss: 0.607656
[epoch 4, batch  1199] avg loss: 0.590193
[epoch 4, batch  1299] avg loss: 0.610560
[epoch 4, batch  1399] avg loss: 0.602796
[epoch 4, batch  1499] avg loss: 0.600461
[epoch 4, batch  1599] avg loss: 0.596469
[epoch 4, batch  1699] avg loss: 0.596573
[epoch 4, batch  1799] avg loss: 0.607495
[epoch 4, batch  1899] avg loss: 0.595752
[epoch 4, batch  1999] avg loss: 0.572747
[epoch 4, batch  2099] avg loss: 0.608228
[epoch 4, batch  2199] avg loss: 0.584822
[epoch 4, batch  2299] avg loss: 0.600352
[epoch 4, batch  2399] avg loss: 0.611564
[epoch 4, batch  2499] avg loss: 0.626639
[epoch 5, batch    99] avg loss: 0.594968
[epoch 5, batch   199] avg loss: 0.604573
[epoch 5, batch   299] avg loss: 0.575976
[epoch 5, batch   399] avg loss: 0.598529
[epoch 5, batch   499] avg loss: 0.581871
[epoch 5, batch   599] avg loss: 0.618840
[epoch 5, batch   699] avg loss: 0.594503
[epoch 5, batch   799] avg loss: 0.597978
[epoch 5, batch   899] avg loss: 0.582380
[epoch 5, batch   999] avg loss: 0.588978
[epoch 5, batch  1099] avg loss: 0.571262
[epoch 5, batch  1199] avg loss: 0.582350
[epoch 5, batch  1299] avg loss: 0.575152
[epoch 5, batch  1399] avg loss: 0.572667
[epoch 5, batch  1499] avg loss: 0.577154
[epoch 5, batch  1599] avg loss: 0.592482
[epoch 5, batch  1699] avg loss: 0.573095
[epoch 5, batch  1799] avg loss: 0.583867
[epoch 5, batch  1899] avg loss: 0.584536
[epoch 5, batch  1999] avg loss: 0.585525
[epoch 5, batch  2099] avg loss: 0.598739
[epoch 5, batch  2199] avg loss: 0.595461
[epoch 5, batch  2299] avg loss: 0.586961
[epoch 5, batch  2399] avg loss: 0.578322
[epoch 5, batch  2499] avg loss: 0.583330
[epoch 6, batch    99] avg loss: 0.560163
[epoch 6, batch   199] avg loss: 0.588782
[epoch 6, batch   299] avg loss: 0.573437
[epoch 6, batch   399] avg loss: 0.575761
[epoch 6, batch   499] avg loss: 0.580267
[epoch 6, batch   599] avg loss: 0.580835
[epoch 6, batch   699] avg loss: 0.587375
[epoch 6, batch   799] avg loss: 0.583992
[epoch 6, batch   899] avg loss: 0.572820
[epoch 6, batch   999] avg loss: 0.582200
[epoch 6, batch  1099] avg loss: 0.593503
[epoch 6, batch  1199] avg loss: 0.572338
[epoch 6, batch  1299] avg loss: 0.572239
[epoch 6, batch  1399] avg loss: 0.573544
[epoch 6, batch  1499] avg loss: 0.593648
[epoch 6, batch  1599] avg loss: 0.601385
[epoch 6, batch  1699] avg loss: 0.592913
[epoch 6, batch  1799] avg loss: 0.572955
[epoch 6, batch  1899] avg loss: 0.588067
[epoch 6, batch  1999] avg loss: 0.569984
[epoch 6, batch  2099] avg loss: 0.580574
[epoch 6, batch  2199] avg loss: 0.574904
[epoch 6, batch  2299] avg loss: 0.581925
[epoch 6, batch  2399] avg loss: 0.571452
[epoch 6, batch  2499] avg loss: 0.563502
[epoch 7, batch    99] avg loss: 0.581853
[epoch 7, batch   199] avg loss: 0.576552
[epoch 7, batch   299] avg loss: 0.566708
[epoch 7, batch   399] avg loss: 0.568537
[epoch 7, batch   499] avg loss: 0.566957
[epoch 7, batch   599] avg loss: 0.563533
[epoch 7, batch   699] avg loss: 0.561176
[epoch 7, batch   799] avg loss: 0.575369
[epoch 7, batch   899] avg loss: 0.587997
[epoch 7, batch   999] avg loss: 0.578155
[epoch 7, batch  1099] avg loss: 0.573702
[epoch 7, batch  1199] avg loss: 0.560259
[epoch 7, batch  1299] avg loss: 0.575846
[epoch 7, batch  1399] avg loss: 0.579137
[epoch 7, batch  1499] avg loss: 0.566614
[epoch 7, batch  1599] avg loss: 0.571575
[epoch 7, batch  1699] avg loss: 0.568553
[epoch 7, batch  1799] avg loss: 0.556718
[epoch 7, batch  1899] avg loss: 0.575898
[epoch 7, batch  1999] avg loss: 0.567405
[epoch 7, batch  2099] avg loss: 0.558416
[epoch 7, batch  2199] avg loss: 0.567385
[epoch 7, batch  2299] avg loss: 0.558577
[epoch 7, batch  2399] avg loss: 0.576171
[epoch 7, batch  2499] avg loss: 0.557707
[epoch 8, batch    99] avg loss: 0.578600
[epoch 8, batch   199] avg loss: 0.576766
[epoch 8, batch   299] avg loss: 0.558921
[epoch 8, batch   399] avg loss: 0.565323
[epoch 8, batch   499] avg loss: 0.564170
[epoch 8, batch   599] avg loss: 0.554122
[epoch 8, batch   699] avg loss: 0.556056
[epoch 8, batch   799] avg loss: 0.555618
[epoch 8, batch   899] avg loss: 0.566648
[epoch 8, batch   999] avg loss: 0.570821
[epoch 8, batch  1099] avg loss: 0.559295
[epoch 8, batch  1199] avg loss: 0.562563
[epoch 8, batch  1299] avg loss: 0.567201
[epoch 8, batch  1399] avg loss: 0.559111
[epoch 8, batch  1499] avg loss: 0.573852
[epoch 8, batch  1599] avg loss: 0.557920
[epoch 8, batch  1699] avg loss: 0.580965
[epoch 8, batch  1799] avg loss: 0.553909
[epoch 8, batch  1899] avg loss: 0.552222
[epoch 8, batch  1999] avg loss: 0.568725
[epoch 8, batch  2099] avg loss: 0.553650
[epoch 8, batch  2199] avg loss: 0.568865
[epoch 8, batch  2299] avg loss: 0.561573
[epoch 8, batch  2399] avg loss: 0.556473
[epoch 8, batch  2499] avg loss: 0.564924
[epoch 9, batch    99] avg loss: 0.572772
[epoch 9, batch   199] avg loss: 0.536629
[epoch 9, batch   299] avg loss: 0.574036
[epoch 9, batch   399] avg loss: 0.547643
[epoch 9, batch   499] avg loss: 0.575709
[epoch 9, batch   599] avg loss: 0.552397
[epoch 9, batch   699] avg loss: 0.560292
[epoch 9, batch   799] avg loss: 0.543520
[epoch 9, batch   899] avg loss: 0.551294
[epoch 9, batch   999] avg loss: 0.569351
[epoch 9, batch  1099] avg loss: 0.563675
[epoch 9, batch  1199] avg loss: 0.546814
[epoch 9, batch  1299] avg loss: 0.570014
[epoch 9, batch  1399] avg loss: 0.560923
[epoch 9, batch  1499] avg loss: 0.565860
[epoch 9, batch  1599] avg loss: 0.561293
[epoch 9, batch  1699] avg loss: 0.563463
[epoch 9, batch  1799] avg loss: 0.557906
[epoch 9, batch  1899] avg loss: 0.556074
[epoch 9, batch  1999] avg loss: 0.567368
[epoch 9, batch  2099] avg loss: 0.542495
[epoch 9, batch  2199] avg loss: 0.555539
[epoch 9, batch  2299] avg loss: 0.551859
[epoch 9, batch  2399] avg loss: 0.549408
[epoch 9, batch  2499] avg loss: 0.558746
[epoch 10, batch    99] avg loss: 0.556235
[epoch 10, batch   199] avg loss: 0.548024
[epoch 10, batch   299] avg loss: 0.542013
[epoch 10, batch   399] avg loss: 0.554582
[epoch 10, batch   499] avg loss: 0.562223
[epoch 10, batch   599] avg loss: 0.543961
[epoch 10, batch   699] avg loss: 0.558848
[epoch 10, batch   799] avg loss: 0.557915
[epoch 10, batch   899] avg loss: 0.537670
[epoch 10, batch   999] avg loss: 0.562140
[epoch 10, batch  1099] avg loss: 0.560232
[epoch 10, batch  1199] avg loss: 0.548294
[epoch 10, batch  1299] avg loss: 0.552806
[epoch 10, batch  1399] avg loss: 0.555659
[epoch 10, batch  1499] avg loss: 0.553276
[epoch 10, batch  1599] avg loss: 0.553211
[epoch 10, batch  1699] avg loss: 0.553196
[epoch 10, batch  1799] avg loss: 0.548586
[epoch 10, batch  1899] avg loss: 0.541681
[epoch 10, batch  1999] avg loss: 0.555629
[epoch 10, batch  2099] avg loss: 0.543607
[epoch 10, batch  2199] avg loss: 0.553836
[epoch 10, batch  2299] avg loss: 0.530131
[epoch 10, batch  2399] avg loss: 0.547694
[epoch 10, batch  2499] avg loss: 0.540433
[epoch 11, batch    99] avg loss: 0.550447
[epoch 11, batch   199] avg loss: 0.545772
[epoch 11, batch   299] avg loss: 0.559689
[epoch 11, batch   399] avg loss: 0.546691
[epoch 11, batch   499] avg loss: 0.539979
[epoch 11, batch   599] avg loss: 0.537479
[epoch 11, batch   699] avg loss: 0.544612
[epoch 11, batch   799] avg loss: 0.552733
[epoch 11, batch   899] avg loss: 0.543290
[epoch 11, batch   999] avg loss: 0.524303
[epoch 11, batch  1099] avg loss: 0.541064
[epoch 11, batch  1199] avg loss: 0.553471
[epoch 11, batch  1299] avg loss: 0.535045
[epoch 11, batch  1399] avg loss: 0.550518
[epoch 11, batch  1499] avg loss: 0.544004
[epoch 11, batch  1599] avg loss: 0.543490
[epoch 11, batch  1699] avg loss: 0.544175
[epoch 11, batch  1799] avg loss: 0.545655
[epoch 11, batch  1899] avg loss: 0.561385
[epoch 11, batch  1999] avg loss: 0.539733
[epoch 11, batch  2099] avg loss: 0.544707
[epoch 11, batch  2199] avg loss: 0.543493
[epoch 11, batch  2299] avg loss: 0.530783
[epoch 11, batch  2399] avg loss: 0.538674
[epoch 11, batch  2499] avg loss: 0.555377
[epoch 12, batch    99] avg loss: 0.538913
[epoch 12, batch   199] avg loss: 0.536616
[epoch 12, batch   299] avg loss: 0.562928
[epoch 12, batch   399] avg loss: 0.528097
[epoch 12, batch   499] avg loss: 0.536079
[epoch 12, batch   599] avg loss: 0.552941
[epoch 12, batch   699] avg loss: 0.552312
[epoch 12, batch   799] avg loss: 0.547951
[epoch 12, batch   899] avg loss: 0.530329
[epoch 12, batch   999] avg loss: 0.544231
[epoch 12, batch  1099] avg loss: 0.537654
[epoch 12, batch  1199] avg loss: 0.532215
[epoch 12, batch  1299] avg loss: 0.536730
[epoch 12, batch  1399] avg loss: 0.548295
[epoch 12, batch  1499] avg loss: 0.539263
[epoch 12, batch  1599] avg loss: 0.551996
[epoch 12, batch  1699] avg loss: 0.539301
[epoch 12, batch  1799] avg loss: 0.532405
[epoch 12, batch  1899] avg loss: 0.530553
[epoch 12, batch  1999] avg loss: 0.518415
[epoch 12, batch  2099] avg loss: 0.525453
[epoch 12, batch  2199] avg loss: 0.534351
[epoch 12, batch  2299] avg loss: 0.568985
[epoch 12, batch  2399] avg loss: 0.537349
[epoch 12, batch  2499] avg loss: 0.533031
[epoch 13, batch    99] avg loss: 0.539013
[epoch 13, batch   199] avg loss: 0.534152
[epoch 13, batch   299] avg loss: 0.532729
[epoch 13, batch   399] avg loss: 0.527654
[epoch 13, batch   499] avg loss: 0.542997
[epoch 13, batch   599] avg loss: 0.533348
[epoch 13, batch   699] avg loss: 0.525339
[epoch 13, batch   799] avg loss: 0.519781
[epoch 13, batch   899] avg loss: 0.549933
[epoch 13, batch   999] avg loss: 0.535806
[epoch 13, batch  1099] avg loss: 0.525029
[epoch 13, batch  1199] avg loss: 0.550230
[epoch 13, batch  1299] avg loss: 0.557340
[epoch 13, batch  1399] avg loss: 0.527669
[epoch 13, batch  1499] avg loss: 0.517829
[epoch 13, batch  1599] avg loss: 0.536879
[epoch 13, batch  1699] avg loss: 0.526074
[epoch 13, batch  1799] avg loss: 0.547241
[epoch 13, batch  1899] avg loss: 0.543047
[epoch 13, batch  1999] avg loss: 0.539644
[epoch 13, batch  2099] avg loss: 0.528126
[epoch 13, batch  2199] avg loss: 0.552470
[epoch 13, batch  2299] avg loss: 0.542898
[epoch 13, batch  2399] avg loss: 0.521238
[epoch 13, batch  2499] avg loss: 0.535337
[epoch 14, batch    99] avg loss: 0.531572
[epoch 14, batch   199] avg loss: 0.541065
[epoch 14, batch   299] avg loss: 0.528679
[epoch 14, batch   399] avg loss: 0.524169
[epoch 14, batch   499] avg loss: 0.526468
[epoch 14, batch   599] avg loss: 0.539494
[epoch 14, batch   699] avg loss: 0.527755
[epoch 14, batch   799] avg loss: 0.523574
[epoch 14, batch   899] avg loss: 0.520821
[epoch 14, batch   999] avg loss: 0.531563
[epoch 14, batch  1099] avg loss: 0.539939
[epoch 14, batch  1199] avg loss: 0.535601
[epoch 14, batch  1299] avg loss: 0.529152
[epoch 14, batch  1399] avg loss: 0.527921
[epoch 14, batch  1499] avg loss: 0.522299
[epoch 14, batch  1599] avg loss: 0.529470
[epoch 14, batch  1699] avg loss: 0.546424
[epoch 14, batch  1799] avg loss: 0.535205
[epoch 14, batch  1899] avg loss: 0.515758
[epoch 14, batch  1999] avg loss: 0.515414
[epoch 14, batch  2099] avg loss: 0.552678
[epoch 14, batch  2199] avg loss: 0.521350
[epoch 14, batch  2299] avg loss: 0.512644
[epoch 14, batch  2399] avg loss: 0.536356
[epoch 14, batch  2499] avg loss: 0.516664
[epoch 15, batch    99] avg loss: 0.546515
[epoch 15, batch   199] avg loss: 0.509695
[epoch 15, batch   299] avg loss: 0.518846
[epoch 15, batch   399] avg loss: 0.530491
[epoch 15, batch   499] avg loss: 0.531093
[epoch 15, batch   599] avg loss: 0.520453
[epoch 15, batch   699] avg loss: 0.527664
[epoch 15, batch   799] avg loss: 0.527967
[epoch 15, batch   899] avg loss: 0.518725
[epoch 15, batch   999] avg loss: 0.507831
[epoch 15, batch  1099] avg loss: 0.529739
[epoch 15, batch  1199] avg loss: 0.519163
[epoch 15, batch  1299] avg loss: 0.511578
[epoch 15, batch  1399] avg loss: 0.521871
[epoch 15, batch  1499] avg loss: 0.523524
[epoch 15, batch  1599] avg loss: 0.527175
[epoch 15, batch  1699] avg loss: 0.554161
[epoch 15, batch  1799] avg loss: 0.529608
[epoch 15, batch  1899] avg loss: 0.510466
[epoch 15, batch  1999] avg loss: 0.528952
[epoch 15, batch  2099] avg loss: 0.512488
[epoch 15, batch  2199] avg loss: 0.531362
[epoch 15, batch  2299] avg loss: 0.523571
[epoch 15, batch  2399] avg loss: 0.516727
[epoch 15, batch  2499] avg loss: 0.539099
[epoch 16, batch    99] avg loss: 0.550854
[epoch 16, batch   199] avg loss: 0.516531
[epoch 16, batch   299] avg loss: 0.550377
[epoch 16, batch   399] avg loss: 0.541188
[epoch 16, batch   499] avg loss: 0.532590
[epoch 16, batch   599] avg loss: 0.518012
[epoch 16, batch   699] avg loss: 0.537795
[epoch 16, batch   799] avg loss: 0.508096
[epoch 16, batch   899] avg loss: 0.515450
[epoch 16, batch   999] avg loss: 0.526492
[epoch 16, batch  1099] avg loss: 0.509694
[epoch 16, batch  1199] avg loss: 0.507296
[epoch 16, batch  1299] avg loss: 0.514661
[epoch 16, batch  1399] avg loss: 0.507642
[epoch 16, batch  1499] avg loss: 0.498188
[epoch 16, batch  1599] avg loss: 0.516297
[epoch 16, batch  1699] avg loss: 0.494574
[epoch 16, batch  1799] avg loss: 0.507501
[epoch 16, batch  1899] avg loss: 0.520092
[epoch 16, batch  1999] avg loss: 0.546155
[epoch 16, batch  2099] avg loss: 0.533224
[epoch 16, batch  2199] avg loss: 0.530734
[epoch 16, batch  2299] avg loss: 0.518074
[epoch 16, batch  2399] avg loss: 0.533176
[epoch 16, batch  2499] avg loss: 0.542727
[epoch 17, batch    99] avg loss: 0.516938
[epoch 17, batch   199] avg loss: 0.517050
[epoch 17, batch   299] avg loss: 0.520243
[epoch 17, batch   399] avg loss: 0.521600
[epoch 17, batch   499] avg loss: 0.503817
[epoch 17, batch   599] avg loss: 0.518565
[epoch 17, batch   699] avg loss: 0.517825
[epoch 17, batch   799] avg loss: 0.507811
[epoch 17, batch   899] avg loss: 0.508755
[epoch 17, batch   999] avg loss: 0.514913
[epoch 17, batch  1099] avg loss: 0.509755
[epoch 17, batch  1199] avg loss: 0.523399
[epoch 17, batch  1299] avg loss: 0.509153
[epoch 17, batch  1399] avg loss: 0.521534
[epoch 17, batch  1499] avg loss: 0.513747
[epoch 17, batch  1599] avg loss: 0.513902
[epoch 17, batch  1699] avg loss: 0.593316
[epoch 17, batch  1799] avg loss: 0.536615
[epoch 17, batch  1899] avg loss: 0.504968
[epoch 17, batch  1999] avg loss: 0.519738
[epoch 17, batch  2099] avg loss: 0.526667
[epoch 17, batch  2199] avg loss: 0.524885
[epoch 17, batch  2299] avg loss: 0.507292
[epoch 17, batch  2399] avg loss: 0.502327
[epoch 17, batch  2499] avg loss: 0.538504
[epoch 18, batch    99] avg loss: 0.501997
[epoch 18, batch   199] avg loss: 0.524639
[epoch 18, batch   299] avg loss: 0.518576
[epoch 18, batch   399] avg loss: 0.510677
[epoch 18, batch   499] avg loss: 0.512810
[epoch 18, batch   599] avg loss: 0.507830
[epoch 18, batch   699] avg loss: 0.521724
[epoch 18, batch   799] avg loss: 0.520705
[epoch 18, batch   899] avg loss: 0.519190
[epoch 18, batch   999] avg loss: 0.501040
[epoch 18, batch  1099] avg loss: 0.500414
[epoch 18, batch  1199] avg loss: 0.533237
[epoch 18, batch  1299] avg loss: 0.494756
[epoch 18, batch  1399] avg loss: 0.516428
[epoch 18, batch  1499] avg loss: 0.515669
[epoch 18, batch  1599] avg loss: 0.509444
[epoch 18, batch  1699] avg loss: 0.502447
[epoch 18, batch  1799] avg loss: 0.510769
[epoch 18, batch  1899] avg loss: 0.506499
[epoch 18, batch  1999] avg loss: 0.509558
[epoch 18, batch  2099] avg loss: 0.509199
[epoch 18, batch  2199] avg loss: 0.516925
[epoch 18, batch  2299] avg loss: 0.521414
[epoch 18, batch  2399] avg loss: 0.512471
[epoch 18, batch  2499] avg loss: 0.503044
[epoch 19, batch    99] avg loss: 0.501821
[epoch 19, batch   199] avg loss: 0.515659
[epoch 19, batch   299] avg loss: 0.510969
[epoch 19, batch   399] avg loss: 0.517300
[epoch 19, batch   499] avg loss: 0.508556
[epoch 19, batch   599] avg loss: 0.516731
[epoch 19, batch   699] avg loss: 0.512104
[epoch 19, batch   799] avg loss: 0.509441
[epoch 19, batch   899] avg loss: 0.516560
[epoch 19, batch   999] avg loss: 0.498173
[epoch 19, batch  1099] avg loss: 0.496125
[epoch 19, batch  1199] avg loss: 0.521055
[epoch 19, batch  1299] avg loss: 0.503060
[epoch 19, batch  1399] avg loss: 0.513549
[epoch 19, batch  1499] avg loss: 0.511963
[epoch 19, batch  1599] avg loss: 0.510181
[epoch 19, batch  1699] avg loss: 0.518373
[epoch 19, batch  1799] avg loss: 0.500087
[epoch 19, batch  1899] avg loss: 0.513595
[epoch 19, batch  1999] avg loss: 0.502735
[epoch 19, batch  2099] avg loss: 0.505862
[epoch 19, batch  2199] avg loss: 0.507272
[epoch 19, batch  2299] avg loss: 0.497916
[epoch 19, batch  2399] avg loss: 0.503777
[epoch 19, batch  2499] avg loss: 0.509855
Model saved to model/20200502-064413.pth.
accuracy/TriangPrismIsosc : 0.7253333333333334
n_examples/TriangPrismIsosc : 1500.0
accuracy/parallelepiped : 0.324
n_examples/parallelepiped : 1500.0
accuracy/sphere : 0.9967320261437909
n_examples/sphere : 306.0
accuracy/wire : 0.8816666666666667
n_examples/wire : 600.0
accuracy/avg_geom : 0.6164874551971327
loss/validation_geom : 0.7910385594512033
accuracy/Au : 0.7864823348694316
n_examples/Au : 1302.0
accuracy/SiN : 0.46159754224270355
n_examples/SiN : 1302.0
accuracy/SiO2 : 0.8932411674347158
n_examples/SiO2 : 1302.0
accuracy/avg_mat : 0.713773681515617
loss/validation_mat : 2.695142502975171
MSE/ShortestDim : 10.932043967525347
MAE/ShortestDim : 1.365181169385368
MSE/MiddleDim : 13.402123066522773
MAE/MiddleDim : 2.1900575345928583
MSE/LongDim : 283.6514259510861
MAE/LongDim : 7.8806185917676075
MSE/log Area/Vol : 16.85638790745889
MAE/log Area/Vol : 2.084728312016266
loss/validation_dim : 324.8419808925931
loss/validation : 328.3281619550195
Metrics saved to model/20200502-064413_metrics.csv.
Parsed 2604 rows from data/sim_train_spectrum_Au.
Parsed 2604 rows from data/sim_train_labels_Au.
Parsed 9765 rows from data/gen_spectrum_Au_00-of-16.
Parsed 9765 rows from data/gen_labels_Au_00-of-16.
Parsed 9765 rows from data/gen_spectrum_Au_01-of-16.
Parsed 9765 rows from data/gen_labels_Au_01-of-16.
Parsed 9765 rows from data/gen_spectrum_Au_02-of-16.
Parsed 9765 rows from data/gen_labels_Au_02-of-16.
Parsed 9765 rows from data/gen_spectrum_Au_03-of-16.
Parsed 9765 rows from data/gen_labels_Au_03-of-16.
Parsed 9765 rows from data/gen_spectrum_Au_04-of-16.
Parsed 9765 rows from data/gen_labels_Au_04-of-16.
Parsed 9765 rows from data/gen_spectrum_Au_05-of-16.
Parsed 9765 rows from data/gen_labels_Au_05-of-16.
Parsed 9765 rows from data/gen_spectrum_Au_06-of-16.
Parsed 9765 rows from data/gen_labels_Au_06-of-16.
Parsed 9765 rows from data/gen_spectrum_Au_07-of-16.
Parsed 9765 rows from data/gen_labels_Au_07-of-16.
Parsed 9765 rows from data/gen_spectrum_Au_08-of-16.
Parsed 9765 rows from data/gen_labels_Au_08-of-16.
Parsed 9765 rows from data/gen_spectrum_Au_09-of-16.
Parsed 9765 rows from data/gen_labels_Au_09-of-16.
Parsed 9765 rows from data/gen_spectrum_Au_10-of-16.
Parsed 9765 rows from data/gen_labels_Au_10-of-16.
Parsed 9765 rows from data/gen_spectrum_Au_11-of-16.
Parsed 9765 rows from data/gen_labels_Au_11-of-16.
Parsed 9765 rows from data/gen_spectrum_Au_12-of-16.
Parsed 9765 rows from data/gen_labels_Au_12-of-16.
Parsed 9765 rows from data/gen_spectrum_Au_13-of-16.
Parsed 9765 rows from data/gen_labels_Au_13-of-16.
Parsed 9765 rows from data/gen_spectrum_Au_14-of-16.
Parsed 9765 rows from data/gen_labels_Au_14-of-16.
Parsed 9765 rows from data/gen_spectrum_Au_15-of-16.
Parsed 9765 rows from data/gen_labels_Au_15-of-16.
Parsed 1302 rows from data/sim_validation_spectrum_Au.
Parsed 1302 rows from data/sim_validation_labels_Au.
Logging training progress to tensorboard dir runs/alexnet-Au-lr_0.000500-trainsize_158844-05_02_2020_06:45-multistage.
[epoch 0, batch    99] avg loss: 0.348477
[epoch 0, batch   199] avg loss: 0.155187
[epoch 0, batch   299] avg loss: 0.105247
[epoch 0, batch   399] avg loss: 0.057968
[epoch 0, batch   499] avg loss: 0.043529
[epoch 0, batch   599] avg loss: 0.043026
[epoch 0, batch   699] avg loss: 0.035557
[epoch 0, batch   799] avg loss: 0.040042
[epoch 0, batch   899] avg loss: 0.032929
[epoch 0, batch   999] avg loss: 0.033271
[epoch 0, batch  1099] avg loss: 0.030030
[epoch 0, batch  1199] avg loss: 0.029897
[epoch 0, batch  1299] avg loss: 0.031303
[epoch 0, batch  1399] avg loss: 0.024075
[epoch 0, batch  1499] avg loss: 0.020363
[epoch 0, batch  1599] avg loss: 0.028659
[epoch 0, batch  1699] avg loss: 0.027613
[epoch 0, batch  1799] avg loss: 0.023024
[epoch 0, batch  1899] avg loss: 0.022613
[epoch 0, batch  1999] avg loss: 0.022584
[epoch 0, batch  2099] avg loss: 0.025596
[epoch 0, batch  2199] avg loss: 0.021348
[epoch 0, batch  2299] avg loss: 0.024572
[epoch 0, batch  2399] avg loss: 0.018423
[epoch 1, batch    99] avg loss: 0.024871
[epoch 1, batch   199] avg loss: 0.015929
[epoch 1, batch   299] avg loss: 0.020211
[epoch 1, batch   399] avg loss: 0.023500
[epoch 1, batch   499] avg loss: 0.016764
[epoch 1, batch   599] avg loss: 0.020908
[epoch 1, batch   699] avg loss: 0.019107
[epoch 1, batch   799] avg loss: 0.020402
[epoch 1, batch   899] avg loss: 0.019863
[epoch 1, batch   999] avg loss: 0.020790
[epoch 1, batch  1099] avg loss: 0.016787
[epoch 1, batch  1199] avg loss: 0.024935
[epoch 1, batch  1299] avg loss: 0.022483
[epoch 1, batch  1399] avg loss: 0.017480
[epoch 1, batch  1499] avg loss: 0.017293
[epoch 1, batch  1599] avg loss: 0.016270
[epoch 1, batch  1699] avg loss: 0.017514
[epoch 1, batch  1799] avg loss: 0.016605
[epoch 1, batch  1899] avg loss: 0.016855
[epoch 1, batch  1999] avg loss: 0.016258
[epoch 1, batch  2099] avg loss: 0.018062
[epoch 1, batch  2199] avg loss: 0.014826
[epoch 1, batch  2299] avg loss: 0.018229
[epoch 1, batch  2399] avg loss: 0.015006
[epoch 2, batch    99] avg loss: 0.015577
[epoch 2, batch   199] avg loss: 0.015490
[epoch 2, batch   299] avg loss: 0.014349
[epoch 2, batch   399] avg loss: 0.017058
[epoch 2, batch   499] avg loss: 0.015769
[epoch 2, batch   599] avg loss: 0.016822
[epoch 2, batch   699] avg loss: 0.016362
[epoch 2, batch   799] avg loss: 0.017358
[epoch 2, batch   899] avg loss: 0.016385
[epoch 2, batch   999] avg loss: 0.016133
[epoch 2, batch  1099] avg loss: 0.014149
[epoch 2, batch  1199] avg loss: 0.020368
[epoch 2, batch  1299] avg loss: 0.013318
[epoch 2, batch  1399] avg loss: 0.014239
[epoch 2, batch  1499] avg loss: 0.011828
[epoch 2, batch  1599] avg loss: 0.012572
[epoch 2, batch  1699] avg loss: 0.011762
[epoch 2, batch  1799] avg loss: 0.014657
[epoch 2, batch  1899] avg loss: 0.014246
[epoch 2, batch  1999] avg loss: 0.016029
[epoch 2, batch  2099] avg loss: 0.011672
[epoch 2, batch  2199] avg loss: 0.012779
[epoch 2, batch  2299] avg loss: 0.011204
[epoch 2, batch  2399] avg loss: 0.013204
[epoch 3, batch    99] avg loss: 0.015042
[epoch 3, batch   199] avg loss: 0.014169
[epoch 3, batch   299] avg loss: 0.012002
[epoch 3, batch   399] avg loss: 0.014348
[epoch 3, batch   499] avg loss: 0.012005
[epoch 3, batch   599] avg loss: 0.011038
[epoch 3, batch   699] avg loss: 0.013668
[epoch 3, batch   799] avg loss: 0.010723
[epoch 3, batch   899] avg loss: 0.010293
[epoch 3, batch   999] avg loss: 0.010400
[epoch 3, batch  1099] avg loss: 0.011470
[epoch 3, batch  1199] avg loss: 0.013314
[epoch 3, batch  1299] avg loss: 0.012333
[epoch 3, batch  1399] avg loss: 0.015257
[epoch 3, batch  1499] avg loss: 0.010599
[epoch 3, batch  1599] avg loss: 0.009493
[epoch 3, batch  1699] avg loss: 0.011985
[epoch 3, batch  1799] avg loss: 0.012588
[epoch 3, batch  1899] avg loss: 0.010602
[epoch 3, batch  1999] avg loss: 0.010057
[epoch 3, batch  2099] avg loss: 0.010661
[epoch 3, batch  2199] avg loss: 0.010654
[epoch 3, batch  2299] avg loss: 0.010930
[epoch 3, batch  2399] avg loss: 0.010567
[epoch 4, batch    99] avg loss: 0.013874
[epoch 4, batch   199] avg loss: 0.010948
[epoch 4, batch   299] avg loss: 0.012622
[epoch 4, batch   399] avg loss: 0.010297
[epoch 4, batch   499] avg loss: 0.011987
[epoch 4, batch   599] avg loss: 0.012456
[epoch 4, batch   699] avg loss: 0.010606
[epoch 4, batch   799] avg loss: 0.009995
[epoch 4, batch   899] avg loss: 0.008265
[epoch 4, batch   999] avg loss: 0.011651
[epoch 4, batch  1099] avg loss: 0.010011
[epoch 4, batch  1199] avg loss: 0.011158
[epoch 4, batch  1299] avg loss: 0.010421
[epoch 4, batch  1399] avg loss: 0.011128
[epoch 4, batch  1499] avg loss: 0.011515
[epoch 4, batch  1599] avg loss: 0.012136
[epoch 4, batch  1699] avg loss: 0.011593
[epoch 4, batch  1799] avg loss: 0.008895
[epoch 4, batch  1899] avg loss: 0.011916
[epoch 4, batch  1999] avg loss: 0.009030
[epoch 4, batch  2099] avg loss: 0.009154
[epoch 4, batch  2199] avg loss: 0.009264
[epoch 4, batch  2299] avg loss: 0.008210
[epoch 4, batch  2399] avg loss: 0.009031
[epoch 5, batch    99] avg loss: 0.008108
[epoch 5, batch   199] avg loss: 0.008147
[epoch 5, batch   299] avg loss: 0.007626
[epoch 5, batch   399] avg loss: 0.008477
[epoch 5, batch   499] avg loss: 0.008441
[epoch 5, batch   599] avg loss: 0.009669
[epoch 5, batch   699] avg loss: 0.008967
[epoch 5, batch   799] avg loss: 0.007668
[epoch 5, batch   899] avg loss: 0.009401
[epoch 5, batch   999] avg loss: 0.010341
[epoch 5, batch  1099] avg loss: 0.007881
[epoch 5, batch  1199] avg loss: 0.011848
[epoch 5, batch  1299] avg loss: 0.011119
[epoch 5, batch  1399] avg loss: 0.008778
[epoch 5, batch  1499] avg loss: 0.010822
[epoch 5, batch  1599] avg loss: 0.009294
[epoch 5, batch  1699] avg loss: 0.010750
[epoch 5, batch  1799] avg loss: 0.010951
[epoch 5, batch  1899] avg loss: 0.008948
[epoch 5, batch  1999] avg loss: 0.011196
[epoch 5, batch  2099] avg loss: 0.008048
[epoch 5, batch  2199] avg loss: 0.008855
[epoch 5, batch  2299] avg loss: 0.010259
[epoch 5, batch  2399] avg loss: 0.008414
[epoch 6, batch    99] avg loss: 0.008508
[epoch 6, batch   199] avg loss: 0.008814
[epoch 6, batch   299] avg loss: 0.008614
[epoch 6, batch   399] avg loss: 0.007208
[epoch 6, batch   499] avg loss: 0.009332
[epoch 6, batch   599] avg loss: 0.006686
[epoch 6, batch   699] avg loss: 0.007415
[epoch 6, batch   799] avg loss: 0.008560
[epoch 6, batch   899] avg loss: 0.006292
[epoch 6, batch   999] avg loss: 0.008274
[epoch 6, batch  1099] avg loss: 0.010201
[epoch 6, batch  1199] avg loss: 0.009660
[epoch 6, batch  1299] avg loss: 0.011715
[epoch 6, batch  1399] avg loss: 0.009848
[epoch 6, batch  1499] avg loss: 0.009645
[epoch 6, batch  1599] avg loss: 0.008917
[epoch 6, batch  1699] avg loss: 0.007187
[epoch 6, batch  1799] avg loss: 0.007009
[epoch 6, batch  1899] avg loss: 0.007220
[epoch 6, batch  1999] avg loss: 0.006913
[epoch 6, batch  2099] avg loss: 0.007940
[epoch 6, batch  2199] avg loss: 0.006842
[epoch 6, batch  2299] avg loss: 0.008565
[epoch 6, batch  2399] avg loss: 0.007993
[epoch 7, batch    99] avg loss: 0.007929
[epoch 7, batch   199] avg loss: 0.008022
[epoch 7, batch   299] avg loss: 0.007740
[epoch 7, batch   399] avg loss: 0.006376
[epoch 7, batch   499] avg loss: 0.007366
[epoch 7, batch   599] avg loss: 0.007822
[epoch 7, batch   699] avg loss: 0.007068
[epoch 7, batch   799] avg loss: 0.008205
[epoch 7, batch   899] avg loss: 0.006018
[epoch 7, batch   999] avg loss: 0.011232
[epoch 7, batch  1099] avg loss: 0.008362
[epoch 7, batch  1199] avg loss: 0.009397
[epoch 7, batch  1299] avg loss: 0.007458
[epoch 7, batch  1399] avg loss: 0.010348
[epoch 7, batch  1499] avg loss: 0.008845
[epoch 7, batch  1599] avg loss: 0.007381
[epoch 7, batch  1699] avg loss: 0.009070
[epoch 7, batch  1799] avg loss: 0.008737
[epoch 7, batch  1899] avg loss: 0.009072
[epoch 7, batch  1999] avg loss: 0.007273
[epoch 7, batch  2099] avg loss: 0.006444
[epoch 7, batch  2199] avg loss: 0.006642
[epoch 7, batch  2299] avg loss: 0.007025
[epoch 7, batch  2399] avg loss: 0.006217
[epoch 8, batch    99] avg loss: 0.008834
[epoch 8, batch   199] avg loss: 0.008886
[epoch 8, batch   299] avg loss: 0.011697
[epoch 8, batch   399] avg loss: 0.006541
[epoch 8, batch   499] avg loss: 0.007019
[epoch 8, batch   599] avg loss: 0.007147
[epoch 8, batch   699] avg loss: 0.006823
[epoch 8, batch   799] avg loss: 0.007218
[epoch 8, batch   899] avg loss: 0.005955
[epoch 8, batch   999] avg loss: 0.007701
[epoch 8, batch  1099] avg loss: 0.007402
[epoch 8, batch  1199] avg loss: 0.008581
[epoch 8, batch  1299] avg loss: 0.006450
[epoch 8, batch  1399] avg loss: 0.007383
[epoch 8, batch  1499] avg loss: 0.011021
[epoch 8, batch  1599] avg loss: 0.009791
[epoch 8, batch  1699] avg loss: 0.005769
[epoch 8, batch  1799] avg loss: 0.007219
[epoch 8, batch  1899] avg loss: 0.007678
[epoch 8, batch  1999] avg loss: 0.005077
[epoch 8, batch  2099] avg loss: 0.006954
[epoch 8, batch  2199] avg loss: 0.006820
[epoch 8, batch  2299] avg loss: 0.007814
[epoch 8, batch  2399] avg loss: 0.007460
[epoch 9, batch    99] avg loss: 0.008407
[epoch 9, batch   199] avg loss: 0.008422
[epoch 9, batch   299] avg loss: 0.005748
[epoch 9, batch   399] avg loss: 0.007687
[epoch 9, batch   499] avg loss: 0.006688
[epoch 9, batch   599] avg loss: 0.007773
[epoch 9, batch   699] avg loss: 0.006798
[epoch 9, batch   799] avg loss: 0.008958
[epoch 9, batch   899] avg loss: 0.006842
[epoch 9, batch   999] avg loss: 0.006410
[epoch 9, batch  1099] avg loss: 0.006403
[epoch 9, batch  1199] avg loss: 0.006176
[epoch 9, batch  1299] avg loss: 0.006098
[epoch 9, batch  1399] avg loss: 0.008264
[epoch 9, batch  1499] avg loss: 0.007739
[epoch 9, batch  1599] avg loss: 0.005589
[epoch 9, batch  1699] avg loss: 0.006544
[epoch 9, batch  1799] avg loss: 0.006689
[epoch 9, batch  1899] avg loss: 0.005869
[epoch 9, batch  1999] avg loss: 0.005611
[epoch 9, batch  2099] avg loss: 0.008373
[epoch 9, batch  2199] avg loss: 0.006668
[epoch 9, batch  2299] avg loss: 0.007053
[epoch 9, batch  2399] avg loss: 0.006190
[epoch 10, batch    99] avg loss: 0.006102
[epoch 10, batch   199] avg loss: 0.005286
[epoch 10, batch   299] avg loss: 0.006838
[epoch 10, batch   399] avg loss: 0.006555
[epoch 10, batch   499] avg loss: 0.006747
[epoch 10, batch   599] avg loss: 0.005381
[epoch 10, batch   699] avg loss: 0.006303
[epoch 10, batch   799] avg loss: 0.006332
[epoch 10, batch   899] avg loss: 0.007470
[epoch 10, batch   999] avg loss: 0.006227
[epoch 10, batch  1099] avg loss: 0.008288
[epoch 10, batch  1199] avg loss: 0.006772
[epoch 10, batch  1299] avg loss: 0.007602
[epoch 10, batch  1399] avg loss: 0.006448
[epoch 10, batch  1499] avg loss: 0.008529
[epoch 10, batch  1599] avg loss: 0.006860
[epoch 10, batch  1699] avg loss: 0.008306
[epoch 10, batch  1799] avg loss: 0.008105
[epoch 10, batch  1899] avg loss: 0.007729
[epoch 10, batch  1999] avg loss: 0.005757
[epoch 10, batch  2099] avg loss: 0.006044
[epoch 10, batch  2199] avg loss: 0.008961
[epoch 10, batch  2299] avg loss: 0.006422
[epoch 10, batch  2399] avg loss: 0.006016
[epoch 11, batch    99] avg loss: 0.006477
[epoch 11, batch   199] avg loss: 0.006049
[epoch 11, batch   299] avg loss: 0.006714
[epoch 11, batch   399] avg loss: 0.006426
[epoch 11, batch   499] avg loss: 0.004792
[epoch 11, batch   599] avg loss: 0.007293
[epoch 11, batch   699] avg loss: 0.006739
[epoch 11, batch   799] avg loss: 0.005391
[epoch 11, batch   899] avg loss: 0.009776
[epoch 11, batch   999] avg loss: 0.006647
[epoch 11, batch  1099] avg loss: 0.006224
[epoch 11, batch  1199] avg loss: 0.005396
[epoch 11, batch  1299] avg loss: 0.005819
[epoch 11, batch  1399] avg loss: 0.005692
[epoch 11, batch  1499] avg loss: 0.005516
[epoch 11, batch  1599] avg loss: 0.005642
[epoch 11, batch  1699] avg loss: 0.007700
[epoch 11, batch  1799] avg loss: 0.005800
[epoch 11, batch  1899] avg loss: 0.006703
[epoch 11, batch  1999] avg loss: 0.006911
[epoch 11, batch  2099] avg loss: 0.007594
[epoch 11, batch  2199] avg loss: 0.007880
[epoch 11, batch  2299] avg loss: 0.005026
[epoch 11, batch  2399] avg loss: 0.007817
[epoch 12, batch    99] avg loss: 0.005973
[epoch 12, batch   199] avg loss: 0.004263
[epoch 12, batch   299] avg loss: 0.006727
[epoch 12, batch   399] avg loss: 0.006483
[epoch 12, batch   499] avg loss: 0.007704
[epoch 12, batch   599] avg loss: 0.005664
[epoch 12, batch   699] avg loss: 0.007986
[epoch 12, batch   799] avg loss: 0.004943
[epoch 12, batch   899] avg loss: 0.005133
[epoch 12, batch   999] avg loss: 0.006972
[epoch 12, batch  1099] avg loss: 0.007101
[epoch 12, batch  1199] avg loss: 0.005897
[epoch 12, batch  1299] avg loss: 0.006979
[epoch 12, batch  1399] avg loss: 0.006371
[epoch 12, batch  1499] avg loss: 0.005682
[epoch 12, batch  1599] avg loss: 0.005757
[epoch 12, batch  1699] avg loss: 0.007980
[epoch 12, batch  1799] avg loss: 0.005809
[epoch 12, batch  1899] avg loss: 0.005771
[epoch 12, batch  1999] avg loss: 0.006619
[epoch 12, batch  2099] avg loss: 0.007125
[epoch 12, batch  2199] avg loss: 0.006195
[epoch 12, batch  2299] avg loss: 0.006322
[epoch 12, batch  2399] avg loss: 0.005047
[epoch 13, batch    99] avg loss: 0.005522
[epoch 13, batch   199] avg loss: 0.006539
[epoch 13, batch   299] avg loss: 0.006046
[epoch 13, batch   399] avg loss: 0.005581
[epoch 13, batch   499] avg loss: 0.006772
[epoch 13, batch   599] avg loss: 0.006649
[epoch 13, batch   699] avg loss: 0.006699
[epoch 13, batch   799] avg loss: 0.006170
[epoch 13, batch   899] avg loss: 0.006455
[epoch 13, batch   999] avg loss: 0.005357
[epoch 13, batch  1099] avg loss: 0.005827
[epoch 13, batch  1199] avg loss: 0.005124
[epoch 13, batch  1299] avg loss: 0.005381
[epoch 13, batch  1399] avg loss: 0.005951
[epoch 13, batch  1499] avg loss: 0.007075
[epoch 13, batch  1599] avg loss: 0.006262
[epoch 13, batch  1699] avg loss: 0.005533
[epoch 13, batch  1799] avg loss: 0.005368
[epoch 13, batch  1899] avg loss: 0.005914
[epoch 13, batch  1999] avg loss: 0.005694
[epoch 13, batch  2099] avg loss: 0.006328
[epoch 13, batch  2199] avg loss: 0.006067
[epoch 13, batch  2299] avg loss: 0.005532
[epoch 13, batch  2399] avg loss: 0.006881
[epoch 14, batch    99] avg loss: 0.005839
[epoch 14, batch   199] avg loss: 0.006334
[epoch 14, batch   299] avg loss: 0.004202
[epoch 14, batch   399] avg loss: 0.006538
[epoch 14, batch   499] avg loss: 0.006779
[epoch 14, batch   599] avg loss: 0.005249
[epoch 14, batch   699] avg loss: 0.005723
[epoch 14, batch   799] avg loss: 0.006985
[epoch 14, batch   899] avg loss: 0.007464
[epoch 14, batch   999] avg loss: 0.005906
[epoch 14, batch  1099] avg loss: 0.004939
[epoch 14, batch  1199] avg loss: 0.006031
[epoch 14, batch  1299] avg loss: 0.006069
[epoch 14, batch  1399] avg loss: 0.005997
[epoch 14, batch  1499] avg loss: 0.005846
[epoch 14, batch  1599] avg loss: 0.007277
[epoch 14, batch  1699] avg loss: 0.005888
[epoch 14, batch  1799] avg loss: 0.005114
[epoch 14, batch  1899] avg loss: 0.005775
[epoch 14, batch  1999] avg loss: 0.005414
[epoch 14, batch  2099] avg loss: 0.005499
[epoch 14, batch  2199] avg loss: 0.005690
[epoch 14, batch  2299] avg loss: 0.006033
[epoch 14, batch  2399] avg loss: 0.006958
[epoch 15, batch    99] avg loss: 0.006062
[epoch 15, batch   199] avg loss: 0.006777
[epoch 15, batch   299] avg loss: 0.007006
[epoch 15, batch   399] avg loss: 0.005974
[epoch 15, batch   499] avg loss: 0.005258
[epoch 15, batch   599] avg loss: 0.006194
[epoch 15, batch   699] avg loss: 0.004988
[epoch 15, batch   799] avg loss: 0.006023
[epoch 15, batch   899] avg loss: 0.005252
[epoch 15, batch   999] avg loss: 0.006063
[epoch 15, batch  1099] avg loss: 0.006135
[epoch 15, batch  1199] avg loss: 0.004871
[epoch 15, batch  1299] avg loss: 0.006220
[epoch 15, batch  1399] avg loss: 0.005805
[epoch 15, batch  1499] avg loss: 0.007199
[epoch 15, batch  1599] avg loss: 0.007418
[epoch 15, batch  1699] avg loss: 0.005339
[epoch 15, batch  1799] avg loss: 0.005204
[epoch 15, batch  1899] avg loss: 0.005541
[epoch 15, batch  1999] avg loss: 0.006238
[epoch 15, batch  2099] avg loss: 0.004671
[epoch 15, batch  2199] avg loss: 0.004606
[epoch 15, batch  2299] avg loss: 0.005538
[epoch 15, batch  2399] avg loss: 0.007236
[epoch 16, batch    99] avg loss: 0.006074
[epoch 16, batch   199] avg loss: 0.006092
[epoch 16, batch   299] avg loss: 0.006269
[epoch 16, batch   399] avg loss: 0.005992
[epoch 16, batch   499] avg loss: 0.005860
[epoch 16, batch   599] avg loss: 0.005489
[epoch 16, batch   699] avg loss: 0.005461
[epoch 16, batch   799] avg loss: 0.004885
[epoch 16, batch   899] avg loss: 0.004852
[epoch 16, batch   999] avg loss: 0.006279
[epoch 16, batch  1099] avg loss: 0.005070
[epoch 16, batch  1199] avg loss: 0.006545
[epoch 16, batch  1299] avg loss: 0.005378
[epoch 16, batch  1399] avg loss: 0.005714
[epoch 16, batch  1499] avg loss: 0.005241
[epoch 16, batch  1599] avg loss: 0.004811
[epoch 16, batch  1699] avg loss: 0.005317
[epoch 16, batch  1799] avg loss: 0.005619
[epoch 16, batch  1899] avg loss: 0.006377
[epoch 16, batch  1999] avg loss: 0.005411
[epoch 16, batch  2099] avg loss: 0.004832
[epoch 16, batch  2199] avg loss: 0.007190
[epoch 16, batch  2299] avg loss: 0.004693
[epoch 16, batch  2399] avg loss: 0.006317
[epoch 17, batch    99] avg loss: 0.004871
[epoch 17, batch   199] avg loss: 0.004736
[epoch 17, batch   299] avg loss: 0.005952
[epoch 17, batch   399] avg loss: 0.005709
[epoch 17, batch   499] avg loss: 0.007164
[epoch 17, batch   599] avg loss: 0.005924
[epoch 17, batch   699] avg loss: 0.004597
[epoch 17, batch   799] avg loss: 0.005359
[epoch 17, batch   899] avg loss: 0.004160
[epoch 17, batch   999] avg loss: 0.006229
[epoch 17, batch  1099] avg loss: 0.004870
[epoch 17, batch  1199] avg loss: 0.006711
[epoch 17, batch  1299] avg loss: 0.005189
[epoch 17, batch  1399] avg loss: 0.004629
[epoch 17, batch  1499] avg loss: 0.006939
[epoch 17, batch  1599] avg loss: 0.005782
[epoch 17, batch  1699] avg loss: 0.005389
[epoch 17, batch  1799] avg loss: 0.006106
[epoch 17, batch  1899] avg loss: 0.004923
[epoch 17, batch  1999] avg loss: 0.005136
[epoch 17, batch  2099] avg loss: 0.005319
[epoch 17, batch  2199] avg loss: 0.004854
[epoch 17, batch  2299] avg loss: 0.005058
[epoch 17, batch  2399] avg loss: 0.006279
[epoch 18, batch    99] avg loss: 0.004686
[epoch 18, batch   199] avg loss: 0.005456
[epoch 18, batch   299] avg loss: 0.004349
[epoch 18, batch   399] avg loss: 0.005498
[epoch 18, batch   499] avg loss: 0.006186
[epoch 18, batch   599] avg loss: 0.004988
[epoch 18, batch   699] avg loss: 0.005531
[epoch 18, batch   799] avg loss: 0.007577
[epoch 18, batch   899] avg loss: 0.005442
[epoch 18, batch   999] avg loss: 0.005620
[epoch 18, batch  1099] avg loss: 0.005916
[epoch 18, batch  1199] avg loss: 0.004986
[epoch 18, batch  1299] avg loss: 0.003778
[epoch 18, batch  1399] avg loss: 0.006239
[epoch 18, batch  1499] avg loss: 0.005361
[epoch 18, batch  1599] avg loss: 0.005695
[epoch 18, batch  1699] avg loss: 0.004897
[epoch 18, batch  1799] avg loss: 0.006039
[epoch 18, batch  1899] avg loss: 0.004758
[epoch 18, batch  1999] avg loss: 0.004361
[epoch 18, batch  2099] avg loss: 0.004565
[epoch 18, batch  2199] avg loss: 0.004280
[epoch 18, batch  2299] avg loss: 0.005394
[epoch 18, batch  2399] avg loss: 0.007074
[epoch 19, batch    99] avg loss: 0.005046
[epoch 19, batch   199] avg loss: 0.004809
[epoch 19, batch   299] avg loss: 0.006413
[epoch 19, batch   399] avg loss: 0.005104
[epoch 19, batch   499] avg loss: 0.004760
[epoch 19, batch   599] avg loss: 0.004911
[epoch 19, batch   699] avg loss: 0.005345
[epoch 19, batch   799] avg loss: 0.005937
[epoch 19, batch   899] avg loss: 0.005122
[epoch 19, batch   999] avg loss: 0.004439
[epoch 19, batch  1099] avg loss: 0.005227
[epoch 19, batch  1199] avg loss: 0.006516
[epoch 19, batch  1299] avg loss: 0.005274
[epoch 19, batch  1399] avg loss: 0.006145
[epoch 19, batch  1499] avg loss: 0.006621
[epoch 19, batch  1599] avg loss: 0.003900
[epoch 19, batch  1699] avg loss: 0.005810
[epoch 19, batch  1799] avg loss: 0.005593
[epoch 19, batch  1899] avg loss: 0.005731
[epoch 19, batch  1999] avg loss: 0.005172
[epoch 19, batch  2099] avg loss: 0.005813
[epoch 19, batch  2199] avg loss: 0.005382
[epoch 19, batch  2299] avg loss: 0.003956
[epoch 19, batch  2399] avg loss: 0.005547
Model saved to model/20200502-070131.pth.
accuracy/TriangPrismIsosc : 0.014
n_examples/TriangPrismIsosc : 500.0
accuracy/parallelepiped : 0.236
n_examples/parallelepiped : 500.0
accuracy/sphere : 0.696078431372549
n_examples/sphere : 102.0
accuracy/wire : 0.13
n_examples/wire : 200.0
accuracy/avg_geom : 0.17050691244239632
loss/validation_geom : 1.461356069268902
accuracy/Au : 1.0
n_examples/Au : 1302.0
accuracy/SiN : 0.0
n_examples/SiN : 0.0
accuracy/SiO2 : 0.0
n_examples/SiO2 : 0.0
accuracy/avg_mat : 1.0
loss/validation_mat : 0.0
MSE/ShortestDim : 0.18945566886397916
MAE/ShortestDim : 0.23778304233345934
MSE/MiddleDim : 0.29570164621883455
MAE/MiddleDim : 0.34658514372947213
MSE/LongDim : 0.7048705411579935
MAE/LongDim : 0.5051751920521351
MSE/log Area/Vol : 0.1538391270762032
MAE/log Area/Vol : 0.29648084977438555
loss/validation_dim : 1.3438669833170105
loss/validation : 2.8052230525859123
Metrics saved to model/20200502-070131_metrics.csv.
[epoch 0, batch    99] avg loss: 1.137019
[epoch 0, batch   199] avg loss: 0.890915
[epoch 0, batch   299] avg loss: 0.832628
[epoch 0, batch   399] avg loss: 0.823998
[epoch 0, batch   499] avg loss: 0.803942
[epoch 0, batch   599] avg loss: 0.783924
[epoch 0, batch   699] avg loss: 0.760290
[epoch 0, batch   799] avg loss: 0.756664
[epoch 0, batch   899] avg loss: 0.738329
[epoch 0, batch   999] avg loss: 0.730934
[epoch 0, batch  1099] avg loss: 0.755762
[epoch 0, batch  1199] avg loss: 0.746161
[epoch 0, batch  1299] avg loss: 0.721746
[epoch 0, batch  1399] avg loss: 0.705334
[epoch 0, batch  1499] avg loss: 0.714366
[epoch 0, batch  1599] avg loss: 0.697427
[epoch 0, batch  1699] avg loss: 0.701505
[epoch 0, batch  1799] avg loss: 0.750258
[epoch 0, batch  1899] avg loss: 0.687468
[epoch 0, batch  1999] avg loss: 0.688676
[epoch 0, batch  2099] avg loss: 0.660898
[epoch 0, batch  2199] avg loss: 0.678923
[epoch 0, batch  2299] avg loss: 0.680605
[epoch 0, batch  2399] avg loss: 0.654396
[epoch 1, batch    99] avg loss: 0.664577
[epoch 1, batch   199] avg loss: 0.664951
[epoch 1, batch   299] avg loss: 0.651848
[epoch 1, batch   399] avg loss: 0.658083
[epoch 1, batch   499] avg loss: 0.651629
[epoch 1, batch   599] avg loss: 0.690394
[epoch 1, batch   699] avg loss: 0.650746
[epoch 1, batch   799] avg loss: 0.657219
[epoch 1, batch   899] avg loss: 0.683326
[epoch 1, batch   999] avg loss: 0.669559
[epoch 1, batch  1099] avg loss: 0.624625
[epoch 1, batch  1199] avg loss: 0.650763
[epoch 1, batch  1299] avg loss: 0.646385
[epoch 1, batch  1399] avg loss: 0.625244
[epoch 1, batch  1499] avg loss: 0.627567
[epoch 1, batch  1599] avg loss: 0.631425
[epoch 1, batch  1699] avg loss: 0.645057
[epoch 1, batch  1799] avg loss: 0.602685
[epoch 1, batch  1899] avg loss: 0.622728
[epoch 1, batch  1999] avg loss: 0.626207
[epoch 1, batch  2099] avg loss: 0.636534
[epoch 1, batch  2199] avg loss: 0.628175
[epoch 1, batch  2299] avg loss: 0.594075
[epoch 1, batch  2399] avg loss: 0.632808
[epoch 2, batch    99] avg loss: 0.625893
[epoch 2, batch   199] avg loss: 0.622214
[epoch 2, batch   299] avg loss: 0.602081
[epoch 2, batch   399] avg loss: 0.604999
[epoch 2, batch   499] avg loss: 0.655652
[epoch 2, batch   599] avg loss: 0.594331
[epoch 2, batch   699] avg loss: 0.590803
[epoch 2, batch   799] avg loss: 0.608691
[epoch 2, batch   899] avg loss: 0.613008
[epoch 2, batch   999] avg loss: 0.601705
[epoch 2, batch  1099] avg loss: 0.659841
[epoch 2, batch  1199] avg loss: 0.612001
[epoch 2, batch  1299] avg loss: 0.630604
[epoch 2, batch  1399] avg loss: 0.589862
[epoch 2, batch  1499] avg loss: 0.635134
[epoch 2, batch  1599] avg loss: 0.603054
[epoch 2, batch  1699] avg loss: 0.594151
[epoch 2, batch  1799] avg loss: 0.594046
[epoch 2, batch  1899] avg loss: 0.590004
[epoch 2, batch  1999] avg loss: 0.608000
[epoch 2, batch  2099] avg loss: 0.597534
[epoch 2, batch  2199] avg loss: 0.573486
[epoch 2, batch  2299] avg loss: 0.590634
[epoch 2, batch  2399] avg loss: 0.578278
[epoch 3, batch    99] avg loss: 0.592849
[epoch 3, batch   199] avg loss: 0.561448
[epoch 3, batch   299] avg loss: 0.662594
[epoch 3, batch   399] avg loss: 0.578635
[epoch 3, batch   499] avg loss: 0.560927
[epoch 3, batch   599] avg loss: 0.584663
[epoch 3, batch   699] avg loss: 0.604701
[epoch 3, batch   799] avg loss: 0.570427
[epoch 3, batch   899] avg loss: 0.565749
[epoch 3, batch   999] avg loss: 0.579781
[epoch 3, batch  1099] avg loss: 0.581819
[epoch 3, batch  1199] avg loss: 0.587379
[epoch 3, batch  1299] avg loss: 0.566332
[epoch 3, batch  1399] avg loss: 0.577751
[epoch 3, batch  1499] avg loss: 0.586232
[epoch 3, batch  1599] avg loss: 0.601989
[epoch 3, batch  1699] avg loss: 0.554202
[epoch 3, batch  1799] avg loss: 0.573969
[epoch 3, batch  1899] avg loss: 0.576526
[epoch 3, batch  1999] avg loss: 0.593170
[epoch 3, batch  2099] avg loss: 0.571973
[epoch 3, batch  2199] avg loss: 0.556035
[epoch 3, batch  2299] avg loss: 0.572769
[epoch 3, batch  2399] avg loss: 0.600042
[epoch 4, batch    99] avg loss: 0.588830
[epoch 4, batch   199] avg loss: 0.581341
[epoch 4, batch   299] avg loss: 0.569812
[epoch 4, batch   399] avg loss: 0.575392
[epoch 4, batch   499] avg loss: 0.562280
[epoch 4, batch   599] avg loss: 0.564344
[epoch 4, batch   699] avg loss: 0.567113
[epoch 4, batch   799] avg loss: 0.595491
[epoch 4, batch   899] avg loss: 0.560945
[epoch 4, batch   999] avg loss: 0.555597
[epoch 4, batch  1099] avg loss: 0.567170
[epoch 4, batch  1199] avg loss: 0.571831
[epoch 4, batch  1299] avg loss: 0.547406
[epoch 4, batch  1399] avg loss: 0.562283
[epoch 4, batch  1499] avg loss: 0.556566
[epoch 4, batch  1599] avg loss: 0.523166
[epoch 4, batch  1699] avg loss: 0.575262
[epoch 4, batch  1799] avg loss: 0.560976
[epoch 4, batch  1899] avg loss: 0.546174
[epoch 4, batch  1999] avg loss: 0.545141
[epoch 4, batch  2099] avg loss: 0.561835
[epoch 4, batch  2199] avg loss: 0.570616
[epoch 4, batch  2299] avg loss: 0.563996
[epoch 4, batch  2399] avg loss: 0.553276
[epoch 5, batch    99] avg loss: 0.562840
[epoch 5, batch   199] avg loss: 0.543750
[epoch 5, batch   299] avg loss: 0.552032
[epoch 5, batch   399] avg loss: 0.568194
[epoch 5, batch   499] avg loss: 0.533440
[epoch 5, batch   599] avg loss: 0.545410
[epoch 5, batch   699] avg loss: 0.533079
[epoch 5, batch   799] avg loss: 0.596523
[epoch 5, batch   899] avg loss: 0.525745
[epoch 5, batch   999] avg loss: 0.521836
[epoch 5, batch  1099] avg loss: 0.558598
[epoch 5, batch  1199] avg loss: 0.558670
[epoch 5, batch  1299] avg loss: 0.575855
[epoch 5, batch  1399] avg loss: 0.566905
[epoch 5, batch  1499] avg loss: 0.532872
[epoch 5, batch  1599] avg loss: 0.540140
[epoch 5, batch  1699] avg loss: 0.540670
[epoch 5, batch  1799] avg loss: 0.579068
[epoch 5, batch  1899] avg loss: 0.549841
[epoch 5, batch  1999] avg loss: 0.521530
[epoch 5, batch  2099] avg loss: 0.522381
[epoch 5, batch  2199] avg loss: 0.533527
[epoch 5, batch  2299] avg loss: 0.569765
[epoch 5, batch  2399] avg loss: 0.541508
[epoch 6, batch    99] avg loss: 0.539035
[epoch 6, batch   199] avg loss: 0.541062
[epoch 6, batch   299] avg loss: 0.565179
[epoch 6, batch   399] avg loss: 0.531374
[epoch 6, batch   499] avg loss: 0.529773
[epoch 6, batch   599] avg loss: 0.517576
[epoch 6, batch   699] avg loss: 0.534275
[epoch 6, batch   799] avg loss: 0.536689
[epoch 6, batch   899] avg loss: 0.512874
[epoch 6, batch   999] avg loss: 0.520074
[epoch 6, batch  1099] avg loss: 0.537828
[epoch 6, batch  1199] avg loss: 0.515663
[epoch 6, batch  1299] avg loss: 0.527889
[epoch 6, batch  1399] avg loss: 0.503582
[epoch 6, batch  1499] avg loss: 0.536425
[epoch 6, batch  1599] avg loss: 0.517565
[epoch 6, batch  1699] avg loss: 0.522086
[epoch 6, batch  1799] avg loss: 0.526708
[epoch 6, batch  1899] avg loss: 0.531169
[epoch 6, batch  1999] avg loss: 0.538682
[epoch 6, batch  2099] avg loss: 0.528849
[epoch 6, batch  2199] avg loss: 0.522082
[epoch 6, batch  2299] avg loss: 0.544105
[epoch 6, batch  2399] avg loss: 0.496424
[epoch 7, batch    99] avg loss: 0.512983
[epoch 7, batch   199] avg loss: 0.516456
[epoch 7, batch   299] avg loss: 0.509130
[epoch 7, batch   399] avg loss: 0.544270
[epoch 7, batch   499] avg loss: 0.522708
[epoch 7, batch   599] avg loss: 0.531877
[epoch 7, batch   699] avg loss: 0.551392
[epoch 7, batch   799] avg loss: 0.528238
[epoch 7, batch   899] avg loss: 0.516229
[epoch 7, batch   999] avg loss: 0.502193
[epoch 7, batch  1099] avg loss: 0.520410
[epoch 7, batch  1199] avg loss: 0.543811
[epoch 7, batch  1299] avg loss: 0.509280
[epoch 7, batch  1399] avg loss: 0.521545
[epoch 7, batch  1499] avg loss: 0.526587
[epoch 7, batch  1599] avg loss: 0.520405
[epoch 7, batch  1699] avg loss: 0.522563
[epoch 7, batch  1799] avg loss: 0.512241
[epoch 7, batch  1899] avg loss: 0.507212
[epoch 7, batch  1999] avg loss: 0.511333
[epoch 7, batch  2099] avg loss: 0.507588
[epoch 7, batch  2199] avg loss: 0.550175
[epoch 7, batch  2299] avg loss: 0.506845
[epoch 7, batch  2399] avg loss: 0.517650
[epoch 8, batch    99] avg loss: 0.492373
[epoch 8, batch   199] avg loss: 0.566872
[epoch 8, batch   299] avg loss: 0.509233
[epoch 8, batch   399] avg loss: 0.497638
[epoch 8, batch   499] avg loss: 0.490338
[epoch 8, batch   599] avg loss: 0.506104
[epoch 8, batch   699] avg loss: 0.540453
[epoch 8, batch   799] avg loss: 0.532075
[epoch 8, batch   899] avg loss: 0.513476
[epoch 8, batch   999] avg loss: 0.482950
[epoch 8, batch  1099] avg loss: 0.526117
[epoch 8, batch  1199] avg loss: 0.528030
[epoch 8, batch  1299] avg loss: 0.503842
[epoch 8, batch  1399] avg loss: 0.508606
[epoch 8, batch  1499] avg loss: 0.514084
[epoch 8, batch  1599] avg loss: 0.522050
[epoch 8, batch  1699] avg loss: 0.510757
[epoch 8, batch  1799] avg loss: 0.528104
[epoch 8, batch  1899] avg loss: 0.518537
[epoch 8, batch  1999] avg loss: 0.542285
[epoch 8, batch  2099] avg loss: 0.502234
[epoch 8, batch  2199] avg loss: 0.539600
[epoch 8, batch  2299] avg loss: 0.495368
[epoch 8, batch  2399] avg loss: 0.526369
[epoch 9, batch    99] avg loss: 0.486680
[epoch 9, batch   199] avg loss: 0.499857
[epoch 9, batch   299] avg loss: 0.517342
[epoch 9, batch   399] avg loss: 0.520122
[epoch 9, batch   499] avg loss: 0.514683
[epoch 9, batch   599] avg loss: 0.508628
[epoch 9, batch   699] avg loss: 0.508743
[epoch 9, batch   799] avg loss: 0.498602
[epoch 9, batch   899] avg loss: 0.511040
[epoch 9, batch   999] avg loss: 0.491523
[epoch 9, batch  1099] avg loss: 0.549104
[epoch 9, batch  1199] avg loss: 0.499987
[epoch 9, batch  1299] avg loss: 0.505127
[epoch 9, batch  1399] avg loss: 0.524215
[epoch 9, batch  1499] avg loss: 0.520510
[epoch 9, batch  1599] avg loss: 0.474703
[epoch 9, batch  1699] avg loss: 0.517556
[epoch 9, batch  1799] avg loss: 0.531299
[epoch 9, batch  1899] avg loss: 0.483066
[epoch 9, batch  1999] avg loss: 0.498621
[epoch 9, batch  2099] avg loss: 0.490460
[epoch 9, batch  2199] avg loss: 0.492020
[epoch 9, batch  2299] avg loss: 0.532208
[epoch 9, batch  2399] avg loss: 0.494461
[epoch 10, batch    99] avg loss: 0.490274
[epoch 10, batch   199] avg loss: 0.493781
[epoch 10, batch   299] avg loss: 0.494017
[epoch 10, batch   399] avg loss: 0.497639
[epoch 10, batch   499] avg loss: 0.503632
[epoch 10, batch   599] avg loss: 0.501541
[epoch 10, batch   699] avg loss: 0.525990
[epoch 10, batch   799] avg loss: 0.497790
[epoch 10, batch   899] avg loss: 0.494744
[epoch 10, batch   999] avg loss: 0.496186
[epoch 10, batch  1099] avg loss: 0.522889
[epoch 10, batch  1199] avg loss: 0.506566
[epoch 10, batch  1299] avg loss: 0.495904
[epoch 10, batch  1399] avg loss: 0.508413
[epoch 10, batch  1499] avg loss: 0.507354
[epoch 10, batch  1599] avg loss: 0.497336
[epoch 10, batch  1699] avg loss: 0.486169
[epoch 10, batch  1799] avg loss: 0.489761
[epoch 10, batch  1899] avg loss: 0.507462
[epoch 10, batch  1999] avg loss: 0.500199
[epoch 10, batch  2099] avg loss: 0.497865
[epoch 10, batch  2199] avg loss: 0.495285
[epoch 10, batch  2299] avg loss: 0.487663
[epoch 10, batch  2399] avg loss: 0.510411
[epoch 11, batch    99] avg loss: 0.482496
[epoch 11, batch   199] avg loss: 0.518661
[epoch 11, batch   299] avg loss: 0.471408
[epoch 11, batch   399] avg loss: 0.497454
[epoch 11, batch   499] avg loss: 0.486858
[epoch 11, batch   599] avg loss: 0.491495
[epoch 11, batch   699] avg loss: 0.498219
[epoch 11, batch   799] avg loss: 0.510624
[epoch 11, batch   899] avg loss: 0.477670
[epoch 11, batch   999] avg loss: 0.496927
[epoch 11, batch  1099] avg loss: 0.486183
[epoch 11, batch  1199] avg loss: 0.487449
[epoch 11, batch  1299] avg loss: 0.479874
[epoch 11, batch  1399] avg loss: 0.477442
[epoch 11, batch  1499] avg loss: 0.478712
[epoch 11, batch  1599] avg loss: 0.472255
[epoch 11, batch  1699] avg loss: 0.500670
[epoch 11, batch  1799] avg loss: 0.502299
[epoch 11, batch  1899] avg loss: 0.490032
[epoch 11, batch  1999] avg loss: 0.510815
[epoch 11, batch  2099] avg loss: 0.502985
[epoch 11, batch  2199] avg loss: 0.503386
[epoch 11, batch  2299] avg loss: 0.519531
[epoch 11, batch  2399] avg loss: 0.525325
[epoch 12, batch    99] avg loss: 0.467360
[epoch 12, batch   199] avg loss: 0.477930
[epoch 12, batch   299] avg loss: 0.479020
[epoch 12, batch   399] avg loss: 0.479200
[epoch 12, batch   499] avg loss: 0.457860
[epoch 12, batch   599] avg loss: 0.506235
[epoch 12, batch   699] avg loss: 0.479255
[epoch 12, batch   799] avg loss: 0.478444
[epoch 12, batch   899] avg loss: 0.482419
[epoch 12, batch   999] avg loss: 0.521271
[epoch 12, batch  1099] avg loss: 0.475917
[epoch 12, batch  1199] avg loss: 0.468281
[epoch 12, batch  1299] avg loss: 0.469850
[epoch 12, batch  1399] avg loss: 0.471992
[epoch 12, batch  1499] avg loss: 0.471361
[epoch 12, batch  1599] avg loss: 0.479153
[epoch 12, batch  1699] avg loss: 0.482601
[epoch 12, batch  1799] avg loss: 0.464176
[epoch 12, batch  1899] avg loss: 0.474061
[epoch 12, batch  1999] avg loss: 0.481377
[epoch 12, batch  2099] avg loss: 0.463976
[epoch 12, batch  2199] avg loss: 0.519129
[epoch 12, batch  2299] avg loss: 0.477784
[epoch 12, batch  2399] avg loss: 0.474830
[epoch 13, batch    99] avg loss: 0.472489
[epoch 13, batch   199] avg loss: 0.478729
[epoch 13, batch   299] avg loss: 0.470707
[epoch 13, batch   399] avg loss: 0.488296
[epoch 13, batch   499] avg loss: 0.469397
[epoch 13, batch   599] avg loss: 0.473361
[epoch 13, batch   699] avg loss: 0.486797
[epoch 13, batch   799] avg loss: 0.465961
[epoch 13, batch   899] avg loss: 0.505675
[epoch 13, batch   999] avg loss: 0.484199
[epoch 13, batch  1099] avg loss: 0.454137
[epoch 13, batch  1199] avg loss: 0.497359
[epoch 13, batch  1299] avg loss: 0.474783
[epoch 13, batch  1399] avg loss: 0.458454
[epoch 13, batch  1499] avg loss: 0.462195
[epoch 13, batch  1599] avg loss: 0.481972
[epoch 13, batch  1699] avg loss: 0.482335
[epoch 13, batch  1799] avg loss: 0.465882
[epoch 13, batch  1899] avg loss: 0.475327
[epoch 13, batch  1999] avg loss: 0.450829
[epoch 13, batch  2099] avg loss: 0.474452
[epoch 13, batch  2199] avg loss: 0.474203
[epoch 13, batch  2299] avg loss: 0.474868
[epoch 13, batch  2399] avg loss: 0.480560
[epoch 14, batch    99] avg loss: 0.451770
[epoch 14, batch   199] avg loss: 0.504485
[epoch 14, batch   299] avg loss: 0.458517
[epoch 14, batch   399] avg loss: 0.457449
[epoch 14, batch   499] avg loss: 0.486686
[epoch 14, batch   599] avg loss: 0.472700
[epoch 14, batch   699] avg loss: 0.473787
[epoch 14, batch   799] avg loss: 0.472722
[epoch 14, batch   899] avg loss: 0.481619
[epoch 14, batch   999] avg loss: 0.464786
[epoch 14, batch  1099] avg loss: 0.474205
[epoch 14, batch  1199] avg loss: 0.466785
[epoch 14, batch  1299] avg loss: 0.468984
[epoch 14, batch  1399] avg loss: 0.479068
[epoch 14, batch  1499] avg loss: 0.509545
[epoch 14, batch  1599] avg loss: 0.463407
[epoch 14, batch  1699] avg loss: 0.444513
[epoch 14, batch  1799] avg loss: 0.497992
[epoch 14, batch  1899] avg loss: 0.481139
[epoch 14, batch  1999] avg loss: 0.473306
[epoch 14, batch  2099] avg loss: 0.476727
[epoch 14, batch  2199] avg loss: 0.459948
[epoch 14, batch  2299] avg loss: 0.447440
[epoch 14, batch  2399] avg loss: 0.452231
[epoch 15, batch    99] avg loss: 0.454409
[epoch 15, batch   199] avg loss: 0.461949
[epoch 15, batch   299] avg loss: 0.452160
[epoch 15, batch   399] avg loss: 0.451201
[epoch 15, batch   499] avg loss: 0.470118
[epoch 15, batch   599] avg loss: 0.467973
[epoch 15, batch   699] avg loss: 0.458830
[epoch 15, batch   799] avg loss: 0.450652
[epoch 15, batch   899] avg loss: 0.450332
[epoch 15, batch   999] avg loss: 0.490734
[epoch 15, batch  1099] avg loss: 0.485830
[epoch 15, batch  1199] avg loss: 0.457332
[epoch 15, batch  1299] avg loss: 0.476006
[epoch 15, batch  1399] avg loss: 0.477252
[epoch 15, batch  1499] avg loss: 0.475990
[epoch 15, batch  1599] avg loss: 0.458851
[epoch 15, batch  1699] avg loss: 0.454236
[epoch 15, batch  1799] avg loss: 0.436813
[epoch 15, batch  1899] avg loss: 0.471166
[epoch 15, batch  1999] avg loss: 0.450538
[epoch 15, batch  2099] avg loss: 0.454614
[epoch 15, batch  2199] avg loss: 0.474829
[epoch 15, batch  2299] avg loss: 0.437646
[epoch 15, batch  2399] avg loss: 0.452027
[epoch 16, batch    99] avg loss: 0.463209
[epoch 16, batch   199] avg loss: 0.462897
[epoch 16, batch   299] avg loss: 0.475980
[epoch 16, batch   399] avg loss: 0.478373
[epoch 16, batch   499] avg loss: 0.450153
[epoch 16, batch   599] avg loss: 0.473071
[epoch 16, batch   699] avg loss: 0.492265
[epoch 16, batch   799] avg loss: 0.481044
[epoch 16, batch   899] avg loss: 0.493609
[epoch 16, batch   999] avg loss: 0.452358
[epoch 16, batch  1099] avg loss: 0.444081
[epoch 16, batch  1199] avg loss: 0.452305
[epoch 16, batch  1299] avg loss: 0.492612
[epoch 16, batch  1399] avg loss: 0.436130
[epoch 16, batch  1499] avg loss: 0.471794
[epoch 16, batch  1599] avg loss: 0.443652
[epoch 16, batch  1699] avg loss: 0.471270
[epoch 16, batch  1799] avg loss: 0.460717
[epoch 16, batch  1899] avg loss: 0.462009
[epoch 16, batch  1999] avg loss: 0.461367
[epoch 16, batch  2099] avg loss: 0.501465
[epoch 16, batch  2199] avg loss: 0.442619
[epoch 16, batch  2299] avg loss: 0.444943
[epoch 16, batch  2399] avg loss: 0.446934
[epoch 17, batch    99] avg loss: 0.466136
[epoch 17, batch   199] avg loss: 0.449325
[epoch 17, batch   299] avg loss: 0.451983
[epoch 17, batch   399] avg loss: 0.449766
[epoch 17, batch   499] avg loss: 0.454794
[epoch 17, batch   599] avg loss: 0.453774
[epoch 17, batch   699] avg loss: 0.449733
[epoch 17, batch   799] avg loss: 0.437752
[epoch 17, batch   899] avg loss: 0.468875
[epoch 17, batch   999] avg loss: 0.446050
[epoch 17, batch  1099] avg loss: 0.459859
[epoch 17, batch  1199] avg loss: 0.469980
[epoch 17, batch  1299] avg loss: 0.464638
[epoch 17, batch  1399] avg loss: 0.459961
[epoch 17, batch  1499] avg loss: 0.469041
[epoch 17, batch  1599] avg loss: 0.454546
[epoch 17, batch  1699] avg loss: 0.477960
[epoch 17, batch  1799] avg loss: 0.448902
[epoch 17, batch  1899] avg loss: 0.449763
[epoch 17, batch  1999] avg loss: 0.443738
[epoch 17, batch  2099] avg loss: 0.447357
[epoch 17, batch  2199] avg loss: 0.462182
[epoch 17, batch  2299] avg loss: 0.458867
[epoch 17, batch  2399] avg loss: 0.437433
[epoch 18, batch    99] avg loss: 0.451504
[epoch 18, batch   199] avg loss: 0.438481
[epoch 18, batch   299] avg loss: 0.458398
[epoch 18, batch   399] avg loss: 0.448368
[epoch 18, batch   499] avg loss: 0.452309
[epoch 18, batch   599] avg loss: 0.449396
[epoch 18, batch   699] avg loss: 0.445703
[epoch 18, batch   799] avg loss: 0.453772
[epoch 18, batch   899] avg loss: 0.490503
[epoch 18, batch   999] avg loss: 0.459996
[epoch 18, batch  1099] avg loss: 0.439118
[epoch 18, batch  1199] avg loss: 0.466957
[epoch 18, batch  1299] avg loss: 0.455062
[epoch 18, batch  1399] avg loss: 0.430222
[epoch 18, batch  1499] avg loss: 0.452294
[epoch 18, batch  1599] avg loss: 0.452635
[epoch 18, batch  1699] avg loss: 0.449202
[epoch 18, batch  1799] avg loss: 0.431695
[epoch 18, batch  1899] avg loss: 0.450566
[epoch 18, batch  1999] avg loss: 0.422946
[epoch 18, batch  2099] avg loss: 0.450572
[epoch 18, batch  2199] avg loss: 0.471553
[epoch 18, batch  2299] avg loss: 0.469347
[epoch 18, batch  2399] avg loss: 0.439472
[epoch 19, batch    99] avg loss: 0.459168
[epoch 19, batch   199] avg loss: 0.458179
[epoch 19, batch   299] avg loss: 0.434234
[epoch 19, batch   399] avg loss: 0.453264
[epoch 19, batch   499] avg loss: 0.459059
[epoch 19, batch   599] avg loss: 0.434796
[epoch 19, batch   699] avg loss: 0.436184
[epoch 19, batch   799] avg loss: 0.460366
[epoch 19, batch   899] avg loss: 0.455596
[epoch 19, batch   999] avg loss: 0.436131
[epoch 19, batch  1099] avg loss: 0.467380
[epoch 19, batch  1199] avg loss: 0.442240
[epoch 19, batch  1299] avg loss: 0.428486
[epoch 19, batch  1399] avg loss: 0.452279
[epoch 19, batch  1499] avg loss: 0.474411
[epoch 19, batch  1599] avg loss: 0.455474
[epoch 19, batch  1699] avg loss: 0.454604
[epoch 19, batch  1799] avg loss: 0.435671
[epoch 19, batch  1899] avg loss: 0.461135
[epoch 19, batch  1999] avg loss: 0.437005
[epoch 19, batch  2099] avg loss: 0.452029
[epoch 19, batch  2199] avg loss: 0.433703
[epoch 19, batch  2299] avg loss: 0.451629
[epoch 19, batch  2399] avg loss: 0.443670
Model saved to model/20200502-071739.pth.
accuracy/TriangPrismIsosc : 0.566
n_examples/TriangPrismIsosc : 500.0
accuracy/parallelepiped : 0.524
n_examples/parallelepiped : 500.0
accuracy/sphere : 1.0
n_examples/sphere : 102.0
accuracy/wire : 0.825
n_examples/wire : 200.0
accuracy/avg_geom : 0.6236559139784946
loss/validation_geom : 0.8824270705473588
accuracy/Au : 1.0
n_examples/Au : 1302.0
accuracy/SiN : 0.0
n_examples/SiN : 0.0
accuracy/SiO2 : 0.0
n_examples/SiO2 : 0.0
accuracy/avg_mat : 1.0
loss/validation_mat : 1.8806664999274443e-05
MSE/ShortestDim : 5.4532869517711635
MAE/ShortestDim : 1.6459601295342277
MSE/MiddleDim : 31.924527966664865
MAE/MiddleDim : 3.660435291296143
MSE/LongDim : 89.30463162142377
MAE/LongDim : 6.0865476874894995
MSE/log Area/Vol : 5.172129934284544
MAE/log Area/Vol : 1.62916259062455
loss/validation_dim : 131.85457647414435
loss/validation : 132.7370223513567
Metrics saved to model/20200502-071739_metrics.csv.
Parsed 2604 rows from data/sim_train_spectrum_SiN.
Parsed 2604 rows from data/sim_train_labels_SiN.
Parsed 9765 rows from data/gen_spectrum_SiN_00-of-16.
Parsed 9765 rows from data/gen_labels_SiN_00-of-16.
Parsed 9765 rows from data/gen_spectrum_SiN_01-of-16.
Parsed 9765 rows from data/gen_labels_SiN_01-of-16.
Parsed 9765 rows from data/gen_spectrum_SiN_02-of-16.
Parsed 9765 rows from data/gen_labels_SiN_02-of-16.
Parsed 9765 rows from data/gen_spectrum_SiN_03-of-16.
Parsed 9765 rows from data/gen_labels_SiN_03-of-16.
Parsed 9765 rows from data/gen_spectrum_SiN_04-of-16.
Parsed 9765 rows from data/gen_labels_SiN_04-of-16.
Parsed 9765 rows from data/gen_spectrum_SiN_05-of-16.
Parsed 9765 rows from data/gen_labels_SiN_05-of-16.
Parsed 9765 rows from data/gen_spectrum_SiN_06-of-16.
Parsed 9765 rows from data/gen_labels_SiN_06-of-16.
Parsed 9765 rows from data/gen_spectrum_SiN_07-of-16.
Parsed 9765 rows from data/gen_labels_SiN_07-of-16.
Parsed 9765 rows from data/gen_spectrum_SiN_08-of-16.
Parsed 9765 rows from data/gen_labels_SiN_08-of-16.
Parsed 9765 rows from data/gen_spectrum_SiN_09-of-16.
Parsed 9765 rows from data/gen_labels_SiN_09-of-16.
Parsed 9765 rows from data/gen_spectrum_SiN_10-of-16.
Parsed 9765 rows from data/gen_labels_SiN_10-of-16.
Parsed 9765 rows from data/gen_spectrum_SiN_11-of-16.
Parsed 9765 rows from data/gen_labels_SiN_11-of-16.
Parsed 9765 rows from data/gen_spectrum_SiN_12-of-16.
Parsed 9765 rows from data/gen_labels_SiN_12-of-16.
Parsed 9765 rows from data/gen_spectrum_SiN_13-of-16.
Parsed 9765 rows from data/gen_labels_SiN_13-of-16.
Parsed 9765 rows from data/gen_spectrum_SiN_14-of-16.
Parsed 9765 rows from data/gen_labels_SiN_14-of-16.
Parsed 9765 rows from data/gen_spectrum_SiN_15-of-16.
Parsed 9765 rows from data/gen_labels_SiN_15-of-16.
Parsed 1302 rows from data/sim_validation_spectrum_SiN.
Parsed 1302 rows from data/sim_validation_labels_SiN.
Logging training progress to tensorboard dir runs/alexnet-SiN-lr_0.000500-trainsize_158844-05_02_2020_07:18-multistage.
[epoch 0, batch    99] avg loss: 0.427893
[epoch 0, batch   199] avg loss: 0.221545
[epoch 0, batch   299] avg loss: 0.202619
[epoch 0, batch   399] avg loss: 0.169512
[epoch 0, batch   499] avg loss: 0.131132
[epoch 0, batch   599] avg loss: 0.120403
[epoch 0, batch   699] avg loss: 0.133206
[epoch 0, batch   799] avg loss: 0.115321
[epoch 0, batch   899] avg loss: 0.100115
[epoch 0, batch   999] avg loss: 0.127610
[epoch 0, batch  1099] avg loss: 0.117911
[epoch 0, batch  1199] avg loss: 0.104738
[epoch 0, batch  1299] avg loss: 0.125183
[epoch 0, batch  1399] avg loss: 0.101048
[epoch 0, batch  1499] avg loss: 0.113081
[epoch 0, batch  1599] avg loss: 0.090378
[epoch 0, batch  1699] avg loss: 0.106838
[epoch 0, batch  1799] avg loss: 0.103591
[epoch 0, batch  1899] avg loss: 0.090151
[epoch 0, batch  1999] avg loss: 0.092058
[epoch 0, batch  2099] avg loss: 0.098422
[epoch 0, batch  2199] avg loss: 0.091162
[epoch 0, batch  2299] avg loss: 0.098042
[epoch 0, batch  2399] avg loss: 0.088354
[epoch 1, batch    99] avg loss: 0.089616
[epoch 1, batch   199] avg loss: 0.097253
[epoch 1, batch   299] avg loss: 0.076416
[epoch 1, batch   399] avg loss: 0.097990
[epoch 1, batch   499] avg loss: 0.100092
[epoch 1, batch   599] avg loss: 0.086699
[epoch 1, batch   699] avg loss: 0.079853
[epoch 1, batch   799] avg loss: 0.085928
[epoch 1, batch   899] avg loss: 0.094156
[epoch 1, batch   999] avg loss: 0.083277
[epoch 1, batch  1099] avg loss: 0.095736
[epoch 1, batch  1199] avg loss: 0.091379
[epoch 1, batch  1299] avg loss: 0.084214
[epoch 1, batch  1399] avg loss: 0.082267
[epoch 1, batch  1499] avg loss: 0.085293
[epoch 1, batch  1599] avg loss: 0.096351
[epoch 1, batch  1699] avg loss: 0.082710
[epoch 1, batch  1799] avg loss: 0.095579
[epoch 1, batch  1899] avg loss: 0.084554
[epoch 1, batch  1999] avg loss: 0.087747
[epoch 1, batch  2099] avg loss: 0.081348
[epoch 1, batch  2199] avg loss: 0.072506
[epoch 1, batch  2299] avg loss: 0.084451
[epoch 1, batch  2399] avg loss: 0.086173
[epoch 2, batch    99] avg loss: 0.086281
[epoch 2, batch   199] avg loss: 0.084145
[epoch 2, batch   299] avg loss: 0.071333
[epoch 2, batch   399] avg loss: 0.073763
[epoch 2, batch   499] avg loss: 0.077211
[epoch 2, batch   599] avg loss: 0.076124
[epoch 2, batch   699] avg loss: 0.087754
[epoch 2, batch   799] avg loss: 0.085561
[epoch 2, batch   899] avg loss: 0.083887
[epoch 2, batch   999] avg loss: 0.083938
[epoch 2, batch  1099] avg loss: 0.062170
[epoch 2, batch  1199] avg loss: 0.076159
[epoch 2, batch  1299] avg loss: 0.074820
[epoch 2, batch  1399] avg loss: 0.076503
[epoch 2, batch  1499] avg loss: 0.085615
[epoch 2, batch  1599] avg loss: 0.084864
[epoch 2, batch  1699] avg loss: 0.076028
[epoch 2, batch  1799] avg loss: 0.073026
[epoch 2, batch  1899] avg loss: 0.087747
[epoch 2, batch  1999] avg loss: 0.081161
[epoch 2, batch  2099] avg loss: 0.068142
[epoch 2, batch  2199] avg loss: 0.077215
[epoch 2, batch  2299] avg loss: 0.076926
[epoch 2, batch  2399] avg loss: 0.088578
[epoch 3, batch    99] avg loss: 0.085769
[epoch 3, batch   199] avg loss: 0.060386
[epoch 3, batch   299] avg loss: 0.090104
[epoch 3, batch   399] avg loss: 0.080896
[epoch 3, batch   499] avg loss: 0.069098
[epoch 3, batch   599] avg loss: 0.067409
[epoch 3, batch   699] avg loss: 0.084861
[epoch 3, batch   799] avg loss: 0.078733
[epoch 3, batch   899] avg loss: 0.069017
[epoch 3, batch   999] avg loss: 0.079110
[epoch 3, batch  1099] avg loss: 0.082290
[epoch 3, batch  1199] avg loss: 0.068130
[epoch 3, batch  1299] avg loss: 0.076743
[epoch 3, batch  1399] avg loss: 0.076773
[epoch 3, batch  1499] avg loss: 0.077225
[epoch 3, batch  1599] avg loss: 0.074337
[epoch 3, batch  1699] avg loss: 0.067408
[epoch 3, batch  1799] avg loss: 0.064627
[epoch 3, batch  1899] avg loss: 0.076935
[epoch 3, batch  1999] avg loss: 0.074904
[epoch 3, batch  2099] avg loss: 0.073342
[epoch 3, batch  2199] avg loss: 0.071556
[epoch 3, batch  2299] avg loss: 0.071512
[epoch 3, batch  2399] avg loss: 0.065085
[epoch 4, batch    99] avg loss: 0.072400
[epoch 4, batch   199] avg loss: 0.073440
[epoch 4, batch   299] avg loss: 0.073013
[epoch 4, batch   399] avg loss: 0.074970
[epoch 4, batch   499] avg loss: 0.072984
[epoch 4, batch   599] avg loss: 0.071463
[epoch 4, batch   699] avg loss: 0.063363
[epoch 4, batch   799] avg loss: 0.073459
[epoch 4, batch   899] avg loss: 0.062553
[epoch 4, batch   999] avg loss: 0.067034
[epoch 4, batch  1099] avg loss: 0.068380
[epoch 4, batch  1199] avg loss: 0.088901
[epoch 4, batch  1299] avg loss: 0.063136
[epoch 4, batch  1399] avg loss: 0.082867
[epoch 4, batch  1499] avg loss: 0.079968
[epoch 4, batch  1599] avg loss: 0.082529
[epoch 4, batch  1699] avg loss: 0.066841
[epoch 4, batch  1799] avg loss: 0.057284
[epoch 4, batch  1899] avg loss: 0.072947
[epoch 4, batch  1999] avg loss: 0.063742
[epoch 4, batch  2099] avg loss: 0.058525
[epoch 4, batch  2199] avg loss: 0.076242
[epoch 4, batch  2299] avg loss: 0.063230
[epoch 4, batch  2399] avg loss: 0.070422
[epoch 5, batch    99] avg loss: 0.071103
[epoch 5, batch   199] avg loss: 0.077524
[epoch 5, batch   299] avg loss: 0.069164
[epoch 5, batch   399] avg loss: 0.072206
[epoch 5, batch   499] avg loss: 0.067243
[epoch 5, batch   599] avg loss: 0.073673
[epoch 5, batch   699] avg loss: 0.070172
[epoch 5, batch   799] avg loss: 0.063132
[epoch 5, batch   899] avg loss: 0.066685
[epoch 5, batch   999] avg loss: 0.065794
[epoch 5, batch  1099] avg loss: 0.071261
[epoch 5, batch  1199] avg loss: 0.069674
[epoch 5, batch  1299] avg loss: 0.069446
[epoch 5, batch  1399] avg loss: 0.070907
[epoch 5, batch  1499] avg loss: 0.059797
[epoch 5, batch  1599] avg loss: 0.060889
[epoch 5, batch  1699] avg loss: 0.077969
[epoch 5, batch  1799] avg loss: 0.070035
[epoch 5, batch  1899] avg loss: 0.071113
[epoch 5, batch  1999] avg loss: 0.065408
[epoch 5, batch  2099] avg loss: 0.067394
[epoch 5, batch  2199] avg loss: 0.068695
[epoch 5, batch  2299] avg loss: 0.067184
[epoch 5, batch  2399] avg loss: 0.063577
[epoch 6, batch    99] avg loss: 0.080823
[epoch 6, batch   199] avg loss: 0.069645
[epoch 6, batch   299] avg loss: 0.060252
[epoch 6, batch   399] avg loss: 0.066562
[epoch 6, batch   499] avg loss: 0.072687
[epoch 6, batch   599] avg loss: 0.076417
[epoch 6, batch   699] avg loss: 0.066363
[epoch 6, batch   799] avg loss: 0.058951
[epoch 6, batch   899] avg loss: 0.062429
[epoch 6, batch   999] avg loss: 0.074819
[epoch 6, batch  1099] avg loss: 0.069251
[epoch 6, batch  1199] avg loss: 0.066279
[epoch 6, batch  1299] avg loss: 0.066392
[epoch 6, batch  1399] avg loss: 0.067282
[epoch 6, batch  1499] avg loss: 0.070029
[epoch 6, batch  1599] avg loss: 0.062333
[epoch 6, batch  1699] avg loss: 0.082460
[epoch 6, batch  1799] avg loss: 0.066737
[epoch 6, batch  1899] avg loss: 0.058284
[epoch 6, batch  1999] avg loss: 0.073010
[epoch 6, batch  2099] avg loss: 0.067696
[epoch 6, batch  2199] avg loss: 0.068405
[epoch 6, batch  2299] avg loss: 0.059521
[epoch 6, batch  2399] avg loss: 0.062690
[epoch 7, batch    99] avg loss: 0.067879
[epoch 7, batch   199] avg loss: 0.065744
[epoch 7, batch   299] avg loss: 0.073747
[epoch 7, batch   399] avg loss: 0.072064
[epoch 7, batch   499] avg loss: 0.055683
[epoch 7, batch   599] avg loss: 0.073320
[epoch 7, batch   699] avg loss: 0.059954
[epoch 7, batch   799] avg loss: 0.069627
[epoch 7, batch   899] avg loss: 0.060462
[epoch 7, batch   999] avg loss: 0.071372
[epoch 7, batch  1099] avg loss: 0.060957
[epoch 7, batch  1199] avg loss: 0.072099
[epoch 7, batch  1299] avg loss: 0.064331
[epoch 7, batch  1399] avg loss: 0.074161
[epoch 7, batch  1499] avg loss: 0.064022
[epoch 7, batch  1599] avg loss: 0.058746
[epoch 7, batch  1699] avg loss: 0.072387
[epoch 7, batch  1799] avg loss: 0.062624
[epoch 7, batch  1899] avg loss: 0.070098
[epoch 7, batch  1999] avg loss: 0.077441
[epoch 7, batch  2099] avg loss: 0.070573
[epoch 7, batch  2199] avg loss: 0.062162
[epoch 7, batch  2299] avg loss: 0.070344
[epoch 7, batch  2399] avg loss: 0.065758
[epoch 8, batch    99] avg loss: 0.072502
[epoch 8, batch   199] avg loss: 0.059766
[epoch 8, batch   299] avg loss: 0.072560
[epoch 8, batch   399] avg loss: 0.065726
[epoch 8, batch   499] avg loss: 0.067583
[epoch 8, batch   599] avg loss: 0.069277
[epoch 8, batch   699] avg loss: 0.065468
[epoch 8, batch   799] avg loss: 0.062944
[epoch 8, batch   899] avg loss: 0.058843
[epoch 8, batch   999] avg loss: 0.053040
[epoch 8, batch  1099] avg loss: 0.070580
[epoch 8, batch  1199] avg loss: 0.063219
[epoch 8, batch  1299] avg loss: 0.062580
[epoch 8, batch  1399] avg loss: 0.061964
[epoch 8, batch  1499] avg loss: 0.068362
[epoch 8, batch  1599] avg loss: 0.073052
[epoch 8, batch  1699] avg loss: 0.067887
[epoch 8, batch  1799] avg loss: 0.064907
[epoch 8, batch  1899] avg loss: 0.070449
[epoch 8, batch  1999] avg loss: 0.069747
[epoch 8, batch  2099] avg loss: 0.057820
[epoch 8, batch  2199] avg loss: 0.063654
[epoch 8, batch  2299] avg loss: 0.060211
[epoch 8, batch  2399] avg loss: 0.071692
[epoch 9, batch    99] avg loss: 0.080643
[epoch 9, batch   199] avg loss: 0.067838
[epoch 9, batch   299] avg loss: 0.065065
[epoch 9, batch   399] avg loss: 0.054193
[epoch 9, batch   499] avg loss: 0.064607
[epoch 9, batch   599] avg loss: 0.073468
[epoch 9, batch   699] avg loss: 0.057246
[epoch 9, batch   799] avg loss: 0.052634
[epoch 9, batch   899] avg loss: 0.066753
[epoch 9, batch   999] avg loss: 0.078102
[epoch 9, batch  1099] avg loss: 0.059912
[epoch 9, batch  1199] avg loss: 0.068557
[epoch 9, batch  1299] avg loss: 0.073343
[epoch 9, batch  1399] avg loss: 0.061196
[epoch 9, batch  1499] avg loss: 0.060327
[epoch 9, batch  1599] avg loss: 0.064231
[epoch 9, batch  1699] avg loss: 0.058241
[epoch 9, batch  1799] avg loss: 0.067699
[epoch 9, batch  1899] avg loss: 0.066507
[epoch 9, batch  1999] avg loss: 0.062971
[epoch 9, batch  2099] avg loss: 0.066737
[epoch 9, batch  2199] avg loss: 0.067429
[epoch 9, batch  2299] avg loss: 0.060506
[epoch 9, batch  2399] avg loss: 0.062535
[epoch 10, batch    99] avg loss: 0.070040
[epoch 10, batch   199] avg loss: 0.066464
[epoch 10, batch   299] avg loss: 0.070027
[epoch 10, batch   399] avg loss: 0.059649
[epoch 10, batch   499] avg loss: 0.062596
[epoch 10, batch   599] avg loss: 0.061732
[epoch 10, batch   699] avg loss: 0.058293
[epoch 10, batch   799] avg loss: 0.065051
[epoch 10, batch   899] avg loss: 0.059505
[epoch 10, batch   999] avg loss: 0.072836
[epoch 10, batch  1099] avg loss: 0.060944
[epoch 10, batch  1199] avg loss: 0.067654
[epoch 10, batch  1299] avg loss: 0.063953
[epoch 10, batch  1399] avg loss: 0.064068
[epoch 10, batch  1499] avg loss: 0.071577
[epoch 10, batch  1599] avg loss: 0.056100
[epoch 10, batch  1699] avg loss: 0.066597
[epoch 10, batch  1799] avg loss: 0.061741
[epoch 10, batch  1899] avg loss: 0.067903
[epoch 10, batch  1999] avg loss: 0.069831
[epoch 10, batch  2099] avg loss: 0.051001
[epoch 10, batch  2199] avg loss: 0.069657
[epoch 10, batch  2299] avg loss: 0.056142
[epoch 10, batch  2399] avg loss: 0.066094
[epoch 11, batch    99] avg loss: 0.061613
[epoch 11, batch   199] avg loss: 0.078976
[epoch 11, batch   299] avg loss: 0.065295
[epoch 11, batch   399] avg loss: 0.067054
[epoch 11, batch   499] avg loss: 0.062038
[epoch 11, batch   599] avg loss: 0.055636
[epoch 11, batch   699] avg loss: 0.064303
[epoch 11, batch   799] avg loss: 0.067710
[epoch 11, batch   899] avg loss: 0.060846
[epoch 11, batch   999] avg loss: 0.063576
[epoch 11, batch  1099] avg loss: 0.070216
[epoch 11, batch  1199] avg loss: 0.059974
[epoch 11, batch  1299] avg loss: 0.057591
[epoch 11, batch  1399] avg loss: 0.064481
[epoch 11, batch  1499] avg loss: 0.064938
[epoch 11, batch  1599] avg loss: 0.062583
[epoch 11, batch  1699] avg loss: 0.062991
[epoch 11, batch  1799] avg loss: 0.060090
[epoch 11, batch  1899] avg loss: 0.064471
[epoch 11, batch  1999] avg loss: 0.062455
[epoch 11, batch  2099] avg loss: 0.063202
[epoch 11, batch  2199] avg loss: 0.055111
[epoch 11, batch  2299] avg loss: 0.054173
[epoch 11, batch  2399] avg loss: 0.063747
[epoch 12, batch    99] avg loss: 0.069792
[epoch 12, batch   199] avg loss: 0.062515
[epoch 12, batch   299] avg loss: 0.063029
[epoch 12, batch   399] avg loss: 0.067140
[epoch 12, batch   499] avg loss: 0.050028
[epoch 12, batch   599] avg loss: 0.055201
[epoch 12, batch   699] avg loss: 0.068188
[epoch 12, batch   799] avg loss: 0.058747
[epoch 12, batch   899] avg loss: 0.064974
[epoch 12, batch   999] avg loss: 0.066383
[epoch 12, batch  1099] avg loss: 0.062543
[epoch 12, batch  1199] avg loss: 0.071887
[epoch 12, batch  1299] avg loss: 0.057116
[epoch 12, batch  1399] avg loss: 0.056645
[epoch 12, batch  1499] avg loss: 0.070335
[epoch 12, batch  1599] avg loss: 0.067651
[epoch 12, batch  1699] avg loss: 0.058602
[epoch 12, batch  1799] avg loss: 0.076061
[epoch 12, batch  1899] avg loss: 0.062562
[epoch 12, batch  1999] avg loss: 0.058256
[epoch 12, batch  2099] avg loss: 0.059244
[epoch 12, batch  2199] avg loss: 0.064532
[epoch 12, batch  2299] avg loss: 0.062049
[epoch 12, batch  2399] avg loss: 0.063801
[epoch 13, batch    99] avg loss: 0.061647
[epoch 13, batch   199] avg loss: 0.058216
[epoch 13, batch   299] avg loss: 0.060221
[epoch 13, batch   399] avg loss: 0.063963
[epoch 13, batch   499] avg loss: 0.050552
[epoch 13, batch   599] avg loss: 0.068206
[epoch 13, batch   699] avg loss: 0.057295
[epoch 13, batch   799] avg loss: 0.066466
[epoch 13, batch   899] avg loss: 0.067666
[epoch 13, batch   999] avg loss: 0.061415
[epoch 13, batch  1099] avg loss: 0.065083
[epoch 13, batch  1199] avg loss: 0.064863
[epoch 13, batch  1299] avg loss: 0.064504
[epoch 13, batch  1399] avg loss: 0.069315
[epoch 13, batch  1499] avg loss: 0.057666
[epoch 13, batch  1599] avg loss: 0.061009
[epoch 13, batch  1699] avg loss: 0.060902
[epoch 13, batch  1799] avg loss: 0.061149
[epoch 13, batch  1899] avg loss: 0.054307
[epoch 13, batch  1999] avg loss: 0.067997
[epoch 13, batch  2099] avg loss: 0.056784
[epoch 13, batch  2199] avg loss: 0.074636
[epoch 13, batch  2299] avg loss: 0.060529
[epoch 13, batch  2399] avg loss: 0.062674
[epoch 14, batch    99] avg loss: 0.070388
[epoch 14, batch   199] avg loss: 0.058032
[epoch 14, batch   299] avg loss: 0.052452
[epoch 14, batch   399] avg loss: 0.066423
[epoch 14, batch   499] avg loss: 0.073532
[epoch 14, batch   599] avg loss: 0.056534
[epoch 14, batch   699] avg loss: 0.065871
[epoch 14, batch   799] avg loss: 0.066235
[epoch 14, batch   899] avg loss: 0.061318
[epoch 14, batch   999] avg loss: 0.059535
[epoch 14, batch  1099] avg loss: 0.062299
[epoch 14, batch  1199] avg loss: 0.058322
[epoch 14, batch  1299] avg loss: 0.063153
[epoch 14, batch  1399] avg loss: 0.057317
[epoch 14, batch  1499] avg loss: 0.063816
[epoch 14, batch  1599] avg loss: 0.061725
[epoch 14, batch  1699] avg loss: 0.061361
[epoch 14, batch  1799] avg loss: 0.058076
[epoch 14, batch  1899] avg loss: 0.063967
[epoch 14, batch  1999] avg loss: 0.068239
[epoch 14, batch  2099] avg loss: 0.062918
[epoch 14, batch  2199] avg loss: 0.059607
[epoch 14, batch  2299] avg loss: 0.054278
[epoch 14, batch  2399] avg loss: 0.059309
[epoch 15, batch    99] avg loss: 0.058108
[epoch 15, batch   199] avg loss: 0.062512
[epoch 15, batch   299] avg loss: 0.059055
[epoch 15, batch   399] avg loss: 0.061320
[epoch 15, batch   499] avg loss: 0.055327
[epoch 15, batch   599] avg loss: 0.064638
[epoch 15, batch   699] avg loss: 0.068839
[epoch 15, batch   799] avg loss: 0.057563
[epoch 15, batch   899] avg loss: 0.055713
[epoch 15, batch   999] avg loss: 0.064252
[epoch 15, batch  1099] avg loss: 0.061709
[epoch 15, batch  1199] avg loss: 0.059688
[epoch 15, batch  1299] avg loss: 0.063429
[epoch 15, batch  1399] avg loss: 0.063945
[epoch 15, batch  1499] avg loss: 0.062955
[epoch 15, batch  1599] avg loss: 0.062807
[epoch 15, batch  1699] avg loss: 0.061531
[epoch 15, batch  1799] avg loss: 0.059471
[epoch 15, batch  1899] avg loss: 0.060062
[epoch 15, batch  1999] avg loss: 0.069125
[epoch 15, batch  2099] avg loss: 0.065901
[epoch 15, batch  2199] avg loss: 0.055496
[epoch 15, batch  2299] avg loss: 0.062202
[epoch 15, batch  2399] avg loss: 0.057251
[epoch 16, batch    99] avg loss: 0.063543
[epoch 16, batch   199] avg loss: 0.060455
[epoch 16, batch   299] avg loss: 0.053192
[epoch 16, batch   399] avg loss: 0.060200
[epoch 16, batch   499] avg loss: 0.059936
[epoch 16, batch   599] avg loss: 0.058558
[epoch 16, batch   699] avg loss: 0.060651
[epoch 16, batch   799] avg loss: 0.056029
[epoch 16, batch   899] avg loss: 0.054807
[epoch 16, batch   999] avg loss: 0.065393
[epoch 16, batch  1099] avg loss: 0.054374
[epoch 16, batch  1199] avg loss: 0.065449
[epoch 16, batch  1299] avg loss: 0.067084
[epoch 16, batch  1399] avg loss: 0.060809
[epoch 16, batch  1499] avg loss: 0.063674
[epoch 16, batch  1599] avg loss: 0.055553
[epoch 16, batch  1699] avg loss: 0.061792
[epoch 16, batch  1799] avg loss: 0.062386
[epoch 16, batch  1899] avg loss: 0.066465
[epoch 16, batch  1999] avg loss: 0.059437
[epoch 16, batch  2099] avg loss: 0.059435
[epoch 16, batch  2199] avg loss: 0.068385
[epoch 16, batch  2299] avg loss: 0.067873
[epoch 16, batch  2399] avg loss: 0.062453
[epoch 17, batch    99] avg loss: 0.060266
[epoch 17, batch   199] avg loss: 0.059324
[epoch 17, batch   299] avg loss: 0.064648
[epoch 17, batch   399] avg loss: 0.071710
[epoch 17, batch   499] avg loss: 0.055049
[epoch 17, batch   599] avg loss: 0.062456
[epoch 17, batch   699] avg loss: 0.065731
[epoch 17, batch   799] avg loss: 0.060993
[epoch 17, batch   899] avg loss: 0.059240
[epoch 17, batch   999] avg loss: 0.059261
[epoch 17, batch  1099] avg loss: 0.063424
[epoch 17, batch  1199] avg loss: 0.061267
[epoch 17, batch  1299] avg loss: 0.061241
[epoch 17, batch  1399] avg loss: 0.055490
[epoch 17, batch  1499] avg loss: 0.062621
[epoch 17, batch  1599] avg loss: 0.067419
[epoch 17, batch  1699] avg loss: 0.051822
[epoch 17, batch  1799] avg loss: 0.063058
[epoch 17, batch  1899] avg loss: 0.051537
[epoch 17, batch  1999] avg loss: 0.057158
[epoch 17, batch  2099] avg loss: 0.053055
[epoch 17, batch  2199] avg loss: 0.060757
[epoch 17, batch  2299] avg loss: 0.068840
[epoch 17, batch  2399] avg loss: 0.061382
[epoch 18, batch    99] avg loss: 0.061492
[epoch 18, batch   199] avg loss: 0.049882
[epoch 18, batch   299] avg loss: 0.062387
[epoch 18, batch   399] avg loss: 0.066138
[epoch 18, batch   499] avg loss: 0.066520
[epoch 18, batch   599] avg loss: 0.064342
[epoch 18, batch   699] avg loss: 0.059769
[epoch 18, batch   799] avg loss: 0.071099
[epoch 18, batch   899] avg loss: 0.049506
[epoch 18, batch   999] avg loss: 0.057451
[epoch 18, batch  1099] avg loss: 0.067555
[epoch 18, batch  1199] avg loss: 0.056514
[epoch 18, batch  1299] avg loss: 0.059742
[epoch 18, batch  1399] avg loss: 0.064346
[epoch 18, batch  1499] avg loss: 0.060733
[epoch 18, batch  1599] avg loss: 0.070695
[epoch 18, batch  1699] avg loss: 0.060408
[epoch 18, batch  1799] avg loss: 0.065481
[epoch 18, batch  1899] avg loss: 0.059833
[epoch 18, batch  1999] avg loss: 0.053245
[epoch 18, batch  2099] avg loss: 0.052938
[epoch 18, batch  2199] avg loss: 0.055259
[epoch 18, batch  2299] avg loss: 0.056388
[epoch 18, batch  2399] avg loss: 0.056022
[epoch 19, batch    99] avg loss: 0.053292
[epoch 19, batch   199] avg loss: 0.062993
[epoch 19, batch   299] avg loss: 0.054773
[epoch 19, batch   399] avg loss: 0.053023
[epoch 19, batch   499] avg loss: 0.054174
[epoch 19, batch   599] avg loss: 0.072134
[epoch 19, batch   699] avg loss: 0.056060
[epoch 19, batch   799] avg loss: 0.063275
[epoch 19, batch   899] avg loss: 0.059697
[epoch 19, batch   999] avg loss: 0.052502
[epoch 19, batch  1099] avg loss: 0.055815
[epoch 19, batch  1199] avg loss: 0.059473
[epoch 19, batch  1299] avg loss: 0.064127
[epoch 19, batch  1399] avg loss: 0.063255
[epoch 19, batch  1499] avg loss: 0.064111
[epoch 19, batch  1599] avg loss: 0.068186
[epoch 19, batch  1699] avg loss: 0.062166
[epoch 19, batch  1799] avg loss: 0.069219
[epoch 19, batch  1899] avg loss: 0.058205
[epoch 19, batch  1999] avg loss: 0.058869
[epoch 19, batch  2099] avg loss: 0.064195
[epoch 19, batch  2199] avg loss: 0.059748
[epoch 19, batch  2299] avg loss: 0.056311
[epoch 19, batch  2399] avg loss: 0.056461
Model saved to model/20200502-073533.pth.
accuracy/TriangPrismIsosc : 0.0
n_examples/TriangPrismIsosc : 500.0
accuracy/parallelepiped : 0.018
n_examples/parallelepiped : 500.0
accuracy/sphere : 0.9509803921568627
n_examples/sphere : 102.0
accuracy/wire : 0.0
n_examples/wire : 200.0
accuracy/avg_geom : 0.08141321044546851
loss/validation_geom : 1.5927706863473636
accuracy/Au : 0.0
n_examples/Au : 0.0
accuracy/SiN : 1.0
n_examples/SiN : 1302.0
accuracy/SiO2 : 0.0
n_examples/SiO2 : 0.0
accuracy/avg_mat : 1.0
loss/validation_mat : 0.0
MSE/ShortestDim : 0.2085863793134323
MAE/ShortestDim : 0.24198893211587416
MSE/MiddleDim : 0.4825628232296711
MAE/MiddleDim : 0.4419078856203047
MSE/LongDim : 21.07264596965456
MAE/LongDim : 2.315802994594779
MSE/log Area/Vol : 0.11722546403858519
MAE/log Area/Vol : 0.2548459539398803
loss/validation_dim : 21.881020636236247
loss/validation : 23.47379132258361
Metrics saved to model/20200502-073533_metrics.csv.
[epoch 0, batch    99] avg loss: 1.075392
[epoch 0, batch   199] avg loss: 0.810796
[epoch 0, batch   299] avg loss: 0.742933
[epoch 0, batch   399] avg loss: 0.721259
[epoch 0, batch   499] avg loss: 0.695498
[epoch 0, batch   599] avg loss: 0.673468
[epoch 0, batch   699] avg loss: 0.646236
[epoch 0, batch   799] avg loss: 0.611619
[epoch 0, batch   899] avg loss: 0.614066
[epoch 0, batch   999] avg loss: 0.603252
[epoch 0, batch  1099] avg loss: 0.573076
[epoch 0, batch  1199] avg loss: 0.559022
[epoch 0, batch  1299] avg loss: 0.571137
[epoch 0, batch  1399] avg loss: 0.548558
[epoch 0, batch  1499] avg loss: 0.563987
[epoch 0, batch  1599] avg loss: 0.568876
[epoch 0, batch  1699] avg loss: 0.545498
[epoch 0, batch  1799] avg loss: 0.544368
[epoch 0, batch  1899] avg loss: 0.539025
[epoch 0, batch  1999] avg loss: 0.555770
[epoch 0, batch  2099] avg loss: 0.540980
[epoch 0, batch  2199] avg loss: 0.531487
[epoch 0, batch  2299] avg loss: 0.531149
[epoch 0, batch  2399] avg loss: 0.533880
[epoch 1, batch    99] avg loss: 0.522461
[epoch 1, batch   199] avg loss: 0.513293
[epoch 1, batch   299] avg loss: 0.512891
[epoch 1, batch   399] avg loss: 0.511720
[epoch 1, batch   499] avg loss: 0.513678
[epoch 1, batch   599] avg loss: 0.515759
[epoch 1, batch   699] avg loss: 0.533625
[epoch 1, batch   799] avg loss: 0.525745
[epoch 1, batch   899] avg loss: 0.526534
[epoch 1, batch   999] avg loss: 0.514469
[epoch 1, batch  1099] avg loss: 0.523804
[epoch 1, batch  1199] avg loss: 0.517155
[epoch 1, batch  1299] avg loss: 0.495146
[epoch 1, batch  1399] avg loss: 0.495465
[epoch 1, batch  1499] avg loss: 0.505031
[epoch 1, batch  1599] avg loss: 0.491732
[epoch 1, batch  1699] avg loss: 0.502838
[epoch 1, batch  1799] avg loss: 0.499444
[epoch 1, batch  1899] avg loss: 0.501405
[epoch 1, batch  1999] avg loss: 0.512229
[epoch 1, batch  2099] avg loss: 0.496400
[epoch 1, batch  2199] avg loss: 0.492733
[epoch 1, batch  2299] avg loss: 0.493413
[epoch 1, batch  2399] avg loss: 0.525393
[epoch 2, batch    99] avg loss: 0.492882
[epoch 2, batch   199] avg loss: 0.490537
[epoch 2, batch   299] avg loss: 0.505681
[epoch 2, batch   399] avg loss: 0.490198
[epoch 2, batch   499] avg loss: 0.496289
[epoch 2, batch   599] avg loss: 0.508644
[epoch 2, batch   699] avg loss: 0.480784
[epoch 2, batch   799] avg loss: 0.497039
[epoch 2, batch   899] avg loss: 0.476994
[epoch 2, batch   999] avg loss: 0.512580
[epoch 2, batch  1099] avg loss: 0.474141
[epoch 2, batch  1199] avg loss: 0.477084
[epoch 2, batch  1299] avg loss: 0.500265
[epoch 2, batch  1399] avg loss: 0.480922
[epoch 2, batch  1499] avg loss: 0.479604
[epoch 2, batch  1599] avg loss: 0.487647
[epoch 2, batch  1699] avg loss: 0.478245
[epoch 2, batch  1799] avg loss: 0.495766
[epoch 2, batch  1899] avg loss: 0.484171
[epoch 2, batch  1999] avg loss: 0.486822
[epoch 2, batch  2099] avg loss: 0.493662
[epoch 2, batch  2199] avg loss: 0.477300
[epoch 2, batch  2299] avg loss: 0.475360
[epoch 2, batch  2399] avg loss: 0.493841
[epoch 3, batch    99] avg loss: 0.488065
[epoch 3, batch   199] avg loss: 0.474455
[epoch 3, batch   299] avg loss: 0.480630
[epoch 3, batch   399] avg loss: 0.497939
[epoch 3, batch   499] avg loss: 0.486863
[epoch 3, batch   599] avg loss: 0.463413
[epoch 3, batch   699] avg loss: 0.466028
[epoch 3, batch   799] avg loss: 0.493812
[epoch 3, batch   899] avg loss: 0.488031
[epoch 3, batch   999] avg loss: 0.472422
[epoch 3, batch  1099] avg loss: 0.479876
[epoch 3, batch  1199] avg loss: 0.467392
[epoch 3, batch  1299] avg loss: 0.464136
[epoch 3, batch  1399] avg loss: 0.479646
[epoch 3, batch  1499] avg loss: 0.469598
[epoch 3, batch  1599] avg loss: 0.468724
[epoch 3, batch  1699] avg loss: 0.484871
[epoch 3, batch  1799] avg loss: 0.469631
[epoch 3, batch  1899] avg loss: 0.478144
[epoch 3, batch  1999] avg loss: 0.480372
[epoch 3, batch  2099] avg loss: 0.466653
[epoch 3, batch  2199] avg loss: 0.466047
[epoch 3, batch  2299] avg loss: 0.462624
[epoch 3, batch  2399] avg loss: 0.460371
[epoch 4, batch    99] avg loss: 0.458156
[epoch 4, batch   199] avg loss: 0.467749
[epoch 4, batch   299] avg loss: 0.480822
[epoch 4, batch   399] avg loss: 0.447529
[epoch 4, batch   499] avg loss: 0.468916
[epoch 4, batch   599] avg loss: 0.470335
[epoch 4, batch   699] avg loss: 0.488756
[epoch 4, batch   799] avg loss: 0.472217
[epoch 4, batch   899] avg loss: 0.469726
[epoch 4, batch   999] avg loss: 0.470187
[epoch 4, batch  1099] avg loss: 0.466600
[epoch 4, batch  1199] avg loss: 0.466179
[epoch 4, batch  1299] avg loss: 0.462670
[epoch 4, batch  1399] avg loss: 0.471404
[epoch 4, batch  1499] avg loss: 0.470670
[epoch 4, batch  1599] avg loss: 0.468084
[epoch 4, batch  1699] avg loss: 0.453835
[epoch 4, batch  1799] avg loss: 0.471140
[epoch 4, batch  1899] avg loss: 0.452612
[epoch 4, batch  1999] avg loss: 0.456081
[epoch 4, batch  2099] avg loss: 0.459237
[epoch 4, batch  2199] avg loss: 0.472142
[epoch 4, batch  2299] avg loss: 0.459021
[epoch 4, batch  2399] avg loss: 0.468985
[epoch 5, batch    99] avg loss: 0.459785
[epoch 5, batch   199] avg loss: 0.429003
[epoch 5, batch   299] avg loss: 0.464334
[epoch 5, batch   399] avg loss: 0.449499
[epoch 5, batch   499] avg loss: 0.448870
[epoch 5, batch   599] avg loss: 0.449325
[epoch 5, batch   699] avg loss: 0.463422
[epoch 5, batch   799] avg loss: 0.459348
[epoch 5, batch   899] avg loss: 0.448280
[epoch 5, batch   999] avg loss: 0.450075
[epoch 5, batch  1099] avg loss: 0.456415
[epoch 5, batch  1199] avg loss: 0.466517
[epoch 5, batch  1299] avg loss: 0.460400
[epoch 5, batch  1399] avg loss: 0.463925
[epoch 5, batch  1499] avg loss: 0.451579
[epoch 5, batch  1599] avg loss: 0.459486
[epoch 5, batch  1699] avg loss: 0.481768
[epoch 5, batch  1799] avg loss: 0.443441
[epoch 5, batch  1899] avg loss: 0.451005
[epoch 5, batch  1999] avg loss: 0.439378
[epoch 5, batch  2099] avg loss: 0.446281
[epoch 5, batch  2199] avg loss: 0.450397
[epoch 5, batch  2299] avg loss: 0.458402
[epoch 5, batch  2399] avg loss: 0.450309
[epoch 6, batch    99] avg loss: 0.459782
[epoch 6, batch   199] avg loss: 0.457021
[epoch 6, batch   299] avg loss: 0.454501
[epoch 6, batch   399] avg loss: 0.443323
[epoch 6, batch   499] avg loss: 0.478470
[epoch 6, batch   599] avg loss: 0.464483
[epoch 6, batch   699] avg loss: 0.459654
[epoch 6, batch   799] avg loss: 0.449868
[epoch 6, batch   899] avg loss: 0.450478
[epoch 6, batch   999] avg loss: 0.444746
[epoch 6, batch  1099] avg loss: 0.455620
[epoch 6, batch  1199] avg loss: 0.439014
[epoch 6, batch  1299] avg loss: 0.436173
[epoch 6, batch  1399] avg loss: 0.444843
[epoch 6, batch  1499] avg loss: 0.451808
[epoch 6, batch  1599] avg loss: 0.420015
[epoch 6, batch  1699] avg loss: 0.441652
[epoch 6, batch  1799] avg loss: 0.428281
[epoch 6, batch  1899] avg loss: 0.450923
[epoch 6, batch  1999] avg loss: 0.423825
[epoch 6, batch  2099] avg loss: 0.431394
[epoch 6, batch  2199] avg loss: 0.448604
[epoch 6, batch  2299] avg loss: 0.445916
[epoch 6, batch  2399] avg loss: 0.451340
[epoch 7, batch    99] avg loss: 0.440148
[epoch 7, batch   199] avg loss: 0.443298
[epoch 7, batch   299] avg loss: 0.442329
[epoch 7, batch   399] avg loss: 0.452573
[epoch 7, batch   499] avg loss: 0.448607
[epoch 7, batch   599] avg loss: 0.440315
[epoch 7, batch   699] avg loss: 0.425005
[epoch 7, batch   799] avg loss: 0.440567
[epoch 7, batch   899] avg loss: 0.451451
[epoch 7, batch   999] avg loss: 0.441295
[epoch 7, batch  1099] avg loss: 0.443395
[epoch 7, batch  1199] avg loss: 0.432504
[epoch 7, batch  1299] avg loss: 0.445858
[epoch 7, batch  1399] avg loss: 0.444032
[epoch 7, batch  1499] avg loss: 0.428219
[epoch 7, batch  1599] avg loss: 0.474209
[epoch 7, batch  1699] avg loss: 0.431577
[epoch 7, batch  1799] avg loss: 0.446249
[epoch 7, batch  1899] avg loss: 0.439407
[epoch 7, batch  1999] avg loss: 0.439268
[epoch 7, batch  2099] avg loss: 0.427893
[epoch 7, batch  2199] avg loss: 0.420300
[epoch 7, batch  2299] avg loss: 0.444294
[epoch 7, batch  2399] avg loss: 0.433206
[epoch 8, batch    99] avg loss: 0.423028
[epoch 8, batch   199] avg loss: 0.424238
[epoch 8, batch   299] avg loss: 0.439279
[epoch 8, batch   399] avg loss: 0.450854
[epoch 8, batch   499] avg loss: 0.432005
[epoch 8, batch   599] avg loss: 0.438374
[epoch 8, batch   699] avg loss: 0.428548
[epoch 8, batch   799] avg loss: 0.428710
[epoch 8, batch   899] avg loss: 0.442430
[epoch 8, batch   999] avg loss: 0.460507
[epoch 8, batch  1099] avg loss: 0.439116
[epoch 8, batch  1199] avg loss: 0.427514
[epoch 8, batch  1299] avg loss: 0.427634
[epoch 8, batch  1399] avg loss: 0.429328
[epoch 8, batch  1499] avg loss: 0.450129
[epoch 8, batch  1599] avg loss: 0.446339
[epoch 8, batch  1699] avg loss: 0.443851
[epoch 8, batch  1799] avg loss: 0.436863
[epoch 8, batch  1899] avg loss: 0.428970
[epoch 8, batch  1999] avg loss: 0.430804
[epoch 8, batch  2099] avg loss: 0.435630
[epoch 8, batch  2199] avg loss: 0.421669
[epoch 8, batch  2299] avg loss: 0.432451
[epoch 8, batch  2399] avg loss: 0.429160
[epoch 9, batch    99] avg loss: 0.429010
[epoch 9, batch   199] avg loss: 0.438626
[epoch 9, batch   299] avg loss: 0.441378
[epoch 9, batch   399] avg loss: 0.428789
[epoch 9, batch   499] avg loss: 0.425017
[epoch 9, batch   599] avg loss: 0.437135
[epoch 9, batch   699] avg loss: 0.431486
[epoch 9, batch   799] avg loss: 0.426534
[epoch 9, batch   899] avg loss: 0.443482
[epoch 9, batch   999] avg loss: 0.458508
[epoch 9, batch  1099] avg loss: 0.425453
[epoch 9, batch  1199] avg loss: 0.412099
[epoch 9, batch  1299] avg loss: 0.441264
[epoch 9, batch  1399] avg loss: 0.423437
[epoch 9, batch  1499] avg loss: 0.438849
[epoch 9, batch  1599] avg loss: 0.436971
[epoch 9, batch  1699] avg loss: 0.423430
[epoch 9, batch  1799] avg loss: 0.425152
[epoch 9, batch  1899] avg loss: 0.434902
[epoch 9, batch  1999] avg loss: 0.442354
[epoch 9, batch  2099] avg loss: 0.430532
[epoch 9, batch  2199] avg loss: 0.418779
[epoch 9, batch  2299] avg loss: 0.428303
[epoch 9, batch  2399] avg loss: 0.433564
[epoch 10, batch    99] avg loss: 0.431263
[epoch 10, batch   199] avg loss: 0.410350
[epoch 10, batch   299] avg loss: 0.427704
[epoch 10, batch   399] avg loss: 0.426258
[epoch 10, batch   499] avg loss: 0.438722
[epoch 10, batch   599] avg loss: 0.426221
[epoch 10, batch   699] avg loss: 0.419772
[epoch 10, batch   799] avg loss: 0.421524
[epoch 10, batch   899] avg loss: 0.438263
[epoch 10, batch   999] avg loss: 0.417576
[epoch 10, batch  1099] avg loss: 0.423796
[epoch 10, batch  1199] avg loss: 0.438874
[epoch 10, batch  1299] avg loss: 0.431136
[epoch 10, batch  1399] avg loss: 0.415935
[epoch 10, batch  1499] avg loss: 0.416711
[epoch 10, batch  1599] avg loss: 0.436699
[epoch 10, batch  1699] avg loss: 0.423078
[epoch 10, batch  1799] avg loss: 0.431972
[epoch 10, batch  1899] avg loss: 0.421682
[epoch 10, batch  1999] avg loss: 0.418484
[epoch 10, batch  2099] avg loss: 0.413039
[epoch 10, batch  2199] avg loss: 0.424794
[epoch 10, batch  2299] avg loss: 0.430596
[epoch 10, batch  2399] avg loss: 0.423209
[epoch 11, batch    99] avg loss: 0.416508
[epoch 11, batch   199] avg loss: 0.417459
[epoch 11, batch   299] avg loss: 0.431835
[epoch 11, batch   399] avg loss: 0.422140
[epoch 11, batch   499] avg loss: 0.424886
[epoch 11, batch   599] avg loss: 0.440442
[epoch 11, batch   699] avg loss: 0.435463
[epoch 11, batch   799] avg loss: 0.426365
[epoch 11, batch   899] avg loss: 0.418260
[epoch 11, batch   999] avg loss: 0.425190
[epoch 11, batch  1099] avg loss: 0.427792
[epoch 11, batch  1199] avg loss: 0.430134
[epoch 11, batch  1299] avg loss: 0.432659
[epoch 11, batch  1399] avg loss: 0.431831
[epoch 11, batch  1499] avg loss: 0.412165
[epoch 11, batch  1599] avg loss: 0.422074
[epoch 11, batch  1699] avg loss: 0.414962
[epoch 11, batch  1799] avg loss: 0.436520
[epoch 11, batch  1899] avg loss: 0.416132
[epoch 11, batch  1999] avg loss: 0.423173
[epoch 11, batch  2099] avg loss: 0.412004
[epoch 11, batch  2199] avg loss: 0.438843
[epoch 11, batch  2299] avg loss: 0.422513
[epoch 11, batch  2399] avg loss: 0.404955
[epoch 12, batch    99] avg loss: 0.418285
[epoch 12, batch   199] avg loss: 0.445732
[epoch 12, batch   299] avg loss: 0.417975
[epoch 12, batch   399] avg loss: 0.423340
[epoch 12, batch   499] avg loss: 0.404052
[epoch 12, batch   599] avg loss: 0.431891
[epoch 12, batch   699] avg loss: 0.422441
[epoch 12, batch   799] avg loss: 0.423977
[epoch 12, batch   899] avg loss: 0.411119
[epoch 12, batch   999] avg loss: 0.414522
[epoch 12, batch  1099] avg loss: 0.425233
[epoch 12, batch  1199] avg loss: 0.415962
[epoch 12, batch  1299] avg loss: 0.407199
[epoch 12, batch  1399] avg loss: 0.421977
[epoch 12, batch  1499] avg loss: 0.386310
[epoch 12, batch  1599] avg loss: 0.444370
[epoch 12, batch  1699] avg loss: 0.424008
[epoch 12, batch  1799] avg loss: 0.419020
[epoch 12, batch  1899] avg loss: 0.421084
[epoch 12, batch  1999] avg loss: 0.435146
[epoch 12, batch  2099] avg loss: 0.419492
[epoch 12, batch  2199] avg loss: 0.424456
[epoch 12, batch  2299] avg loss: 0.416985
[epoch 12, batch  2399] avg loss: 0.417041
[epoch 13, batch    99] avg loss: 0.414042
[epoch 13, batch   199] avg loss: 0.434115
[epoch 13, batch   299] avg loss: 0.413438
[epoch 13, batch   399] avg loss: 0.425223
[epoch 13, batch   499] avg loss: 0.433043
[epoch 13, batch   599] avg loss: 0.399198
[epoch 13, batch   699] avg loss: 0.403952
[epoch 13, batch   799] avg loss: 0.421218
[epoch 13, batch   899] avg loss: 0.402691
[epoch 13, batch   999] avg loss: 0.408882
[epoch 13, batch  1099] avg loss: 0.429737
[epoch 13, batch  1199] avg loss: 0.431656
[epoch 13, batch  1299] avg loss: 0.412465
[epoch 13, batch  1399] avg loss: 0.409217
[epoch 13, batch  1499] avg loss: 0.412107
[epoch 13, batch  1599] avg loss: 0.417511
[epoch 13, batch  1699] avg loss: 0.409698
[epoch 13, batch  1799] avg loss: 0.417852
[epoch 13, batch  1899] avg loss: 0.413502
[epoch 13, batch  1999] avg loss: 0.399787
[epoch 13, batch  2099] avg loss: 0.409830
[epoch 13, batch  2199] avg loss: 0.420317
[epoch 13, batch  2299] avg loss: 0.428719
[epoch 13, batch  2399] avg loss: 0.433501
[epoch 14, batch    99] avg loss: 0.409813
[epoch 14, batch   199] avg loss: 0.413320
[epoch 14, batch   299] avg loss: 0.419791
[epoch 14, batch   399] avg loss: 0.416760
[epoch 14, batch   499] avg loss: 0.413555
[epoch 14, batch   599] avg loss: 0.407791
[epoch 14, batch   699] avg loss: 0.402158
[epoch 14, batch   799] avg loss: 0.415489
[epoch 14, batch   899] avg loss: 0.409668
[epoch 14, batch   999] avg loss: 0.399189
[epoch 14, batch  1099] avg loss: 0.412167
[epoch 14, batch  1199] avg loss: 0.417184
[epoch 14, batch  1299] avg loss: 0.400813
[epoch 14, batch  1399] avg loss: 0.424155
[epoch 14, batch  1499] avg loss: 0.410666
[epoch 14, batch  1599] avg loss: 0.395906
[epoch 14, batch  1699] avg loss: 0.444467
[epoch 14, batch  1799] avg loss: 0.416121
[epoch 14, batch  1899] avg loss: 0.396392
[epoch 14, batch  1999] avg loss: 0.404994
[epoch 14, batch  2099] avg loss: 0.398436
[epoch 14, batch  2199] avg loss: 0.420081
[epoch 14, batch  2299] avg loss: 0.414012
[epoch 14, batch  2399] avg loss: 0.420012
[epoch 15, batch    99] avg loss: 0.410963
[epoch 15, batch   199] avg loss: 0.403716
[epoch 15, batch   299] avg loss: 0.400903
[epoch 15, batch   399] avg loss: 0.414555
[epoch 15, batch   499] avg loss: 0.407579
[epoch 15, batch   599] avg loss: 0.413785
[epoch 15, batch   699] avg loss: 0.403228
[epoch 15, batch   799] avg loss: 0.411719
[epoch 15, batch   899] avg loss: 0.417229
[epoch 15, batch   999] avg loss: 0.407538
[epoch 15, batch  1099] avg loss: 0.431748
[epoch 15, batch  1199] avg loss: 0.405612
[epoch 15, batch  1299] avg loss: 0.404811
[epoch 15, batch  1399] avg loss: 0.418148
[epoch 15, batch  1499] avg loss: 0.405604
[epoch 15, batch  1599] avg loss: 0.416925
[epoch 15, batch  1699] avg loss: 0.405913
[epoch 15, batch  1799] avg loss: 0.417923
[epoch 15, batch  1899] avg loss: 0.396276
[epoch 15, batch  1999] avg loss: 0.397479
[epoch 15, batch  2099] avg loss: 0.400623
[epoch 15, batch  2199] avg loss: 0.400525
[epoch 15, batch  2299] avg loss: 0.422462
[epoch 15, batch  2399] avg loss: 0.411847
[epoch 16, batch    99] avg loss: 0.397350
[epoch 16, batch   199] avg loss: 0.391426
[epoch 16, batch   299] avg loss: 0.418974
[epoch 16, batch   399] avg loss: 0.405830
[epoch 16, batch   499] avg loss: 0.397061
[epoch 16, batch   599] avg loss: 0.404407
[epoch 16, batch   699] avg loss: 0.385471
[epoch 16, batch   799] avg loss: 0.406381
[epoch 16, batch   899] avg loss: 0.407328
[epoch 16, batch   999] avg loss: 0.405424
[epoch 16, batch  1099] avg loss: 0.414150
[epoch 16, batch  1199] avg loss: 0.410759
[epoch 16, batch  1299] avg loss: 0.426381
[epoch 16, batch  1399] avg loss: 0.405466
[epoch 16, batch  1499] avg loss: 0.407103
[epoch 16, batch  1599] avg loss: 0.399156
[epoch 16, batch  1699] avg loss: 0.400183
[epoch 16, batch  1799] avg loss: 0.421089
[epoch 16, batch  1899] avg loss: 0.402058
[epoch 16, batch  1999] avg loss: 0.400713
[epoch 16, batch  2099] avg loss: 0.417069
[epoch 16, batch  2199] avg loss: 0.405480
[epoch 16, batch  2299] avg loss: 0.403933
[epoch 16, batch  2399] avg loss: 0.403541
[epoch 17, batch    99] avg loss: 0.414011
[epoch 17, batch   199] avg loss: 0.403894
[epoch 17, batch   299] avg loss: 0.411541
[epoch 17, batch   399] avg loss: 0.404775
[epoch 17, batch   499] avg loss: 0.397381
[epoch 17, batch   599] avg loss: 0.397209
[epoch 17, batch   699] avg loss: 0.393116
[epoch 17, batch   799] avg loss: 0.403181
[epoch 17, batch   899] avg loss: 0.400096
[epoch 17, batch   999] avg loss: 0.398307
[epoch 17, batch  1099] avg loss: 0.397278
[epoch 17, batch  1199] avg loss: 0.395630
[epoch 17, batch  1299] avg loss: 0.403975
[epoch 17, batch  1399] avg loss: 0.397832
[epoch 17, batch  1499] avg loss: 0.393748
[epoch 17, batch  1599] avg loss: 0.408832
[epoch 17, batch  1699] avg loss: 0.394558
[epoch 17, batch  1799] avg loss: 0.401147
[epoch 17, batch  1899] avg loss: 0.409835
[epoch 17, batch  1999] avg loss: 0.402043
[epoch 17, batch  2099] avg loss: 0.407205
[epoch 17, batch  2199] avg loss: 0.397221
[epoch 17, batch  2299] avg loss: 0.397829
[epoch 17, batch  2399] avg loss: 0.400839
[epoch 18, batch    99] avg loss: 0.411608
[epoch 18, batch   199] avg loss: 0.402061
[epoch 18, batch   299] avg loss: 0.386870
[epoch 18, batch   399] avg loss: 0.398653
[epoch 18, batch   499] avg loss: 0.389171
[epoch 18, batch   599] avg loss: 0.394219
[epoch 18, batch   699] avg loss: 0.401302
[epoch 18, batch   799] avg loss: 0.398798
[epoch 18, batch   899] avg loss: 0.393346
[epoch 18, batch   999] avg loss: 0.414138
[epoch 18, batch  1099] avg loss: 0.406895
[epoch 18, batch  1199] avg loss: 0.402011
[epoch 18, batch  1299] avg loss: 0.394014
[epoch 18, batch  1399] avg loss: 0.397872
[epoch 18, batch  1499] avg loss: 0.407809
[epoch 18, batch  1599] avg loss: 0.392070
[epoch 18, batch  1699] avg loss: 0.411207
[epoch 18, batch  1799] avg loss: 0.417685
[epoch 18, batch  1899] avg loss: 0.442029
[epoch 18, batch  1999] avg loss: 0.404213
[epoch 18, batch  2099] avg loss: 0.409576
[epoch 18, batch  2199] avg loss: 0.400157
[epoch 18, batch  2299] avg loss: 0.396890
[epoch 18, batch  2399] avg loss: 0.409001
[epoch 19, batch    99] avg loss: 0.402611
[epoch 19, batch   199] avg loss: 0.390720
[epoch 19, batch   299] avg loss: 0.442306
[epoch 19, batch   399] avg loss: 0.401358
[epoch 19, batch   499] avg loss: 0.390660
[epoch 19, batch   599] avg loss: 0.382372
[epoch 19, batch   699] avg loss: 0.429763
[epoch 19, batch   799] avg loss: 0.398576
[epoch 19, batch   899] avg loss: 0.406412
[epoch 19, batch   999] avg loss: 0.393131
[epoch 19, batch  1099] avg loss: 0.395577
[epoch 19, batch  1199] avg loss: 0.401510
[epoch 19, batch  1299] avg loss: 0.430242
[epoch 19, batch  1399] avg loss: 0.403139
[epoch 19, batch  1499] avg loss: 0.399737
[epoch 19, batch  1599] avg loss: 0.405126
[epoch 19, batch  1699] avg loss: 0.387259
[epoch 19, batch  1799] avg loss: 0.400492
[epoch 19, batch  1899] avg loss: 0.401553
[epoch 19, batch  1999] avg loss: 0.394076
[epoch 19, batch  2099] avg loss: 0.398758
[epoch 19, batch  2199] avg loss: 0.385988
[epoch 19, batch  2299] avg loss: 0.400067
[epoch 19, batch  2399] avg loss: 0.391116
Model saved to model/20200502-075155.pth.
accuracy/TriangPrismIsosc : 0.62
n_examples/TriangPrismIsosc : 500.0
accuracy/parallelepiped : 0.432
n_examples/parallelepiped : 500.0
accuracy/sphere : 1.0
n_examples/sphere : 102.0
accuracy/wire : 0.975
n_examples/wire : 200.0
accuracy/avg_geom : 0.6321044546850998
loss/validation_geom : 0.7606947000858055
accuracy/Au : 0.0
n_examples/Au : 0.0
accuracy/SiN : 1.0
n_examples/SiN : 1302.0
accuracy/SiO2 : 0.0
n_examples/SiO2 : 0.0
accuracy/avg_mat : 1.0
loss/validation_mat : 0.003587860456695022
MSE/ShortestDim : 14.066566215315905
MAE/ShortestDim : 1.6525415985998104
MSE/MiddleDim : 29.76968200964862
MAE/MiddleDim : 3.968930480300739
MSE/LongDim : 161.72050042452716
MAE/LongDim : 7.075603842552173
MSE/log Area/Vol : 30.58272911106936
MAE/log Area/Vol : 3.319760737148115
loss/validation_dim : 236.13947776056105
loss/validation : 236.90376032110356
Metrics saved to model/20200502-075155_metrics.csv.
Parsed 2604 rows from data/sim_train_spectrum_SiO2.
Parsed 2604 rows from data/sim_train_labels_SiO2.
Parsed 9765 rows from data/gen_spectrum_SiO2_00-of-16.
Parsed 9765 rows from data/gen_labels_SiO2_00-of-16.
Parsed 9765 rows from data/gen_spectrum_SiO2_01-of-16.
Parsed 9765 rows from data/gen_labels_SiO2_01-of-16.
Parsed 9765 rows from data/gen_spectrum_SiO2_02-of-16.
Parsed 9765 rows from data/gen_labels_SiO2_02-of-16.
Parsed 9765 rows from data/gen_spectrum_SiO2_03-of-16.
Parsed 9765 rows from data/gen_labels_SiO2_03-of-16.
Parsed 9765 rows from data/gen_spectrum_SiO2_04-of-16.
Parsed 9765 rows from data/gen_labels_SiO2_04-of-16.
Parsed 9765 rows from data/gen_spectrum_SiO2_05-of-16.
Parsed 9765 rows from data/gen_labels_SiO2_05-of-16.
Parsed 9765 rows from data/gen_spectrum_SiO2_06-of-16.
Parsed 9765 rows from data/gen_labels_SiO2_06-of-16.
Parsed 9765 rows from data/gen_spectrum_SiO2_07-of-16.
Parsed 9765 rows from data/gen_labels_SiO2_07-of-16.
Parsed 9765 rows from data/gen_spectrum_SiO2_08-of-16.
Parsed 9765 rows from data/gen_labels_SiO2_08-of-16.
Parsed 9765 rows from data/gen_spectrum_SiO2_09-of-16.
Parsed 9765 rows from data/gen_labels_SiO2_09-of-16.
Parsed 9765 rows from data/gen_spectrum_SiO2_10-of-16.
Parsed 9765 rows from data/gen_labels_SiO2_10-of-16.
Parsed 9765 rows from data/gen_spectrum_SiO2_11-of-16.
Parsed 9765 rows from data/gen_labels_SiO2_11-of-16.
Parsed 9765 rows from data/gen_spectrum_SiO2_12-of-16.
Parsed 9765 rows from data/gen_labels_SiO2_12-of-16.
Parsed 9765 rows from data/gen_spectrum_SiO2_13-of-16.
Parsed 9765 rows from data/gen_labels_SiO2_13-of-16.
Parsed 9765 rows from data/gen_spectrum_SiO2_14-of-16.
Parsed 9765 rows from data/gen_labels_SiO2_14-of-16.
Parsed 9765 rows from data/gen_spectrum_SiO2_15-of-16.
Parsed 9765 rows from data/gen_labels_SiO2_15-of-16.
Parsed 1302 rows from data/sim_validation_spectrum_SiO2.
Parsed 1302 rows from data/sim_validation_labels_SiO2.
Logging training progress to tensorboard dir runs/alexnet-SiO2-lr_0.000500-trainsize_158844-05_02_2020_07:52-multistage.
[epoch 0, batch    99] avg loss: 0.330697
[epoch 0, batch   199] avg loss: 0.195737
[epoch 0, batch   299] avg loss: 0.183588
[epoch 0, batch   399] avg loss: 0.171903
[epoch 0, batch   499] avg loss: 0.156446
[epoch 0, batch   599] avg loss: 0.159345
[epoch 0, batch   699] avg loss: 0.136387
[epoch 0, batch   799] avg loss: 0.131974
[epoch 0, batch   899] avg loss: 0.140538
[epoch 0, batch   999] avg loss: 0.126673
[epoch 0, batch  1099] avg loss: 0.127520
[epoch 0, batch  1199] avg loss: 0.112563
[epoch 0, batch  1299] avg loss: 0.123403
[epoch 0, batch  1399] avg loss: 0.110615
[epoch 0, batch  1499] avg loss: 0.111309
[epoch 0, batch  1599] avg loss: 0.116306
[epoch 0, batch  1699] avg loss: 0.107770
[epoch 0, batch  1799] avg loss: 0.115264
[epoch 0, batch  1899] avg loss: 0.106063
[epoch 0, batch  1999] avg loss: 0.106126
[epoch 0, batch  2099] avg loss: 0.097559
[epoch 0, batch  2199] avg loss: 0.096375
[epoch 0, batch  2299] avg loss: 0.107593
[epoch 0, batch  2399] avg loss: 0.098939
[epoch 1, batch    99] avg loss: 0.080779
[epoch 1, batch   199] avg loss: 0.089632
[epoch 1, batch   299] avg loss: 0.097565
[epoch 1, batch   399] avg loss: 0.105464
[epoch 1, batch   499] avg loss: 0.099108
[epoch 1, batch   599] avg loss: 0.084860
[epoch 1, batch   699] avg loss: 0.090938
[epoch 1, batch   799] avg loss: 0.097814
[epoch 1, batch   899] avg loss: 0.095196
[epoch 1, batch   999] avg loss: 0.098654
[epoch 1, batch  1099] avg loss: 0.083760
[epoch 1, batch  1199] avg loss: 0.090336
[epoch 1, batch  1299] avg loss: 0.097925
[epoch 1, batch  1399] avg loss: 0.094043
[epoch 1, batch  1499] avg loss: 0.080689
[epoch 1, batch  1599] avg loss: 0.094651
[epoch 1, batch  1699] avg loss: 0.090412
[epoch 1, batch  1799] avg loss: 0.092626
[epoch 1, batch  1899] avg loss: 0.101117
[epoch 1, batch  1999] avg loss: 0.095369
[epoch 1, batch  2099] avg loss: 0.087059
[epoch 1, batch  2199] avg loss: 0.078925
[epoch 1, batch  2299] avg loss: 0.086681
[epoch 1, batch  2399] avg loss: 0.083424
[epoch 2, batch    99] avg loss: 0.093373
[epoch 2, batch   199] avg loss: 0.097166
[epoch 2, batch   299] avg loss: 0.085452
[epoch 2, batch   399] avg loss: 0.086381
[epoch 2, batch   499] avg loss: 0.072464
[epoch 2, batch   599] avg loss: 0.088713
[epoch 2, batch   699] avg loss: 0.071508
[epoch 2, batch   799] avg loss: 0.071304
[epoch 2, batch   899] avg loss: 0.077999
[epoch 2, batch   999] avg loss: 0.094704
[epoch 2, batch  1099] avg loss: 0.083759
[epoch 2, batch  1199] avg loss: 0.090148
[epoch 2, batch  1299] avg loss: 0.092230
[epoch 2, batch  1399] avg loss: 0.087085
[epoch 2, batch  1499] avg loss: 0.078234
[epoch 2, batch  1599] avg loss: 0.074517
[epoch 2, batch  1699] avg loss: 0.074640
[epoch 2, batch  1799] avg loss: 0.086395
[epoch 2, batch  1899] avg loss: 0.094371
[epoch 2, batch  1999] avg loss: 0.090991
[epoch 2, batch  2099] avg loss: 0.083210
[epoch 2, batch  2199] avg loss: 0.080884
[epoch 2, batch  2299] avg loss: 0.080731
[epoch 2, batch  2399] avg loss: 0.084485
[epoch 3, batch    99] avg loss: 0.078580
[epoch 3, batch   199] avg loss: 0.083881
[epoch 3, batch   299] avg loss: 0.090194
[epoch 3, batch   399] avg loss: 0.083748
[epoch 3, batch   499] avg loss: 0.076867
[epoch 3, batch   599] avg loss: 0.084864
[epoch 3, batch   699] avg loss: 0.090936
[epoch 3, batch   799] avg loss: 0.076469
[epoch 3, batch   899] avg loss: 0.081406
[epoch 3, batch   999] avg loss: 0.077366
[epoch 3, batch  1099] avg loss: 0.080265
[epoch 3, batch  1199] avg loss: 0.080259
[epoch 3, batch  1299] avg loss: 0.083933
[epoch 3, batch  1399] avg loss: 0.076781
[epoch 3, batch  1499] avg loss: 0.070917
[epoch 3, batch  1599] avg loss: 0.084511
[epoch 3, batch  1699] avg loss: 0.074606
[epoch 3, batch  1799] avg loss: 0.079285
[epoch 3, batch  1899] avg loss: 0.093737
[epoch 3, batch  1999] avg loss: 0.088098
[epoch 3, batch  2099] avg loss: 0.067364
[epoch 3, batch  2199] avg loss: 0.075920
[epoch 3, batch  2299] avg loss: 0.082697
[epoch 3, batch  2399] avg loss: 0.075496
[epoch 4, batch    99] avg loss: 0.075118
[epoch 4, batch   199] avg loss: 0.080088
[epoch 4, batch   299] avg loss: 0.078303
[epoch 4, batch   399] avg loss: 0.084037
[epoch 4, batch   499] avg loss: 0.076763
[epoch 4, batch   599] avg loss: 0.085323
[epoch 4, batch   699] avg loss: 0.080020
[epoch 4, batch   799] avg loss: 0.075354
[epoch 4, batch   899] avg loss: 0.080561
[epoch 4, batch   999] avg loss: 0.079683
[epoch 4, batch  1099] avg loss: 0.086653
[epoch 4, batch  1199] avg loss: 0.079410
[epoch 4, batch  1299] avg loss: 0.084986
[epoch 4, batch  1399] avg loss: 0.073618
[epoch 4, batch  1499] avg loss: 0.075162
[epoch 4, batch  1599] avg loss: 0.075816
[epoch 4, batch  1699] avg loss: 0.085043
[epoch 4, batch  1799] avg loss: 0.077338
[epoch 4, batch  1899] avg loss: 0.073561
[epoch 4, batch  1999] avg loss: 0.076572
[epoch 4, batch  2099] avg loss: 0.078045
[epoch 4, batch  2199] avg loss: 0.077759
[epoch 4, batch  2299] avg loss: 0.076500
[epoch 4, batch  2399] avg loss: 0.065134
[epoch 5, batch    99] avg loss: 0.082542
[epoch 5, batch   199] avg loss: 0.068551
[epoch 5, batch   299] avg loss: 0.071623
[epoch 5, batch   399] avg loss: 0.077188
[epoch 5, batch   499] avg loss: 0.081901
[epoch 5, batch   599] avg loss: 0.075869
[epoch 5, batch   699] avg loss: 0.071522
[epoch 5, batch   799] avg loss: 0.076730
[epoch 5, batch   899] avg loss: 0.080122
[epoch 5, batch   999] avg loss: 0.068514
[epoch 5, batch  1099] avg loss: 0.076095
[epoch 5, batch  1199] avg loss: 0.081994
[epoch 5, batch  1299] avg loss: 0.083595
[epoch 5, batch  1399] avg loss: 0.072986
[epoch 5, batch  1499] avg loss: 0.072927
[epoch 5, batch  1599] avg loss: 0.070217
[epoch 5, batch  1699] avg loss: 0.080210
[epoch 5, batch  1799] avg loss: 0.074023
[epoch 5, batch  1899] avg loss: 0.075922
[epoch 5, batch  1999] avg loss: 0.084886
[epoch 5, batch  2099] avg loss: 0.081276
[epoch 5, batch  2199] avg loss: 0.071313
[epoch 5, batch  2299] avg loss: 0.089924
[epoch 5, batch  2399] avg loss: 0.073291
[epoch 6, batch    99] avg loss: 0.073773
[epoch 6, batch   199] avg loss: 0.069454
[epoch 6, batch   299] avg loss: 0.075871
[epoch 6, batch   399] avg loss: 0.079964
[epoch 6, batch   499] avg loss: 0.071448
[epoch 6, batch   599] avg loss: 0.070237
[epoch 6, batch   699] avg loss: 0.066787
[epoch 6, batch   799] avg loss: 0.078809
[epoch 6, batch   899] avg loss: 0.078199
[epoch 6, batch   999] avg loss: 0.075319
[epoch 6, batch  1099] avg loss: 0.071012
[epoch 6, batch  1199] avg loss: 0.076858
[epoch 6, batch  1299] avg loss: 0.078744
[epoch 6, batch  1399] avg loss: 0.077221
[epoch 6, batch  1499] avg loss: 0.072267
[epoch 6, batch  1599] avg loss: 0.084314
[epoch 6, batch  1699] avg loss: 0.069047
[epoch 6, batch  1799] avg loss: 0.080692
[epoch 6, batch  1899] avg loss: 0.078485
[epoch 6, batch  1999] avg loss: 0.078018
[epoch 6, batch  2099] avg loss: 0.074530
[epoch 6, batch  2199] avg loss: 0.083131
[epoch 6, batch  2299] avg loss: 0.075197
[epoch 6, batch  2399] avg loss: 0.077237
[epoch 7, batch    99] avg loss: 0.070155
[epoch 7, batch   199] avg loss: 0.071202
[epoch 7, batch   299] avg loss: 0.085289
[epoch 7, batch   399] avg loss: 0.071549
[epoch 7, batch   499] avg loss: 0.079344
[epoch 7, batch   599] avg loss: 0.069810
[epoch 7, batch   699] avg loss: 0.070619
[epoch 7, batch   799] avg loss: 0.079861
[epoch 7, batch   899] avg loss: 0.077656
[epoch 7, batch   999] avg loss: 0.073665
[epoch 7, batch  1099] avg loss: 0.073574
[epoch 7, batch  1199] avg loss: 0.072883
[epoch 7, batch  1299] avg loss: 0.068324
[epoch 7, batch  1399] avg loss: 0.069055
[epoch 7, batch  1499] avg loss: 0.073262
[epoch 7, batch  1599] avg loss: 0.079531
[epoch 7, batch  1699] avg loss: 0.080738
[epoch 7, batch  1799] avg loss: 0.069000
[epoch 7, batch  1899] avg loss: 0.086305
[epoch 7, batch  1999] avg loss: 0.077770
[epoch 7, batch  2099] avg loss: 0.073119
[epoch 7, batch  2199] avg loss: 0.076247
[epoch 7, batch  2299] avg loss: 0.074092
[epoch 7, batch  2399] avg loss: 0.072604
[epoch 8, batch    99] avg loss: 0.076601
[epoch 8, batch   199] avg loss: 0.068357
[epoch 8, batch   299] avg loss: 0.083322
[epoch 8, batch   399] avg loss: 0.072523
[epoch 8, batch   499] avg loss: 0.075618
[epoch 8, batch   599] avg loss: 0.065990
[epoch 8, batch   699] avg loss: 0.069204
[epoch 8, batch   799] avg loss: 0.069751
[epoch 8, batch   899] avg loss: 0.074015
[epoch 8, batch   999] avg loss: 0.073254
[epoch 8, batch  1099] avg loss: 0.088418
[epoch 8, batch  1199] avg loss: 0.065898
[epoch 8, batch  1299] avg loss: 0.074645
[epoch 8, batch  1399] avg loss: 0.071800
[epoch 8, batch  1499] avg loss: 0.073879
[epoch 8, batch  1599] avg loss: 0.080813
[epoch 8, batch  1699] avg loss: 0.072486
[epoch 8, batch  1799] avg loss: 0.060964
[epoch 8, batch  1899] avg loss: 0.084033
[epoch 8, batch  1999] avg loss: 0.070014
[epoch 8, batch  2099] avg loss: 0.074290
[epoch 8, batch  2199] avg loss: 0.078721
[epoch 8, batch  2299] avg loss: 0.070337
[epoch 8, batch  2399] avg loss: 0.070336
[epoch 9, batch    99] avg loss: 0.082684
[epoch 9, batch   199] avg loss: 0.074803
[epoch 9, batch   299] avg loss: 0.075136
[epoch 9, batch   399] avg loss: 0.069252
[epoch 9, batch   499] avg loss: 0.072002
[epoch 9, batch   599] avg loss: 0.064562
[epoch 9, batch   699] avg loss: 0.077943
[epoch 9, batch   799] avg loss: 0.082186
[epoch 9, batch   899] avg loss: 0.063426
[epoch 9, batch   999] avg loss: 0.069388
[epoch 9, batch  1099] avg loss: 0.073865
[epoch 9, batch  1199] avg loss: 0.068104
[epoch 9, batch  1299] avg loss: 0.071354
[epoch 9, batch  1399] avg loss: 0.073857
[epoch 9, batch  1499] avg loss: 0.081950
[epoch 9, batch  1599] avg loss: 0.072542
[epoch 9, batch  1699] avg loss: 0.078858
[epoch 9, batch  1799] avg loss: 0.068360
[epoch 9, batch  1899] avg loss: 0.067722
[epoch 9, batch  1999] avg loss: 0.072246
[epoch 9, batch  2099] avg loss: 0.062686
[epoch 9, batch  2199] avg loss: 0.068899
[epoch 9, batch  2299] avg loss: 0.076057
[epoch 9, batch  2399] avg loss: 0.075297
[epoch 10, batch    99] avg loss: 0.068852
[epoch 10, batch   199] avg loss: 0.070508
[epoch 10, batch   299] avg loss: 0.070850
[epoch 10, batch   399] avg loss: 0.084552
[epoch 10, batch   499] avg loss: 0.066446
[epoch 10, batch   599] avg loss: 0.074932
[epoch 10, batch   699] avg loss: 0.065926
[epoch 10, batch   799] avg loss: 0.082704
[epoch 10, batch   899] avg loss: 0.074441
[epoch 10, batch   999] avg loss: 0.074227
[epoch 10, batch  1099] avg loss: 0.075762
[epoch 10, batch  1199] avg loss: 0.066136
[epoch 10, batch  1299] avg loss: 0.069695
[epoch 10, batch  1399] avg loss: 0.069475
[epoch 10, batch  1499] avg loss: 0.066781
[epoch 10, batch  1599] avg loss: 0.073148
[epoch 10, batch  1699] avg loss: 0.072042
[epoch 10, batch  1799] avg loss: 0.071481
[epoch 10, batch  1899] avg loss: 0.071696
[epoch 10, batch  1999] avg loss: 0.063750
[epoch 10, batch  2099] avg loss: 0.068497
[epoch 10, batch  2199] avg loss: 0.070999
[epoch 10, batch  2299] avg loss: 0.074393
[epoch 10, batch  2399] avg loss: 0.065159
[epoch 11, batch    99] avg loss: 0.085106
[epoch 11, batch   199] avg loss: 0.064412
[epoch 11, batch   299] avg loss: 0.072138
[epoch 11, batch   399] avg loss: 0.067222
[epoch 11, batch   499] avg loss: 0.074700
[epoch 11, batch   599] avg loss: 0.062891
[epoch 11, batch   699] avg loss: 0.066778
[epoch 11, batch   799] avg loss: 0.067685
[epoch 11, batch   899] avg loss: 0.074044
[epoch 11, batch   999] avg loss: 0.066903
[epoch 11, batch  1099] avg loss: 0.068853
[epoch 11, batch  1199] avg loss: 0.062622
[epoch 11, batch  1299] avg loss: 0.071699
[epoch 11, batch  1399] avg loss: 0.066789
[epoch 11, batch  1499] avg loss: 0.077377
[epoch 11, batch  1599] avg loss: 0.062299
[epoch 11, batch  1699] avg loss: 0.073490
[epoch 11, batch  1799] avg loss: 0.074971
[epoch 11, batch  1899] avg loss: 0.070873
[epoch 11, batch  1999] avg loss: 0.073118
[epoch 11, batch  2099] avg loss: 0.076324
[epoch 11, batch  2199] avg loss: 0.066580
[epoch 11, batch  2299] avg loss: 0.074393
[epoch 11, batch  2399] avg loss: 0.078957
[epoch 12, batch    99] avg loss: 0.061842
[epoch 12, batch   199] avg loss: 0.064913
[epoch 12, batch   299] avg loss: 0.066263
[epoch 12, batch   399] avg loss: 0.079919
[epoch 12, batch   499] avg loss: 0.077381
[epoch 12, batch   599] avg loss: 0.078688
[epoch 12, batch   699] avg loss: 0.068724
[epoch 12, batch   799] avg loss: 0.074186
[epoch 12, batch   899] avg loss: 0.065720
[epoch 12, batch   999] avg loss: 0.073594
[epoch 12, batch  1099] avg loss: 0.071186
[epoch 12, batch  1199] avg loss: 0.073523
[epoch 12, batch  1299] avg loss: 0.064817
[epoch 12, batch  1399] avg loss: 0.079864
[epoch 12, batch  1499] avg loss: 0.063432
[epoch 12, batch  1599] avg loss: 0.064248
[epoch 12, batch  1699] avg loss: 0.069462
[epoch 12, batch  1799] avg loss: 0.068346
[epoch 12, batch  1899] avg loss: 0.069158
[epoch 12, batch  1999] avg loss: 0.075336
[epoch 12, batch  2099] avg loss: 0.075722
[epoch 12, batch  2199] avg loss: 0.067374
[epoch 12, batch  2299] avg loss: 0.069343
[epoch 12, batch  2399] avg loss: 0.075107
[epoch 13, batch    99] avg loss: 0.069576
[epoch 13, batch   199] avg loss: 0.074286
[epoch 13, batch   299] avg loss: 0.075545
[epoch 13, batch   399] avg loss: 0.070730
[epoch 13, batch   499] avg loss: 0.065636
[epoch 13, batch   599] avg loss: 0.074562
[epoch 13, batch   699] avg loss: 0.076062
[epoch 13, batch   799] avg loss: 0.068431
[epoch 13, batch   899] avg loss: 0.078815
[epoch 13, batch   999] avg loss: 0.072262
[epoch 13, batch  1099] avg loss: 0.063463
[epoch 13, batch  1199] avg loss: 0.067879
[epoch 13, batch  1299] avg loss: 0.069834
[epoch 13, batch  1399] avg loss: 0.071236
[epoch 13, batch  1499] avg loss: 0.060908
[epoch 13, batch  1599] avg loss: 0.072112
[epoch 13, batch  1699] avg loss: 0.074054
[epoch 13, batch  1799] avg loss: 0.076304
[epoch 13, batch  1899] avg loss: 0.065583
[epoch 13, batch  1999] avg loss: 0.068113
[epoch 13, batch  2099] avg loss: 0.066503
[epoch 13, batch  2199] avg loss: 0.069928
[epoch 13, batch  2299] avg loss: 0.059023
[epoch 13, batch  2399] avg loss: 0.064216
[epoch 14, batch    99] avg loss: 0.071395
[epoch 14, batch   199] avg loss: 0.083748
[epoch 14, batch   299] avg loss: 0.070151
[epoch 14, batch   399] avg loss: 0.067244
[epoch 14, batch   499] avg loss: 0.074533
[epoch 14, batch   599] avg loss: 0.062094
[epoch 14, batch   699] avg loss: 0.063872
[epoch 14, batch   799] avg loss: 0.063839
[epoch 14, batch   899] avg loss: 0.070675
[epoch 14, batch   999] avg loss: 0.063500
[epoch 14, batch  1099] avg loss: 0.078992
[epoch 14, batch  1199] avg loss: 0.068898
[epoch 14, batch  1299] avg loss: 0.065845
[epoch 14, batch  1399] avg loss: 0.070864
[epoch 14, batch  1499] avg loss: 0.059331
[epoch 14, batch  1599] avg loss: 0.062074
[epoch 14, batch  1699] avg loss: 0.079585
[epoch 14, batch  1799] avg loss: 0.073497
[epoch 14, batch  1899] avg loss: 0.055804
[epoch 14, batch  1999] avg loss: 0.064016
[epoch 14, batch  2099] avg loss: 0.081667
[epoch 14, batch  2199] avg loss: 0.072979
[epoch 14, batch  2299] avg loss: 0.078300
[epoch 14, batch  2399] avg loss: 0.064495
[epoch 15, batch    99] avg loss: 0.067479
[epoch 15, batch   199] avg loss: 0.067885
[epoch 15, batch   299] avg loss: 0.069282
[epoch 15, batch   399] avg loss: 0.067544
[epoch 15, batch   499] avg loss: 0.070452
[epoch 15, batch   599] avg loss: 0.060313
[epoch 15, batch   699] avg loss: 0.070569
[epoch 15, batch   799] avg loss: 0.072895
[epoch 15, batch   899] avg loss: 0.058171
[epoch 15, batch   999] avg loss: 0.073010
[epoch 15, batch  1099] avg loss: 0.077183
[epoch 15, batch  1199] avg loss: 0.072036
[epoch 15, batch  1299] avg loss: 0.065869
[epoch 15, batch  1399] avg loss: 0.065262
[epoch 15, batch  1499] avg loss: 0.070694
[epoch 15, batch  1599] avg loss: 0.063186
[epoch 15, batch  1699] avg loss: 0.076283
[epoch 15, batch  1799] avg loss: 0.059886
[epoch 15, batch  1899] avg loss: 0.069103
[epoch 15, batch  1999] avg loss: 0.056150
[epoch 15, batch  2099] avg loss: 0.073156
[epoch 15, batch  2199] avg loss: 0.068928
[epoch 15, batch  2299] avg loss: 0.066727
[epoch 15, batch  2399] avg loss: 0.078529
[epoch 16, batch    99] avg loss: 0.070482
[epoch 16, batch   199] avg loss: 0.064630
[epoch 16, batch   299] avg loss: 0.060852
[epoch 16, batch   399] avg loss: 0.076487
[epoch 16, batch   499] avg loss: 0.064900
[epoch 16, batch   599] avg loss: 0.072573
[epoch 16, batch   699] avg loss: 0.064710
[epoch 16, batch   799] avg loss: 0.062533
[epoch 16, batch   899] avg loss: 0.070116
[epoch 16, batch   999] avg loss: 0.063237
[epoch 16, batch  1099] avg loss: 0.055352
[epoch 16, batch  1199] avg loss: 0.058411
[epoch 16, batch  1299] avg loss: 0.072121
[epoch 16, batch  1399] avg loss: 0.068430
[epoch 16, batch  1499] avg loss: 0.064954
[epoch 16, batch  1599] avg loss: 0.070007
[epoch 16, batch  1699] avg loss: 0.075399
[epoch 16, batch  1799] avg loss: 0.080425
[epoch 16, batch  1899] avg loss: 0.071531
[epoch 16, batch  1999] avg loss: 0.072259
[epoch 16, batch  2099] avg loss: 0.068403
[epoch 16, batch  2199] avg loss: 0.069489
[epoch 16, batch  2299] avg loss: 0.074404
[epoch 16, batch  2399] avg loss: 0.063975
[epoch 17, batch    99] avg loss: 0.058435
[epoch 17, batch   199] avg loss: 0.084161
[epoch 17, batch   299] avg loss: 0.070080
[epoch 17, batch   399] avg loss: 0.066688
[epoch 17, batch   499] avg loss: 0.066526
[epoch 17, batch   599] avg loss: 0.059472
[epoch 17, batch   699] avg loss: 0.070287
[epoch 17, batch   799] avg loss: 0.067129
[epoch 17, batch   899] avg loss: 0.065924
[epoch 17, batch   999] avg loss: 0.070898
[epoch 17, batch  1099] avg loss: 0.070574
[epoch 17, batch  1199] avg loss: 0.066025
[epoch 17, batch  1299] avg loss: 0.066876
[epoch 17, batch  1399] avg loss: 0.068868
[epoch 17, batch  1499] avg loss: 0.071486
[epoch 17, batch  1599] avg loss: 0.065133
[epoch 17, batch  1699] avg loss: 0.063501
[epoch 17, batch  1799] avg loss: 0.074792
[epoch 17, batch  1899] avg loss: 0.067824
[epoch 17, batch  1999] avg loss: 0.066566
[epoch 17, batch  2099] avg loss: 0.073388
[epoch 17, batch  2199] avg loss: 0.076634
[epoch 17, batch  2299] avg loss: 0.075689
[epoch 17, batch  2399] avg loss: 0.063262
[epoch 18, batch    99] avg loss: 0.068525
[epoch 18, batch   199] avg loss: 0.070514
[epoch 18, batch   299] avg loss: 0.064270
[epoch 18, batch   399] avg loss: 0.074054
[epoch 18, batch   499] avg loss: 0.064914
[epoch 18, batch   599] avg loss: 0.067430
[epoch 18, batch   699] avg loss: 0.069177
[epoch 18, batch   799] avg loss: 0.060290
[epoch 18, batch   899] avg loss: 0.066920
[epoch 18, batch   999] avg loss: 0.066680
[epoch 18, batch  1099] avg loss: 0.065836
[epoch 18, batch  1199] avg loss: 0.061428
[epoch 18, batch  1299] avg loss: 0.079060
[epoch 18, batch  1399] avg loss: 0.060013
[epoch 18, batch  1499] avg loss: 0.072978
[epoch 18, batch  1599] avg loss: 0.066208
[epoch 18, batch  1699] avg loss: 0.059096
[epoch 18, batch  1799] avg loss: 0.077071
[epoch 18, batch  1899] avg loss: 0.062800
[epoch 18, batch  1999] avg loss: 0.071909
[epoch 18, batch  2099] avg loss: 0.065191
[epoch 18, batch  2199] avg loss: 0.066990
[epoch 18, batch  2299] avg loss: 0.064324
[epoch 18, batch  2399] avg loss: 0.077733
[epoch 19, batch    99] avg loss: 0.061747
[epoch 19, batch   199] avg loss: 0.067170
[epoch 19, batch   299] avg loss: 0.074225
[epoch 19, batch   399] avg loss: 0.065333
[epoch 19, batch   499] avg loss: 0.066402
[epoch 19, batch   599] avg loss: 0.062742
[epoch 19, batch   699] avg loss: 0.075136
[epoch 19, batch   799] avg loss: 0.067123
[epoch 19, batch   899] avg loss: 0.064150
[epoch 19, batch   999] avg loss: 0.072463
[epoch 19, batch  1099] avg loss: 0.069487
[epoch 19, batch  1199] avg loss: 0.068621
[epoch 19, batch  1299] avg loss: 0.057788
[epoch 19, batch  1399] avg loss: 0.064519
[epoch 19, batch  1499] avg loss: 0.062772
[epoch 19, batch  1599] avg loss: 0.065467
[epoch 19, batch  1699] avg loss: 0.062635
[epoch 19, batch  1799] avg loss: 0.061769
[epoch 19, batch  1899] avg loss: 0.064329
[epoch 19, batch  1999] avg loss: 0.068311
[epoch 19, batch  2099] avg loss: 0.065380
[epoch 19, batch  2199] avg loss: 0.075617
[epoch 19, batch  2299] avg loss: 0.076375
[epoch 19, batch  2399] avg loss: 0.071659
Model saved to model/20200502-080848.pth.
accuracy/TriangPrismIsosc : 0.214
n_examples/TriangPrismIsosc : 500.0
accuracy/parallelepiped : 0.474
n_examples/parallelepiped : 500.0
accuracy/sphere : 0.049019607843137254
n_examples/sphere : 102.0
accuracy/wire : 0.245
n_examples/wire : 200.0
accuracy/avg_geom : 0.30568356374807987
loss/validation_geom : 1.4125402306997648
accuracy/Au : 0.0
n_examples/Au : 0.0
accuracy/SiN : 0.0
n_examples/SiN : 0.0
accuracy/SiO2 : 1.0
n_examples/SiO2 : 1302.0
accuracy/avg_mat : 1.0
loss/validation_mat : 9.155858866770653e-11
MSE/ShortestDim : 0.13345832612291092
MAE/ShortestDim : 0.20962320860996042
MSE/MiddleDim : 0.46172968345120574
MAE/MiddleDim : 0.4168654608836372
MSE/LongDim : 16.03364436710668
MAE/LongDim : 2.0229078577044555
MSE/log Area/Vol : 0.09938371813242336
MAE/log Area/Vol : 0.24291708429105088
loss/validation_dim : 16.72821609481322
loss/validation : 18.140756325604542
Metrics saved to model/20200502-080848_metrics.csv.
[epoch 0, batch    99] avg loss: 0.996026
[epoch 0, batch   199] avg loss: 0.724193
[epoch 0, batch   299] avg loss: 0.634466
[epoch 0, batch   399] avg loss: 0.578976
[epoch 0, batch   499] avg loss: 0.569487
[epoch 0, batch   599] avg loss: 0.543543
[epoch 0, batch   699] avg loss: 0.544345
[epoch 0, batch   799] avg loss: 0.534039
[epoch 0, batch   899] avg loss: 0.526393
[epoch 0, batch   999] avg loss: 0.519423
[epoch 0, batch  1099] avg loss: 0.518328
[epoch 0, batch  1199] avg loss: 0.502157
[epoch 0, batch  1299] avg loss: 0.511391
[epoch 0, batch  1399] avg loss: 0.505847
[epoch 0, batch  1499] avg loss: 0.505257
[epoch 0, batch  1599] avg loss: 0.508566
[epoch 0, batch  1699] avg loss: 0.489829
[epoch 0, batch  1799] avg loss: 0.497375
[epoch 0, batch  1899] avg loss: 0.485693
[epoch 0, batch  1999] avg loss: 0.495753
[epoch 0, batch  2099] avg loss: 0.495238
[epoch 0, batch  2199] avg loss: 0.491777
[epoch 0, batch  2299] avg loss: 0.490997
[epoch 0, batch  2399] avg loss: 0.484829
[epoch 1, batch    99] avg loss: 0.495205
[epoch 1, batch   199] avg loss: 0.484316
[epoch 1, batch   299] avg loss: 0.470696
[epoch 1, batch   399] avg loss: 0.474755
[epoch 1, batch   499] avg loss: 0.473829
[epoch 1, batch   599] avg loss: 0.471304
[epoch 1, batch   699] avg loss: 0.464863
[epoch 1, batch   799] avg loss: 0.481093
[epoch 1, batch   899] avg loss: 0.468922
[epoch 1, batch   999] avg loss: 0.462955
[epoch 1, batch  1099] avg loss: 0.464561
[epoch 1, batch  1199] avg loss: 0.473697
[epoch 1, batch  1299] avg loss: 0.462155
[epoch 1, batch  1399] avg loss: 0.468156
[epoch 1, batch  1499] avg loss: 0.476196
[epoch 1, batch  1599] avg loss: 0.473809
[epoch 1, batch  1699] avg loss: 0.461919
[epoch 1, batch  1799] avg loss: 0.454822
[epoch 1, batch  1899] avg loss: 0.450869
[epoch 1, batch  1999] avg loss: 0.463529
[epoch 1, batch  2099] avg loss: 0.465458
[epoch 1, batch  2199] avg loss: 0.450951
[epoch 1, batch  2299] avg loss: 0.463295
[epoch 1, batch  2399] avg loss: 0.464926
[epoch 2, batch    99] avg loss: 0.457496
[epoch 2, batch   199] avg loss: 0.453694
[epoch 2, batch   299] avg loss: 0.455154
[epoch 2, batch   399] avg loss: 0.451729
[epoch 2, batch   499] avg loss: 0.459957
[epoch 2, batch   599] avg loss: 0.458913
[epoch 2, batch   699] avg loss: 0.448148
[epoch 2, batch   799] avg loss: 0.442783
[epoch 2, batch   899] avg loss: 0.459775
[epoch 2, batch   999] avg loss: 0.444139
[epoch 2, batch  1099] avg loss: 0.446144
[epoch 2, batch  1199] avg loss: 0.458796
[epoch 2, batch  1299] avg loss: 0.442188
[epoch 2, batch  1399] avg loss: 0.439511
[epoch 2, batch  1499] avg loss: 0.446825
[epoch 2, batch  1599] avg loss: 0.446303
[epoch 2, batch  1699] avg loss: 0.455500
[epoch 2, batch  1799] avg loss: 0.448040
[epoch 2, batch  1899] avg loss: 0.437023
[epoch 2, batch  1999] avg loss: 0.436318
[epoch 2, batch  2099] avg loss: 0.429972
[epoch 2, batch  2199] avg loss: 0.441208
[epoch 2, batch  2299] avg loss: 0.438355
[epoch 2, batch  2399] avg loss: 0.450852
[epoch 3, batch    99] avg loss: 0.441268
[epoch 3, batch   199] avg loss: 0.435072
[epoch 3, batch   299] avg loss: 0.442167
[epoch 3, batch   399] avg loss: 0.429334
[epoch 3, batch   499] avg loss: 0.438603
[epoch 3, batch   599] avg loss: 0.431576
[epoch 3, batch   699] avg loss: 0.442512
[epoch 3, batch   799] avg loss: 0.426227
[epoch 3, batch   899] avg loss: 0.424950
[epoch 3, batch   999] avg loss: 0.419745
[epoch 3, batch  1099] avg loss: 0.416851
[epoch 3, batch  1199] avg loss: 0.428879
[epoch 3, batch  1299] avg loss: 0.431822
[epoch 3, batch  1399] avg loss: 0.431808
[epoch 3, batch  1499] avg loss: 0.423849
[epoch 3, batch  1599] avg loss: 0.439388
[epoch 3, batch  1699] avg loss: 0.423328
[epoch 3, batch  1799] avg loss: 0.439635
[epoch 3, batch  1899] avg loss: 0.432762
[epoch 3, batch  1999] avg loss: 0.422499
[epoch 3, batch  2099] avg loss: 0.411470
[epoch 3, batch  2199] avg loss: 0.438292
[epoch 3, batch  2299] avg loss: 0.422034
[epoch 3, batch  2399] avg loss: 0.415586
[epoch 4, batch    99] avg loss: 0.430114
[epoch 4, batch   199] avg loss: 0.429369
[epoch 4, batch   299] avg loss: 0.428157
[epoch 4, batch   399] avg loss: 0.395563
[epoch 4, batch   499] avg loss: 0.417708
[epoch 4, batch   599] avg loss: 0.429370
[epoch 4, batch   699] avg loss: 0.436799
[epoch 4, batch   799] avg loss: 0.425217
[epoch 4, batch   899] avg loss: 0.427311
[epoch 4, batch   999] avg loss: 0.417444
[epoch 4, batch  1099] avg loss: 0.406189
[epoch 4, batch  1199] avg loss: 0.413186
[epoch 4, batch  1299] avg loss: 0.427453
[epoch 4, batch  1399] avg loss: 0.409027
[epoch 4, batch  1499] avg loss: 0.421249
[epoch 4, batch  1599] avg loss: 0.437642
[epoch 4, batch  1699] avg loss: 0.423205
[epoch 4, batch  1799] avg loss: 0.410239
[epoch 4, batch  1899] avg loss: 0.419428
[epoch 4, batch  1999] avg loss: 0.420484
[epoch 4, batch  2099] avg loss: 0.421713
[epoch 4, batch  2199] avg loss: 0.409284
[epoch 4, batch  2299] avg loss: 0.417407
[epoch 4, batch  2399] avg loss: 0.423595
[epoch 5, batch    99] avg loss: 0.403882
[epoch 5, batch   199] avg loss: 0.412676
[epoch 5, batch   299] avg loss: 0.411794
[epoch 5, batch   399] avg loss: 0.414112
[epoch 5, batch   499] avg loss: 0.412452
[epoch 5, batch   599] avg loss: 0.419801
[epoch 5, batch   699] avg loss: 0.413193
[epoch 5, batch   799] avg loss: 0.402843
[epoch 5, batch   899] avg loss: 0.402762
[epoch 5, batch   999] avg loss: 0.423722
[epoch 5, batch  1099] avg loss: 0.411353
[epoch 5, batch  1199] avg loss: 0.411102
[epoch 5, batch  1299] avg loss: 0.406848
[epoch 5, batch  1399] avg loss: 0.425801
[epoch 5, batch  1499] avg loss: 0.410577
[epoch 5, batch  1599] avg loss: 0.418455
[epoch 5, batch  1699] avg loss: 0.416905
[epoch 5, batch  1799] avg loss: 0.400810
[epoch 5, batch  1899] avg loss: 0.413934
[epoch 5, batch  1999] avg loss: 0.411294
[epoch 5, batch  2099] avg loss: 0.414393
[epoch 5, batch  2199] avg loss: 0.399380
[epoch 5, batch  2299] avg loss: 0.410671
[epoch 5, batch  2399] avg loss: 0.410013
[epoch 6, batch    99] avg loss: 0.411562
[epoch 6, batch   199] avg loss: 0.403620
[epoch 6, batch   299] avg loss: 0.401419
[epoch 6, batch   399] avg loss: 0.412679
[epoch 6, batch   499] avg loss: 0.405956
[epoch 6, batch   599] avg loss: 0.393815
[epoch 6, batch   699] avg loss: 0.414657
[epoch 6, batch   799] avg loss: 0.395876
[epoch 6, batch   899] avg loss: 0.402344
[epoch 6, batch   999] avg loss: 0.412158
[epoch 6, batch  1099] avg loss: 0.400142
[epoch 6, batch  1199] avg loss: 0.391087
[epoch 6, batch  1299] avg loss: 0.400775
[epoch 6, batch  1399] avg loss: 0.401715
[epoch 6, batch  1499] avg loss: 0.417216
[epoch 6, batch  1599] avg loss: 0.421383
[epoch 6, batch  1699] avg loss: 0.404973
[epoch 6, batch  1799] avg loss: 0.406959
[epoch 6, batch  1899] avg loss: 0.389433
[epoch 6, batch  1999] avg loss: 0.402227
[epoch 6, batch  2099] avg loss: 0.400689
[epoch 6, batch  2199] avg loss: 0.397707
[epoch 6, batch  2299] avg loss: 0.394936
[epoch 6, batch  2399] avg loss: 0.388794
[epoch 7, batch    99] avg loss: 0.404139
[epoch 7, batch   199] avg loss: 0.392475
[epoch 7, batch   299] avg loss: 0.386322
[epoch 7, batch   399] avg loss: 0.388604
[epoch 7, batch   499] avg loss: 0.384766
[epoch 7, batch   599] avg loss: 0.383428
[epoch 7, batch   699] avg loss: 0.403752
[epoch 7, batch   799] avg loss: 0.395684
[epoch 7, batch   899] avg loss: 0.392164
[epoch 7, batch   999] avg loss: 0.389708
[epoch 7, batch  1099] avg loss: 0.398817
[epoch 7, batch  1199] avg loss: 0.388971
[epoch 7, batch  1299] avg loss: 0.399843
[epoch 7, batch  1399] avg loss: 0.383743
[epoch 7, batch  1499] avg loss: 0.396278
[epoch 7, batch  1599] avg loss: 0.403430
[epoch 7, batch  1699] avg loss: 0.399693
[epoch 7, batch  1799] avg loss: 0.384804
[epoch 7, batch  1899] avg loss: 0.389150
[epoch 7, batch  1999] avg loss: 0.386741
[epoch 7, batch  2099] avg loss: 0.407729
[epoch 7, batch  2199] avg loss: 0.398196
[epoch 7, batch  2299] avg loss: 0.398670
[epoch 7, batch  2399] avg loss: 0.396276
[epoch 8, batch    99] avg loss: 0.401665
[epoch 8, batch   199] avg loss: 0.385175
[epoch 8, batch   299] avg loss: 0.380158
[epoch 8, batch   399] avg loss: 0.376249
[epoch 8, batch   499] avg loss: 0.385796
[epoch 8, batch   599] avg loss: 0.384895
[epoch 8, batch   699] avg loss: 0.383845
[epoch 8, batch   799] avg loss: 0.406867
[epoch 8, batch   899] avg loss: 0.379017
[epoch 8, batch   999] avg loss: 0.383552
[epoch 8, batch  1099] avg loss: 0.385670
[epoch 8, batch  1199] avg loss: 0.379504
[epoch 8, batch  1299] avg loss: 0.395649
[epoch 8, batch  1399] avg loss: 0.384911
[epoch 8, batch  1499] avg loss: 0.381598
[epoch 8, batch  1599] avg loss: 0.371687
[epoch 8, batch  1699] avg loss: 0.376841
[epoch 8, batch  1799] avg loss: 0.403978
[epoch 8, batch  1899] avg loss: 0.383184
[epoch 8, batch  1999] avg loss: 0.381075
[epoch 8, batch  2099] avg loss: 0.381487
[epoch 8, batch  2199] avg loss: 0.380927
[epoch 8, batch  2299] avg loss: 0.385270
[epoch 8, batch  2399] avg loss: 0.389259
[epoch 9, batch    99] avg loss: 0.375891
[epoch 9, batch   199] avg loss: 0.387766
[epoch 9, batch   299] avg loss: 0.377511
[epoch 9, batch   399] avg loss: 0.390028
[epoch 9, batch   499] avg loss: 0.373979
[epoch 9, batch   599] avg loss: 0.391503
[epoch 9, batch   699] avg loss: 0.361882
[epoch 9, batch   799] avg loss: 0.373273
[epoch 9, batch   899] avg loss: 0.401294
[epoch 9, batch   999] avg loss: 0.368355
[epoch 9, batch  1099] avg loss: 0.378892
[epoch 9, batch  1199] avg loss: 0.373688
[epoch 9, batch  1299] avg loss: 0.373721
[epoch 9, batch  1399] avg loss: 0.386431
[epoch 9, batch  1499] avg loss: 0.390144
[epoch 9, batch  1599] avg loss: 0.368266
[epoch 9, batch  1699] avg loss: 0.378716
[epoch 9, batch  1799] avg loss: 0.375931
[epoch 9, batch  1899] avg loss: 0.380878
[epoch 9, batch  1999] avg loss: 0.367317
[epoch 9, batch  2099] avg loss: 0.367610
[epoch 9, batch  2199] avg loss: 0.377158
[epoch 9, batch  2299] avg loss: 0.400995
[epoch 9, batch  2399] avg loss: 0.377745
[epoch 10, batch    99] avg loss: 0.376877
[epoch 10, batch   199] avg loss: 0.358015
[epoch 10, batch   299] avg loss: 0.363159
[epoch 10, batch   399] avg loss: 0.382399
[epoch 10, batch   499] avg loss: 0.368288
[epoch 10, batch   599] avg loss: 0.376983
[epoch 10, batch   699] avg loss: 0.376098
[epoch 10, batch   799] avg loss: 0.370920
[epoch 10, batch   899] avg loss: 0.371390
[epoch 10, batch   999] avg loss: 0.373413
[epoch 10, batch  1099] avg loss: 0.368556
[epoch 10, batch  1199] avg loss: 0.364655
[epoch 10, batch  1299] avg loss: 0.361976
[epoch 10, batch  1399] avg loss: 0.375934
[epoch 10, batch  1499] avg loss: 0.367654
[epoch 10, batch  1599] avg loss: 0.362129
[epoch 10, batch  1699] avg loss: 0.364020
[epoch 10, batch  1799] avg loss: 0.365961
[epoch 10, batch  1899] avg loss: 0.365443
[epoch 10, batch  1999] avg loss: 0.376164
[epoch 10, batch  2099] avg loss: 0.362565
[epoch 10, batch  2199] avg loss: 0.386639
[epoch 10, batch  2299] avg loss: 0.374169
[epoch 10, batch  2399] avg loss: 0.368599
[epoch 11, batch    99] avg loss: 0.386091
[epoch 11, batch   199] avg loss: 0.366447
[epoch 11, batch   299] avg loss: 0.370402
[epoch 11, batch   399] avg loss: 0.354596
[epoch 11, batch   499] avg loss: 0.367688
[epoch 11, batch   599] avg loss: 0.363155
[epoch 11, batch   699] avg loss: 0.361611
[epoch 11, batch   799] avg loss: 0.365114
[epoch 11, batch   899] avg loss: 0.359263
[epoch 11, batch   999] avg loss: 0.363539
[epoch 11, batch  1099] avg loss: 0.362381
[epoch 11, batch  1199] avg loss: 0.370523
[epoch 11, batch  1299] avg loss: 0.362350
[epoch 11, batch  1399] avg loss: 0.356613
[epoch 11, batch  1499] avg loss: 0.357915
[epoch 11, batch  1599] avg loss: 0.358376
[epoch 11, batch  1699] avg loss: 0.363562
[epoch 11, batch  1799] avg loss: 0.359142
[epoch 11, batch  1899] avg loss: 0.370659
[epoch 11, batch  1999] avg loss: 0.353212
[epoch 11, batch  2099] avg loss: 0.365501
[epoch 11, batch  2199] avg loss: 0.363372
[epoch 11, batch  2299] avg loss: 0.353052
[epoch 11, batch  2399] avg loss: 0.363685
[epoch 12, batch    99] avg loss: 0.348089
[epoch 12, batch   199] avg loss: 0.353537
[epoch 12, batch   299] avg loss: 0.362400
[epoch 12, batch   399] avg loss: 0.353627
[epoch 12, batch   499] avg loss: 0.357933
[epoch 12, batch   599] avg loss: 0.353820
[epoch 12, batch   699] avg loss: 0.356413
[epoch 12, batch   799] avg loss: 0.350836
[epoch 12, batch   899] avg loss: 0.350444
[epoch 12, batch   999] avg loss: 0.347243
[epoch 12, batch  1099] avg loss: 0.362180
[epoch 12, batch  1199] avg loss: 0.345782
[epoch 12, batch  1299] avg loss: 0.354309
[epoch 12, batch  1399] avg loss: 0.350915
[epoch 12, batch  1499] avg loss: 0.348005
[epoch 12, batch  1599] avg loss: 0.347238
[epoch 12, batch  1699] avg loss: 0.359839
[epoch 12, batch  1799] avg loss: 0.352309
[epoch 12, batch  1899] avg loss: 0.357635
[epoch 12, batch  1999] avg loss: 0.351238
[epoch 12, batch  2099] avg loss: 0.349196
[epoch 12, batch  2199] avg loss: 0.357105
[epoch 12, batch  2299] avg loss: 0.348891
[epoch 12, batch  2399] avg loss: 0.357795
[epoch 13, batch    99] avg loss: 0.354081
[epoch 13, batch   199] avg loss: 0.346528
[epoch 13, batch   299] avg loss: 0.351581
[epoch 13, batch   399] avg loss: 0.346871
[epoch 13, batch   499] avg loss: 0.353615
[epoch 13, batch   599] avg loss: 0.343174
[epoch 13, batch   699] avg loss: 0.339648
[epoch 13, batch   799] avg loss: 0.351388
[epoch 13, batch   899] avg loss: 0.357506
[epoch 13, batch   999] avg loss: 0.341007
[epoch 13, batch  1099] avg loss: 0.337431
[epoch 13, batch  1199] avg loss: 0.361766
[epoch 13, batch  1299] avg loss: 0.334714
[epoch 13, batch  1399] avg loss: 0.343003
[epoch 13, batch  1499] avg loss: 0.341797
[epoch 13, batch  1599] avg loss: 0.355265
[epoch 13, batch  1699] avg loss: 0.346441
[epoch 13, batch  1799] avg loss: 0.342876
[epoch 13, batch  1899] avg loss: 0.346967
[epoch 13, batch  1999] avg loss: 0.333929
[epoch 13, batch  2099] avg loss: 0.347799
[epoch 13, batch  2199] avg loss: 0.340455
[epoch 13, batch  2299] avg loss: 0.351599
[epoch 13, batch  2399] avg loss: 0.359970
[epoch 14, batch    99] avg loss: 0.345181
[epoch 14, batch   199] avg loss: 0.343322
[epoch 14, batch   299] avg loss: 0.338469
[epoch 14, batch   399] avg loss: 0.339643
[epoch 14, batch   499] avg loss: 0.342622
[epoch 14, batch   599] avg loss: 0.334930
[epoch 14, batch   699] avg loss: 0.351046
[epoch 14, batch   799] avg loss: 0.328837
[epoch 14, batch   899] avg loss: 0.328143
[epoch 14, batch   999] avg loss: 0.343957
[epoch 14, batch  1099] avg loss: 0.348011
[epoch 14, batch  1199] avg loss: 0.338849
[epoch 14, batch  1299] avg loss: 0.336032
[epoch 14, batch  1399] avg loss: 0.334723
[epoch 14, batch  1499] avg loss: 0.341047
[epoch 14, batch  1599] avg loss: 0.327620
[epoch 14, batch  1699] avg loss: 0.338715
[epoch 14, batch  1799] avg loss: 0.342039
[epoch 14, batch  1899] avg loss: 0.343857
[epoch 14, batch  1999] avg loss: 0.327816
[epoch 14, batch  2099] avg loss: 0.329336
[epoch 14, batch  2199] avg loss: 0.339525
[epoch 14, batch  2299] avg loss: 0.341146
[epoch 14, batch  2399] avg loss: 0.328295
[epoch 15, batch    99] avg loss: 0.321356
[epoch 15, batch   199] avg loss: 0.338770
[epoch 15, batch   299] avg loss: 0.331530
[epoch 15, batch   399] avg loss: 0.326341
[epoch 15, batch   499] avg loss: 0.336574
[epoch 15, batch   599] avg loss: 0.327424
[epoch 15, batch   699] avg loss: 0.331783
[epoch 15, batch   799] avg loss: 0.329873
[epoch 15, batch   899] avg loss: 0.337158
[epoch 15, batch   999] avg loss: 0.328052
[epoch 15, batch  1099] avg loss: 0.334461
[epoch 15, batch  1199] avg loss: 0.327520
[epoch 15, batch  1299] avg loss: 0.338041
[epoch 15, batch  1399] avg loss: 0.340924
[epoch 15, batch  1499] avg loss: 0.340969
[epoch 15, batch  1599] avg loss: 0.342936
[epoch 15, batch  1699] avg loss: 0.332350
[epoch 15, batch  1799] avg loss: 0.324328
[epoch 15, batch  1899] avg loss: 0.338395
[epoch 15, batch  1999] avg loss: 0.338316
[epoch 15, batch  2099] avg loss: 0.343078
[epoch 15, batch  2199] avg loss: 0.330867
[epoch 15, batch  2299] avg loss: 0.317215
[epoch 15, batch  2399] avg loss: 0.342521
[epoch 16, batch    99] avg loss: 0.316882
[epoch 16, batch   199] avg loss: 0.334377
[epoch 16, batch   299] avg loss: 0.327876
[epoch 16, batch   399] avg loss: 0.319675
[epoch 16, batch   499] avg loss: 0.317709
[epoch 16, batch   599] avg loss: 0.325342
[epoch 16, batch   699] avg loss: 0.324627
[epoch 16, batch   799] avg loss: 0.325401
[epoch 16, batch   899] avg loss: 0.334796
[epoch 16, batch   999] avg loss: 0.326333
[epoch 16, batch  1099] avg loss: 0.331455
[epoch 16, batch  1199] avg loss: 0.317081
[epoch 16, batch  1299] avg loss: 0.338958
[epoch 16, batch  1399] avg loss: 0.333401
[epoch 16, batch  1499] avg loss: 0.331450
[epoch 16, batch  1599] avg loss: 0.315573
[epoch 16, batch  1699] avg loss: 0.320476
[epoch 16, batch  1799] avg loss: 0.329152
[epoch 16, batch  1899] avg loss: 0.332022
[epoch 16, batch  1999] avg loss: 0.329287
[epoch 16, batch  2099] avg loss: 0.339367
[epoch 16, batch  2199] avg loss: 0.329292
[epoch 16, batch  2299] avg loss: 0.339465
[epoch 16, batch  2399] avg loss: 0.310506
[epoch 17, batch    99] avg loss: 0.320203
[epoch 17, batch   199] avg loss: 0.328126
[epoch 17, batch   299] avg loss: 0.327523
[epoch 17, batch   399] avg loss: 0.320650
[epoch 17, batch   499] avg loss: 0.339278
[epoch 17, batch   599] avg loss: 0.308459
[epoch 17, batch   699] avg loss: 0.309006
[epoch 17, batch   799] avg loss: 0.314741
[epoch 17, batch   899] avg loss: 0.310369
[epoch 17, batch   999] avg loss: 0.319933
[epoch 17, batch  1099] avg loss: 0.316687
[epoch 17, batch  1199] avg loss: 0.315226
[epoch 17, batch  1299] avg loss: 0.329263
[epoch 17, batch  1399] avg loss: 0.315184
[epoch 17, batch  1499] avg loss: 0.323958
[epoch 17, batch  1599] avg loss: 0.336641
[epoch 17, batch  1699] avg loss: 0.326756
[epoch 17, batch  1799] avg loss: 0.325620
[epoch 17, batch  1899] avg loss: 0.310139
[epoch 17, batch  1999] avg loss: 0.319116
[epoch 17, batch  2099] avg loss: 0.310495
[epoch 17, batch  2199] avg loss: 0.313872
[epoch 17, batch  2299] avg loss: 0.310459
[epoch 17, batch  2399] avg loss: 0.311037
[epoch 18, batch    99] avg loss: 0.320614
[epoch 18, batch   199] avg loss: 0.306854
[epoch 18, batch   299] avg loss: 0.324721
[epoch 18, batch   399] avg loss: 0.315797
[epoch 18, batch   499] avg loss: 0.312947
[epoch 18, batch   599] avg loss: 0.309464
[epoch 18, batch   699] avg loss: 0.317363
[epoch 18, batch   799] avg loss: 0.322943
[epoch 18, batch   899] avg loss: 0.328115
[epoch 18, batch   999] avg loss: 0.304640
[epoch 18, batch  1099] avg loss: 0.312678
[epoch 18, batch  1199] avg loss: 0.320078
[epoch 18, batch  1299] avg loss: 0.315772
[epoch 18, batch  1399] avg loss: 0.319930
[epoch 18, batch  1499] avg loss: 0.293217
[epoch 18, batch  1599] avg loss: 0.296594
[epoch 18, batch  1699] avg loss: 0.322456
[epoch 18, batch  1799] avg loss: 0.307210
[epoch 18, batch  1899] avg loss: 0.320524
[epoch 18, batch  1999] avg loss: 0.310694
[epoch 18, batch  2099] avg loss: 0.317264
[epoch 18, batch  2199] avg loss: 0.295256
[epoch 18, batch  2299] avg loss: 0.308386
[epoch 18, batch  2399] avg loss: 0.300522
[epoch 19, batch    99] avg loss: 0.322317
[epoch 19, batch   199] avg loss: 0.311439
[epoch 19, batch   299] avg loss: 0.300528
[epoch 19, batch   399] avg loss: 0.313161
[epoch 19, batch   499] avg loss: 0.305698
[epoch 19, batch   599] avg loss: 0.312393
[epoch 19, batch   699] avg loss: 0.307048
[epoch 19, batch   799] avg loss: 0.317396
[epoch 19, batch   899] avg loss: 0.303882
[epoch 19, batch   999] avg loss: 0.299151
[epoch 19, batch  1099] avg loss: 0.324199
[epoch 19, batch  1199] avg loss: 0.321314
[epoch 19, batch  1299] avg loss: 0.311786
[epoch 19, batch  1399] avg loss: 0.297935
[epoch 19, batch  1499] avg loss: 0.305151
[epoch 19, batch  1599] avg loss: 0.300384
[epoch 19, batch  1699] avg loss: 0.323523
[epoch 19, batch  1799] avg loss: 0.288104
[epoch 19, batch  1899] avg loss: 0.299492
[epoch 19, batch  1999] avg loss: 0.313285
[epoch 19, batch  2099] avg loss: 0.302101
[epoch 19, batch  2199] avg loss: 0.301802
[epoch 19, batch  2299] avg loss: 0.309419
[epoch 19, batch  2399] avg loss: 0.306413
Model saved to model/20200502-082549.pth.
accuracy/TriangPrismIsosc : 0.78
n_examples/TriangPrismIsosc : 500.0
accuracy/parallelepiped : 0.67
n_examples/parallelepiped : 500.0
accuracy/sphere : 1.0
n_examples/sphere : 102.0
accuracy/wire : 0.935
n_examples/wire : 200.0
accuracy/avg_geom : 0.7788018433179723
loss/validation_geom : 0.6018220715442194
accuracy/Au : 0.0
n_examples/Au : 0.0
accuracy/SiN : 0.0
n_examples/SiN : 0.0
accuracy/SiO2 : 1.0
n_examples/SiO2 : 1302.0
accuracy/avg_mat : 1.0
loss/validation_mat : 0.001243409231453905
MSE/ShortestDim : 5.324338933472992
MAE/ShortestDim : 1.3456971041068504
MSE/MiddleDim : 82.22716113206245
MAE/MiddleDim : 5.406693676832817
MSE/LongDim : 545.3687029719902
MAE/LongDim : 11.246914952947614
MSE/log Area/Vol : 48.81258288030434
MAE/log Area/Vol : 3.5955372589158205
loss/validation_dim : 681.73278591783
loss/validation : 682.3358513986057
Metrics saved to model/20200502-082549_metrics.csv.
Parsed 2604 rows from data/sim_train_spectrum_Au.
Parsed 2604 rows from data/sim_train_labels_Au.
Parsed 9765 rows from data/gen_spectrum_Au_00-of-16.
Parsed 9765 rows from data/gen_labels_Au_00-of-16.
Parsed 9765 rows from data/gen_spectrum_Au_01-of-16.
Parsed 9765 rows from data/gen_labels_Au_01-of-16.
Parsed 9765 rows from data/gen_spectrum_Au_02-of-16.
Parsed 9765 rows from data/gen_labels_Au_02-of-16.
Parsed 9765 rows from data/gen_spectrum_Au_03-of-16.
Parsed 9765 rows from data/gen_labels_Au_03-of-16.
Parsed 9765 rows from data/gen_spectrum_Au_04-of-16.
Parsed 9765 rows from data/gen_labels_Au_04-of-16.
Parsed 9765 rows from data/gen_spectrum_Au_05-of-16.
Parsed 9765 rows from data/gen_labels_Au_05-of-16.
Parsed 9765 rows from data/gen_spectrum_Au_06-of-16.
Parsed 9765 rows from data/gen_labels_Au_06-of-16.
Parsed 9765 rows from data/gen_spectrum_Au_07-of-16.
Parsed 9765 rows from data/gen_labels_Au_07-of-16.
Parsed 9765 rows from data/gen_spectrum_Au_08-of-16.
Parsed 9765 rows from data/gen_labels_Au_08-of-16.
Parsed 9765 rows from data/gen_spectrum_Au_09-of-16.
Parsed 9765 rows from data/gen_labels_Au_09-of-16.
Parsed 9765 rows from data/gen_spectrum_Au_10-of-16.
Parsed 9765 rows from data/gen_labels_Au_10-of-16.
Parsed 9765 rows from data/gen_spectrum_Au_11-of-16.
Parsed 9765 rows from data/gen_labels_Au_11-of-16.
Parsed 9765 rows from data/gen_spectrum_Au_12-of-16.
Parsed 9765 rows from data/gen_labels_Au_12-of-16.
Parsed 9765 rows from data/gen_spectrum_Au_13-of-16.
Parsed 9765 rows from data/gen_labels_Au_13-of-16.
Parsed 9765 rows from data/gen_spectrum_Au_14-of-16.
Parsed 9765 rows from data/gen_labels_Au_14-of-16.
Parsed 9765 rows from data/gen_spectrum_Au_15-of-16.
Parsed 9765 rows from data/gen_labels_Au_15-of-16.
Parsed 1302 rows from data/sim_validation_spectrum_Au.
Parsed 1302 rows from data/sim_validation_labels_Au.
Logging training progress to tensorboard dir runs/alexnet-Au-lr_0.000100-trainsize_158844-05_02_2020_21:26-joint.
[epoch 0, batch    99] avg loss: 1.385486
[epoch 0, batch   199] avg loss: 1.315284
[epoch 0, batch   299] avg loss: 1.114021
[epoch 0, batch   399] avg loss: 1.035852
[epoch 0, batch   499] avg loss: 0.994207
[epoch 0, batch   599] avg loss: 0.963689
[epoch 0, batch   699] avg loss: 0.962597
[epoch 0, batch   799] avg loss: 0.946964
[epoch 0, batch   899] avg loss: 0.949620
[epoch 0, batch   999] avg loss: 0.945689
[epoch 0, batch  1099] avg loss: 0.923680
[epoch 0, batch  1199] avg loss: 0.917563
[epoch 0, batch  1299] avg loss: 0.918248
[epoch 0, batch  1399] avg loss: 0.898763
[epoch 0, batch  1499] avg loss: 0.893772
[epoch 0, batch  1599] avg loss: 0.894057
[epoch 0, batch  1699] avg loss: 0.893061
[epoch 0, batch  1799] avg loss: 0.884953
[epoch 0, batch  1899] avg loss: 0.879488
[epoch 0, batch  1999] avg loss: 0.877135
[epoch 0, batch  2099] avg loss: 0.876548
[epoch 0, batch  2199] avg loss: 0.867308
[epoch 0, batch  2299] avg loss: 0.865838
[epoch 0, batch  2399] avg loss: 0.863280
[epoch 1, batch    99] avg loss: 0.833183
[epoch 1, batch   199] avg loss: 0.844212
[epoch 1, batch   299] avg loss: 0.850279
[epoch 1, batch   399] avg loss: 0.840342
[epoch 1, batch   499] avg loss: 0.850615
[epoch 1, batch   599] avg loss: 0.848553
[epoch 1, batch   699] avg loss: 0.845758
[epoch 1, batch   799] avg loss: 0.838180
[epoch 1, batch   899] avg loss: 0.832982
[epoch 1, batch   999] avg loss: 0.842820
[epoch 1, batch  1099] avg loss: 0.835376
[epoch 1, batch  1199] avg loss: 0.813787
[epoch 1, batch  1299] avg loss: 0.829435
[epoch 1, batch  1399] avg loss: 0.854324
[epoch 1, batch  1499] avg loss: 0.847701
[epoch 1, batch  1599] avg loss: 0.821193
[epoch 1, batch  1699] avg loss: 0.818228
[epoch 1, batch  1799] avg loss: 0.810795
[epoch 1, batch  1899] avg loss: 0.819578
[epoch 1, batch  1999] avg loss: 0.806134
[epoch 1, batch  2099] avg loss: 0.807596
[epoch 1, batch  2199] avg loss: 0.817175
[epoch 1, batch  2299] avg loss: 0.823323
[epoch 1, batch  2399] avg loss: 0.826340
[epoch 2, batch    99] avg loss: 0.803520
[epoch 2, batch   199] avg loss: 0.815986
[epoch 2, batch   299] avg loss: 0.806403
[epoch 2, batch   399] avg loss: 0.808059
[epoch 2, batch   499] avg loss: 0.813420
[epoch 2, batch   599] avg loss: 0.805487
[epoch 2, batch   699] avg loss: 0.828804
[epoch 2, batch   799] avg loss: 0.800958
[epoch 2, batch   899] avg loss: 0.818475
[epoch 2, batch   999] avg loss: 0.797664
[epoch 2, batch  1099] avg loss: 0.805648
[epoch 2, batch  1199] avg loss: 0.790544
[epoch 2, batch  1299] avg loss: 0.793308
[epoch 2, batch  1399] avg loss: 0.802050
[epoch 2, batch  1499] avg loss: 0.802858
[epoch 2, batch  1599] avg loss: 0.808411
[epoch 2, batch  1699] avg loss: 0.798260
[epoch 2, batch  1799] avg loss: 0.788878
[epoch 2, batch  1899] avg loss: 0.777312
[epoch 2, batch  1999] avg loss: 0.800794
[epoch 2, batch  2099] avg loss: 0.788770
[epoch 2, batch  2199] avg loss: 0.778454
[epoch 2, batch  2299] avg loss: 0.782179
[epoch 2, batch  2399] avg loss: 0.791549
[epoch 3, batch    99] avg loss: 0.786051
[epoch 3, batch   199] avg loss: 0.778381
[epoch 3, batch   299] avg loss: 0.789274
[epoch 3, batch   399] avg loss: 0.794884
[epoch 3, batch   499] avg loss: 0.757557
[epoch 3, batch   599] avg loss: 0.794295
[epoch 3, batch   699] avg loss: 0.787435
[epoch 3, batch   799] avg loss: 0.773726
[epoch 3, batch   899] avg loss: 0.776540
[epoch 3, batch   999] avg loss: 0.790152
[epoch 3, batch  1099] avg loss: 0.764422
[epoch 3, batch  1199] avg loss: 0.782456
[epoch 3, batch  1299] avg loss: 0.776148
[epoch 3, batch  1399] avg loss: 0.768091
[epoch 3, batch  1499] avg loss: 0.768318
[epoch 3, batch  1599] avg loss: 0.785437
[epoch 3, batch  1699] avg loss: 0.761078
[epoch 3, batch  1799] avg loss: 0.764624
[epoch 3, batch  1899] avg loss: 0.779538
[epoch 3, batch  1999] avg loss: 0.764526
[epoch 3, batch  2099] avg loss: 0.776011
[epoch 3, batch  2199] avg loss: 0.757354
[epoch 3, batch  2299] avg loss: 0.750952
[epoch 3, batch  2399] avg loss: 0.781474
[epoch 4, batch    99] avg loss: 0.773167
[epoch 4, batch   199] avg loss: 0.776838
[epoch 4, batch   299] avg loss: 0.769410
[epoch 4, batch   399] avg loss: 0.745570
[epoch 4, batch   499] avg loss: 0.786779
[epoch 4, batch   599] avg loss: 0.753526
[epoch 4, batch   699] avg loss: 0.776674
[epoch 4, batch   799] avg loss: 0.758886
[epoch 4, batch   899] avg loss: 0.764041
[epoch 4, batch   999] avg loss: 0.758888
[epoch 4, batch  1099] avg loss: 0.759296
[epoch 4, batch  1199] avg loss: 0.736103
[epoch 4, batch  1299] avg loss: 0.739002
[epoch 4, batch  1399] avg loss: 0.745752
[epoch 4, batch  1499] avg loss: 0.748173
[epoch 4, batch  1599] avg loss: 0.746030
[epoch 4, batch  1699] avg loss: 0.751234
[epoch 4, batch  1799] avg loss: 0.719603
[epoch 4, batch  1899] avg loss: 0.752503
[epoch 4, batch  1999] avg loss: 0.737895
[epoch 4, batch  2099] avg loss: 0.734319
[epoch 4, batch  2199] avg loss: 0.748435
[epoch 4, batch  2299] avg loss: 0.731618
[epoch 4, batch  2399] avg loss: 0.762014
[epoch 5, batch    99] avg loss: 0.741338
[epoch 5, batch   199] avg loss: 0.733612
[epoch 5, batch   299] avg loss: 0.746690
[epoch 5, batch   399] avg loss: 0.745283
[epoch 5, batch   499] avg loss: 0.730144
[epoch 5, batch   599] avg loss: 0.734925
[epoch 5, batch   699] avg loss: 0.743885
[epoch 5, batch   799] avg loss: 0.726902
[epoch 5, batch   899] avg loss: 0.739063
[epoch 5, batch   999] avg loss: 0.730974
[epoch 5, batch  1099] avg loss: 0.736888
[epoch 5, batch  1199] avg loss: 0.739217
[epoch 5, batch  1299] avg loss: 0.724192
[epoch 5, batch  1399] avg loss: 0.715436
[epoch 5, batch  1499] avg loss: 0.722923
[epoch 5, batch  1599] avg loss: 0.733862
[epoch 5, batch  1699] avg loss: 0.733189
[epoch 5, batch  1799] avg loss: 0.718577
[epoch 5, batch  1899] avg loss: 0.726569
[epoch 5, batch  1999] avg loss: 0.733022
[epoch 5, batch  2099] avg loss: 0.719914
[epoch 5, batch  2199] avg loss: 0.731952
[epoch 5, batch  2299] avg loss: 0.729386
[epoch 5, batch  2399] avg loss: 0.731528
[epoch 6, batch    99] avg loss: 0.707261
[epoch 6, batch   199] avg loss: 0.734738
[epoch 6, batch   299] avg loss: 0.717916
[epoch 6, batch   399] avg loss: 0.707413
[epoch 6, batch   499] avg loss: 0.729089
[epoch 6, batch   599] avg loss: 0.702255
[epoch 6, batch   699] avg loss: 0.718380
[epoch 6, batch   799] avg loss: 0.706619
[epoch 6, batch   899] avg loss: 0.735904
[epoch 6, batch   999] avg loss: 0.723190
[epoch 6, batch  1099] avg loss: 0.714703
[epoch 6, batch  1199] avg loss: 0.728497
[epoch 6, batch  1299] avg loss: 0.704198
[epoch 6, batch  1399] avg loss: 0.723856
[epoch 6, batch  1499] avg loss: 0.717695
[epoch 6, batch  1599] avg loss: 0.711422
[epoch 6, batch  1699] avg loss: 0.729144
[epoch 6, batch  1799] avg loss: 0.711534
[epoch 6, batch  1899] avg loss: 0.704625
[epoch 6, batch  1999] avg loss: 0.726920
[epoch 6, batch  2099] avg loss: 0.717611
[epoch 6, batch  2199] avg loss: 0.708724
[epoch 6, batch  2299] avg loss: 0.705815
[epoch 6, batch  2399] avg loss: 0.718717
[epoch 7, batch    99] avg loss: 0.697991
[epoch 7, batch   199] avg loss: 0.703813
[epoch 7, batch   299] avg loss: 0.715031
[epoch 7, batch   399] avg loss: 0.713747
[epoch 7, batch   499] avg loss: 0.704223
[epoch 7, batch   599] avg loss: 0.703376
[epoch 7, batch   699] avg loss: 0.702045
[epoch 7, batch   799] avg loss: 0.714048
[epoch 7, batch   899] avg loss: 0.695539
[epoch 7, batch   999] avg loss: 0.711684
[epoch 7, batch  1099] avg loss: 0.704271
[epoch 7, batch  1199] avg loss: 0.710157
[epoch 7, batch  1299] avg loss: 0.702208
[epoch 7, batch  1399] avg loss: 0.697230
[epoch 7, batch  1499] avg loss: 0.693362
[epoch 7, batch  1599] avg loss: 0.697815
[epoch 7, batch  1699] avg loss: 0.669634
[epoch 7, batch  1799] avg loss: 0.724354
[epoch 7, batch  1899] avg loss: 0.707732
[epoch 7, batch  1999] avg loss: 0.688811
[epoch 7, batch  2099] avg loss: 0.687982
[epoch 7, batch  2199] avg loss: 0.707448
[epoch 7, batch  2299] avg loss: 0.704298
[epoch 7, batch  2399] avg loss: 0.737371
[epoch 8, batch    99] avg loss: 0.682746
[epoch 8, batch   199] avg loss: 0.686702
[epoch 8, batch   299] avg loss: 0.706245
[epoch 8, batch   399] avg loss: 0.683654
[epoch 8, batch   499] avg loss: 0.684626
[epoch 8, batch   599] avg loss: 0.707300
[epoch 8, batch   699] avg loss: 0.697345
[epoch 8, batch   799] avg loss: 0.701242
[epoch 8, batch   899] avg loss: 0.702491
[epoch 8, batch   999] avg loss: 0.693727
[epoch 8, batch  1099] avg loss: 0.689432
[epoch 8, batch  1199] avg loss: 0.684658
[epoch 8, batch  1299] avg loss: 0.698053
[epoch 8, batch  1399] avg loss: 0.679092
[epoch 8, batch  1499] avg loss: 0.731245
[epoch 8, batch  1599] avg loss: 0.701724
[epoch 8, batch  1699] avg loss: 0.698891
[epoch 8, batch  1799] avg loss: 0.699700
[epoch 8, batch  1899] avg loss: 0.706610
[epoch 8, batch  1999] avg loss: 0.677067
[epoch 8, batch  2099] avg loss: 0.691537
[epoch 8, batch  2199] avg loss: 0.704717
[epoch 8, batch  2299] avg loss: 0.682123
[epoch 8, batch  2399] avg loss: 0.683425
[epoch 9, batch    99] avg loss: 0.680717
[epoch 9, batch   199] avg loss: 0.676461
[epoch 9, batch   299] avg loss: 0.689414
[epoch 9, batch   399] avg loss: 0.688563
[epoch 9, batch   499] avg loss: 0.686220
[epoch 9, batch   599] avg loss: 0.691108
[epoch 9, batch   699] avg loss: 0.673096
[epoch 9, batch   799] avg loss: 0.684569
[epoch 9, batch   899] avg loss: 0.683017
[epoch 9, batch   999] avg loss: 0.681859
[epoch 9, batch  1099] avg loss: 0.712596
[epoch 9, batch  1199] avg loss: 0.702134
[epoch 9, batch  1299] avg loss: 0.668659
[epoch 9, batch  1399] avg loss: 0.672328
[epoch 9, batch  1499] avg loss: 0.676831
[epoch 9, batch  1599] avg loss: 0.675942
[epoch 9, batch  1699] avg loss: 0.688738
[epoch 9, batch  1799] avg loss: 0.682407
[epoch 9, batch  1899] avg loss: 0.678480
[epoch 9, batch  1999] avg loss: 0.675797
[epoch 9, batch  2099] avg loss: 0.674707
[epoch 9, batch  2199] avg loss: 0.666496
[epoch 9, batch  2299] avg loss: 0.674425
[epoch 9, batch  2399] avg loss: 0.682791
[epoch 10, batch    99] avg loss: 0.663644
[epoch 10, batch   199] avg loss: 0.649697
[epoch 10, batch   299] avg loss: 0.659961
[epoch 10, batch   399] avg loss: 0.665971
[epoch 10, batch   499] avg loss: 0.670974
[epoch 10, batch   599] avg loss: 0.664126
[epoch 10, batch   699] avg loss: 0.681873
[epoch 10, batch   799] avg loss: 0.661898
[epoch 10, batch   899] avg loss: 0.658166
[epoch 10, batch   999] avg loss: 0.657232
[epoch 10, batch  1099] avg loss: 0.667262
[epoch 10, batch  1199] avg loss: 0.681504
[epoch 10, batch  1299] avg loss: 0.661257
[epoch 10, batch  1399] avg loss: 0.657789
[epoch 10, batch  1499] avg loss: 0.654725
[epoch 10, batch  1599] avg loss: 0.674060
[epoch 10, batch  1699] avg loss: 0.673497
[epoch 10, batch  1799] avg loss: 0.656453
[epoch 10, batch  1899] avg loss: 0.666160
[epoch 10, batch  1999] avg loss: 0.656014
[epoch 10, batch  2099] avg loss: 0.679827
[epoch 10, batch  2199] avg loss: 0.662129
[epoch 10, batch  2299] avg loss: 0.660042
[epoch 10, batch  2399] avg loss: 0.675687
[epoch 11, batch    99] avg loss: 0.648411
[epoch 11, batch   199] avg loss: 0.656477
[epoch 11, batch   299] avg loss: 0.672917
[epoch 11, batch   399] avg loss: 0.633618
[epoch 11, batch   499] avg loss: 0.649288
[epoch 11, batch   599] avg loss: 0.659808
[epoch 11, batch   699] avg loss: 0.650345
[epoch 11, batch   799] avg loss: 0.661247
[epoch 11, batch   899] avg loss: 0.663595
[epoch 11, batch   999] avg loss: 0.628278
[epoch 11, batch  1099] avg loss: 0.654226
[epoch 11, batch  1199] avg loss: 0.672948
[epoch 11, batch  1299] avg loss: 0.667379
[epoch 11, batch  1399] avg loss: 0.656370
[epoch 11, batch  1499] avg loss: 0.653894
[epoch 11, batch  1599] avg loss: 0.638117
[epoch 11, batch  1699] avg loss: 0.657377
[epoch 11, batch  1799] avg loss: 0.654111
[epoch 11, batch  1899] avg loss: 0.640794
[epoch 11, batch  1999] avg loss: 0.649711
[epoch 11, batch  2099] avg loss: 0.641514
[epoch 11, batch  2199] avg loss: 0.665891
[epoch 11, batch  2299] avg loss: 0.671964
[epoch 11, batch  2399] avg loss: 0.633295
[epoch 12, batch    99] avg loss: 0.641832
[epoch 12, batch   199] avg loss: 0.620739
[epoch 12, batch   299] avg loss: 0.635432
[epoch 12, batch   399] avg loss: 0.651214
[epoch 12, batch   499] avg loss: 0.650117
[epoch 12, batch   599] avg loss: 0.634651
[epoch 12, batch   699] avg loss: 0.645679
[epoch 12, batch   799] avg loss: 0.626372
[epoch 12, batch   899] avg loss: 0.643941
[epoch 12, batch   999] avg loss: 0.624449
[epoch 12, batch  1099] avg loss: 0.637787
[epoch 12, batch  1199] avg loss: 0.638642
[epoch 12, batch  1299] avg loss: 0.630942
[epoch 12, batch  1399] avg loss: 0.638085
[epoch 12, batch  1499] avg loss: 0.644073
[epoch 12, batch  1599] avg loss: 0.632445
[epoch 12, batch  1699] avg loss: 0.634870
[epoch 12, batch  1799] avg loss: 0.646524
[epoch 12, batch  1899] avg loss: 0.645491
[epoch 12, batch  1999] avg loss: 0.635526
[epoch 12, batch  2099] avg loss: 0.642683
[epoch 12, batch  2199] avg loss: 0.624416
[epoch 12, batch  2299] avg loss: 0.635194
[epoch 12, batch  2399] avg loss: 0.635230
[epoch 13, batch    99] avg loss: 0.637960
[epoch 13, batch   199] avg loss: 0.633763
[epoch 13, batch   299] avg loss: 0.645287
[epoch 13, batch   399] avg loss: 0.635496
[epoch 13, batch   499] avg loss: 0.623600
[epoch 13, batch   599] avg loss: 0.611219
[epoch 13, batch   699] avg loss: 0.628983
[epoch 13, batch   799] avg loss: 0.627893
[epoch 13, batch   899] avg loss: 0.616548
[epoch 13, batch   999] avg loss: 0.622009
[epoch 13, batch  1099] avg loss: 0.617461
[epoch 13, batch  1199] avg loss: 0.627624
[epoch 13, batch  1299] avg loss: 0.632040
[epoch 13, batch  1399] avg loss: 0.667925
[epoch 13, batch  1499] avg loss: 0.621313
[epoch 13, batch  1599] avg loss: 0.654786
[epoch 13, batch  1699] avg loss: 0.640652
[epoch 13, batch  1799] avg loss: 0.618162
[epoch 13, batch  1899] avg loss: 0.614760
[epoch 13, batch  1999] avg loss: 0.618670
[epoch 13, batch  2099] avg loss: 0.627683
[epoch 13, batch  2199] avg loss: 0.630813
[epoch 13, batch  2299] avg loss: 0.623902
[epoch 13, batch  2399] avg loss: 0.617550
[epoch 14, batch    99] avg loss: 0.606078
[epoch 14, batch   199] avg loss: 0.618099
[epoch 14, batch   299] avg loss: 0.615493
[epoch 14, batch   399] avg loss: 0.615975
[epoch 14, batch   499] avg loss: 0.618467
[epoch 14, batch   599] avg loss: 0.610670
[epoch 14, batch   699] avg loss: 0.630857
[epoch 14, batch   799] avg loss: 0.603482
[epoch 14, batch   899] avg loss: 0.617530
[epoch 14, batch   999] avg loss: 0.632898
[epoch 14, batch  1099] avg loss: 0.619560
[epoch 14, batch  1199] avg loss: 0.615534
[epoch 14, batch  1299] avg loss: 0.608151
[epoch 14, batch  1399] avg loss: 0.591750
[epoch 14, batch  1499] avg loss: 0.630158
[epoch 14, batch  1599] avg loss: 0.612367
[epoch 14, batch  1699] avg loss: 0.601045
[epoch 14, batch  1799] avg loss: 0.612807
[epoch 14, batch  1899] avg loss: 0.605620
[epoch 14, batch  1999] avg loss: 0.596541
[epoch 14, batch  2099] avg loss: 0.624521
[epoch 14, batch  2199] avg loss: 0.599803
[epoch 14, batch  2299] avg loss: 0.610937
[epoch 14, batch  2399] avg loss: 0.611527
[epoch 15, batch    99] avg loss: 0.612584
[epoch 15, batch   199] avg loss: 0.603352
[epoch 15, batch   299] avg loss: 0.606700
[epoch 15, batch   399] avg loss: 0.609678
[epoch 15, batch   499] avg loss: 0.604410
[epoch 15, batch   599] avg loss: 0.590356
[epoch 15, batch   699] avg loss: 0.613954
[epoch 15, batch   799] avg loss: 0.630365
[epoch 15, batch   899] avg loss: 0.617022
[epoch 15, batch   999] avg loss: 0.626323
[epoch 15, batch  1099] avg loss: 0.593159
[epoch 15, batch  1199] avg loss: 0.614282
[epoch 15, batch  1299] avg loss: 0.585780
[epoch 15, batch  1399] avg loss: 0.591768
[epoch 15, batch  1499] avg loss: 0.610263
[epoch 15, batch  1599] avg loss: 0.601220
[epoch 15, batch  1699] avg loss: 0.608874
[epoch 15, batch  1799] avg loss: 0.595076
[epoch 15, batch  1899] avg loss: 0.601132
[epoch 15, batch  1999] avg loss: 0.593932
[epoch 15, batch  2099] avg loss: 0.595925
[epoch 15, batch  2199] avg loss: 0.604858
[epoch 15, batch  2299] avg loss: 0.607241
[epoch 15, batch  2399] avg loss: 0.598717
[epoch 16, batch    99] avg loss: 0.597921
[epoch 16, batch   199] avg loss: 0.588938
[epoch 16, batch   299] avg loss: 0.591770
[epoch 16, batch   399] avg loss: 0.630851
[epoch 16, batch   499] avg loss: 0.609624
[epoch 16, batch   599] avg loss: 0.570561
[epoch 16, batch   699] avg loss: 0.600911
[epoch 16, batch   799] avg loss: 0.577405
[epoch 16, batch   899] avg loss: 0.581318
[epoch 16, batch   999] avg loss: 0.609368
[epoch 16, batch  1099] avg loss: 0.591604
[epoch 16, batch  1199] avg loss: 0.583045
[epoch 16, batch  1299] avg loss: 0.588147
[epoch 16, batch  1399] avg loss: 0.591923
[epoch 16, batch  1499] avg loss: 0.592018
[epoch 16, batch  1599] avg loss: 0.613981
[epoch 16, batch  1699] avg loss: 0.613408
[epoch 16, batch  1799] avg loss: 0.585864
[epoch 16, batch  1899] avg loss: 0.589527
[epoch 16, batch  1999] avg loss: 0.592352
[epoch 16, batch  2099] avg loss: 0.577780
[epoch 16, batch  2199] avg loss: 0.589378
[epoch 16, batch  2299] avg loss: 0.591155
[epoch 16, batch  2399] avg loss: 0.585372
[epoch 17, batch    99] avg loss: 0.583712
[epoch 17, batch   199] avg loss: 0.601861
[epoch 17, batch   299] avg loss: 0.569280
[epoch 17, batch   399] avg loss: 0.587186
[epoch 17, batch   499] avg loss: 0.584236
[epoch 17, batch   599] avg loss: 0.600472
[epoch 17, batch   699] avg loss: 0.585459
[epoch 17, batch   799] avg loss: 0.577346
[epoch 17, batch   899] avg loss: 0.594393
[epoch 17, batch   999] avg loss: 0.585277
[epoch 17, batch  1099] avg loss: 0.581607
[epoch 17, batch  1199] avg loss: 0.581664
[epoch 17, batch  1299] avg loss: 0.581864
[epoch 17, batch  1399] avg loss: 0.590934
[epoch 17, batch  1499] avg loss: 0.589802
[epoch 17, batch  1599] avg loss: 0.586295
[epoch 17, batch  1699] avg loss: 0.602636
[epoch 17, batch  1799] avg loss: 0.588496
[epoch 17, batch  1899] avg loss: 0.590958
[epoch 17, batch  1999] avg loss: 0.576608
[epoch 17, batch  2099] avg loss: 0.570100
[epoch 17, batch  2199] avg loss: 0.578236
[epoch 17, batch  2299] avg loss: 0.573812
[epoch 17, batch  2399] avg loss: 0.579276
[epoch 18, batch    99] avg loss: 0.582972
[epoch 18, batch   199] avg loss: 0.584319
[epoch 18, batch   299] avg loss: 0.590391
[epoch 18, batch   399] avg loss: 0.581879
[epoch 18, batch   499] avg loss: 0.592408
[epoch 18, batch   599] avg loss: 0.580129
[epoch 18, batch   699] avg loss: 0.579077
[epoch 18, batch   799] avg loss: 0.586032
[epoch 18, batch   899] avg loss: 0.566148
[epoch 18, batch   999] avg loss: 0.572446
[epoch 18, batch  1099] avg loss: 0.593774
[epoch 18, batch  1199] avg loss: 0.574904
[epoch 18, batch  1299] avg loss: 0.571439
[epoch 18, batch  1399] avg loss: 0.574212
[epoch 18, batch  1499] avg loss: 0.577914
[epoch 18, batch  1599] avg loss: 0.584150
[epoch 18, batch  1699] avg loss: 0.557331
[epoch 18, batch  1799] avg loss: 0.593851
[epoch 18, batch  1899] avg loss: 0.587440
[epoch 18, batch  1999] avg loss: 0.567060
[epoch 18, batch  2099] avg loss: 0.583796
[epoch 18, batch  2199] avg loss: 0.565383
[epoch 18, batch  2299] avg loss: 0.591233
[epoch 18, batch  2399] avg loss: 0.572984
[epoch 19, batch    99] avg loss: 0.573083
[epoch 19, batch   199] avg loss: 0.564547
[epoch 19, batch   299] avg loss: 0.591761
[epoch 19, batch   399] avg loss: 0.572180
[epoch 19, batch   499] avg loss: 0.568352
[epoch 19, batch   599] avg loss: 0.568730
[epoch 19, batch   699] avg loss: 0.573624
[epoch 19, batch   799] avg loss: 0.568246
[epoch 19, batch   899] avg loss: 0.565106
[epoch 19, batch   999] avg loss: 0.566398
[epoch 19, batch  1099] avg loss: 0.554533
[epoch 19, batch  1199] avg loss: 0.571760
[epoch 19, batch  1299] avg loss: 0.566809
[epoch 19, batch  1399] avg loss: 0.583689
[epoch 19, batch  1499] avg loss: 0.570506
[epoch 19, batch  1599] avg loss: 0.580384
[epoch 19, batch  1699] avg loss: 0.579246
[epoch 19, batch  1799] avg loss: 0.575581
[epoch 19, batch  1899] avg loss: 0.561143
[epoch 19, batch  1999] avg loss: 0.566801
[epoch 19, batch  2099] avg loss: 0.571238
[epoch 19, batch  2199] avg loss: 0.561136
[epoch 19, batch  2299] avg loss: 0.561736
[epoch 19, batch  2399] avg loss: 0.571740
[epoch 20, batch    99] avg loss: 0.578061
[epoch 20, batch   199] avg loss: 0.581979
[epoch 20, batch   299] avg loss: 0.553972
[epoch 20, batch   399] avg loss: 0.562751
[epoch 20, batch   499] avg loss: 0.567827
[epoch 20, batch   599] avg loss: 0.572591
[epoch 20, batch   699] avg loss: 0.563953
[epoch 20, batch   799] avg loss: 0.548719
[epoch 20, batch   899] avg loss: 0.584158
[epoch 20, batch   999] avg loss: 0.561672
[epoch 20, batch  1099] avg loss: 0.568823
[epoch 20, batch  1199] avg loss: 0.573119
[epoch 20, batch  1299] avg loss: 0.550311
[epoch 20, batch  1399] avg loss: 0.562057
[epoch 20, batch  1499] avg loss: 0.559373
[epoch 20, batch  1599] avg loss: 0.576500
[epoch 20, batch  1699] avg loss: 0.565962
[epoch 20, batch  1799] avg loss: 0.570394
[epoch 20, batch  1899] avg loss: 0.550790
[epoch 20, batch  1999] avg loss: 0.545712
[epoch 20, batch  2099] avg loss: 0.559897
[epoch 20, batch  2199] avg loss: 0.541967
[epoch 20, batch  2299] avg loss: 0.555408
[epoch 20, batch  2399] avg loss: 0.578501
[epoch 21, batch    99] avg loss: 0.572834
[epoch 21, batch   199] avg loss: 0.556935
[epoch 21, batch   299] avg loss: 0.551377
[epoch 21, batch   399] avg loss: 0.556835
[epoch 21, batch   499] avg loss: 0.553760
[epoch 21, batch   599] avg loss: 0.547638
[epoch 21, batch   699] avg loss: 0.541569
[epoch 21, batch   799] avg loss: 0.594186
[epoch 21, batch   899] avg loss: 0.566367
[epoch 21, batch   999] avg loss: 0.552834
[epoch 21, batch  1099] avg loss: 0.574630
[epoch 21, batch  1199] avg loss: 0.555522
[epoch 21, batch  1299] avg loss: 0.555953
[epoch 21, batch  1399] avg loss: 0.544470
[epoch 21, batch  1499] avg loss: 0.580172
[epoch 21, batch  1599] avg loss: 0.547755
[epoch 21, batch  1699] avg loss: 0.534238
[epoch 21, batch  1799] avg loss: 0.553903
[epoch 21, batch  1899] avg loss: 0.606062
[epoch 21, batch  1999] avg loss: 0.557142
[epoch 21, batch  2099] avg loss: 0.535138
[epoch 21, batch  2199] avg loss: 0.572013
[epoch 21, batch  2299] avg loss: 0.551455
[epoch 21, batch  2399] avg loss: 0.557258
[epoch 22, batch    99] avg loss: 0.562843
[epoch 22, batch   199] avg loss: 0.571720
[epoch 22, batch   299] avg loss: 0.557243
[epoch 22, batch   399] avg loss: 0.597770
[epoch 22, batch   499] avg loss: 0.537072
[epoch 22, batch   599] avg loss: 0.533800
[epoch 22, batch   699] avg loss: 0.555977
[epoch 22, batch   799] avg loss: 0.554525
[epoch 22, batch   899] avg loss: 0.553845
[epoch 22, batch   999] avg loss: 0.549223
[epoch 22, batch  1099] avg loss: 0.567880
[epoch 22, batch  1199] avg loss: 0.547491
[epoch 22, batch  1299] avg loss: 0.555724
[epoch 22, batch  1399] avg loss: 0.566478
[epoch 22, batch  1499] avg loss: 0.549184
[epoch 22, batch  1599] avg loss: 0.533348
[epoch 22, batch  1699] avg loss: 0.542801
[epoch 22, batch  1799] avg loss: 0.560000
[epoch 22, batch  1899] avg loss: 0.585960
[epoch 22, batch  1999] avg loss: 0.548180
[epoch 22, batch  2099] avg loss: 0.546493
[epoch 22, batch  2199] avg loss: 0.535421
[epoch 22, batch  2299] avg loss: 0.550226
[epoch 22, batch  2399] avg loss: 0.577102
[epoch 23, batch    99] avg loss: 0.533892
[epoch 23, batch   199] avg loss: 0.556984
[epoch 23, batch   299] avg loss: 0.534983
[epoch 23, batch   399] avg loss: 0.554288
[epoch 23, batch   499] avg loss: 0.604599
[epoch 23, batch   599] avg loss: 0.546337
[epoch 23, batch   699] avg loss: 0.545412
[epoch 23, batch   799] avg loss: 0.548801
[epoch 23, batch   899] avg loss: 0.523039
[epoch 23, batch   999] avg loss: 0.558268
[epoch 23, batch  1099] avg loss: 0.544160
[epoch 23, batch  1199] avg loss: 0.537357
[epoch 23, batch  1299] avg loss: 0.544146
[epoch 23, batch  1399] avg loss: 0.540922
[epoch 23, batch  1499] avg loss: 0.543400
[epoch 23, batch  1599] avg loss: 0.526113
[epoch 23, batch  1699] avg loss: 0.541163
[epoch 23, batch  1799] avg loss: 0.553985
[epoch 23, batch  1899] avg loss: 0.536687
[epoch 23, batch  1999] avg loss: 0.531872
[epoch 23, batch  2099] avg loss: 0.533095
[epoch 23, batch  2199] avg loss: 0.561945
[epoch 23, batch  2299] avg loss: 0.538336
[epoch 23, batch  2399] avg loss: 0.541609
[epoch 24, batch    99] avg loss: 0.558704
[epoch 24, batch   199] avg loss: 0.536077
[epoch 24, batch   299] avg loss: 0.545155
[epoch 24, batch   399] avg loss: 0.576302
[epoch 24, batch   499] avg loss: 0.543876
[epoch 24, batch   599] avg loss: 0.537933
[epoch 24, batch   699] avg loss: 0.543017
[epoch 24, batch   799] avg loss: 0.540747
[epoch 24, batch   899] avg loss: 0.537818
[epoch 24, batch   999] avg loss: 0.567454
[epoch 24, batch  1099] avg loss: 0.557955
[epoch 24, batch  1199] avg loss: 0.545110
[epoch 24, batch  1299] avg loss: 0.558669
[epoch 24, batch  1399] avg loss: 0.531657
[epoch 24, batch  1499] avg loss: 0.525407
[epoch 24, batch  1599] avg loss: 0.548656
[epoch 24, batch  1699] avg loss: 0.534622
[epoch 24, batch  1799] avg loss: 0.582168
[epoch 24, batch  1899] avg loss: 0.543256
[epoch 24, batch  1999] avg loss: 0.531847
[epoch 24, batch  2099] avg loss: 0.531261
[epoch 24, batch  2199] avg loss: 0.546360
[epoch 24, batch  2299] avg loss: 0.530564
[epoch 24, batch  2399] avg loss: 0.534758
[epoch 25, batch    99] avg loss: 0.530605
[epoch 25, batch   199] avg loss: 0.601053
[epoch 25, batch   299] avg loss: 0.535646
[epoch 25, batch   399] avg loss: 0.545723
[epoch 25, batch   499] avg loss: 0.546345
[epoch 25, batch   599] avg loss: 0.542043
[epoch 25, batch   699] avg loss: 0.541520
[epoch 25, batch   799] avg loss: 0.548693
[epoch 25, batch   899] avg loss: 0.528693
[epoch 25, batch   999] avg loss: 0.529085
[epoch 25, batch  1099] avg loss: 0.530803
[epoch 25, batch  1199] avg loss: 0.546016
[epoch 25, batch  1299] avg loss: 0.535912
[epoch 25, batch  1399] avg loss: 0.543598
[epoch 25, batch  1499] avg loss: 0.564338
[epoch 25, batch  1599] avg loss: 0.533645
[epoch 25, batch  1699] avg loss: 0.526591
[epoch 25, batch  1799] avg loss: 0.561234
[epoch 25, batch  1899] avg loss: 0.521940
[epoch 25, batch  1999] avg loss: 0.530528
[epoch 25, batch  2099] avg loss: 0.552839
[epoch 25, batch  2199] avg loss: 0.524534
[epoch 25, batch  2299] avg loss: 0.524135
[epoch 25, batch  2399] avg loss: 0.530440
[epoch 26, batch    99] avg loss: 0.543202
[epoch 26, batch   199] avg loss: 0.529997
[epoch 26, batch   299] avg loss: 0.548805
[epoch 26, batch   399] avg loss: 0.544334
[epoch 26, batch   499] avg loss: 0.530157
[epoch 26, batch   599] avg loss: 0.526874
[epoch 26, batch   699] avg loss: 0.546586
[epoch 26, batch   799] avg loss: 0.535854
[epoch 26, batch   899] avg loss: 0.547120
[epoch 26, batch   999] avg loss: 0.530680
[epoch 26, batch  1099] avg loss: 0.534877
[epoch 26, batch  1199] avg loss: 0.527876
[epoch 26, batch  1299] avg loss: 0.563043
[epoch 26, batch  1399] avg loss: 0.536165
[epoch 26, batch  1499] avg loss: 0.533310
[epoch 26, batch  1599] avg loss: 0.511392
[epoch 26, batch  1699] avg loss: 0.535710
[epoch 26, batch  1799] avg loss: 0.533882
[epoch 26, batch  1899] avg loss: 0.532589
[epoch 26, batch  1999] avg loss: 0.521825
[epoch 26, batch  2099] avg loss: 0.521571
[epoch 26, batch  2199] avg loss: 0.544320
[epoch 26, batch  2299] avg loss: 0.524486
[epoch 26, batch  2399] avg loss: 0.529178
[epoch 27, batch    99] avg loss: 0.518460
[epoch 27, batch   199] avg loss: 0.520455
[epoch 27, batch   299] avg loss: 0.521037
[epoch 27, batch   399] avg loss: 0.560396
[epoch 27, batch   499] avg loss: 0.523311
[epoch 27, batch   599] avg loss: 0.559625
[epoch 27, batch   699] avg loss: 0.527178
[epoch 27, batch   799] avg loss: 0.528042
[epoch 27, batch   899] avg loss: 0.522793
[epoch 27, batch   999] avg loss: 0.525772
[epoch 27, batch  1099] avg loss: 0.522144
[epoch 27, batch  1199] avg loss: 0.529544
[epoch 27, batch  1299] avg loss: 0.531541
[epoch 27, batch  1399] avg loss: 0.541729
[epoch 27, batch  1499] avg loss: 0.539837
[epoch 27, batch  1599] avg loss: 0.538399
[epoch 27, batch  1699] avg loss: 0.526392
[epoch 27, batch  1799] avg loss: 0.543092
[epoch 27, batch  1899] avg loss: 0.531512
[epoch 27, batch  1999] avg loss: 0.533184
[epoch 27, batch  2099] avg loss: 0.531807
[epoch 27, batch  2199] avg loss: 0.547005
[epoch 27, batch  2299] avg loss: 0.537393
[epoch 27, batch  2399] avg loss: 0.543671
[epoch 28, batch    99] avg loss: 0.518890
[epoch 28, batch   199] avg loss: 0.528280
[epoch 28, batch   299] avg loss: 0.524300
[epoch 28, batch   399] avg loss: 0.521937
[epoch 28, batch   499] avg loss: 0.525594
[epoch 28, batch   599] avg loss: 0.517090
[epoch 28, batch   699] avg loss: 0.526212
[epoch 28, batch   799] avg loss: 0.531214
[epoch 28, batch   899] avg loss: 0.535513
[epoch 28, batch   999] avg loss: 0.542387
[epoch 28, batch  1099] avg loss: 0.521129
[epoch 28, batch  1199] avg loss: 0.533211
[epoch 28, batch  1299] avg loss: 0.526377
[epoch 28, batch  1399] avg loss: 0.551108
[epoch 28, batch  1499] avg loss: 0.522718
[epoch 28, batch  1599] avg loss: 0.546422
[epoch 28, batch  1699] avg loss: 0.524346
[epoch 28, batch  1799] avg loss: 0.545312
[epoch 28, batch  1899] avg loss: 0.504133
[epoch 28, batch  1999] avg loss: 0.535204
[epoch 28, batch  2099] avg loss: 0.532798
[epoch 28, batch  2199] avg loss: 0.538179
[epoch 28, batch  2299] avg loss: 0.521633
[epoch 28, batch  2399] avg loss: 0.510595
[epoch 29, batch    99] avg loss: 0.535272
[epoch 29, batch   199] avg loss: 0.527794
[epoch 29, batch   299] avg loss: 0.536297
[epoch 29, batch   399] avg loss: 0.516361
[epoch 29, batch   499] avg loss: 0.520001
[epoch 29, batch   599] avg loss: 0.513160
[epoch 29, batch   699] avg loss: 0.516376
[epoch 29, batch   799] avg loss: 0.517462
[epoch 29, batch   899] avg loss: 0.503802
[epoch 29, batch   999] avg loss: 0.529352
[epoch 29, batch  1099] avg loss: 0.512689
[epoch 29, batch  1199] avg loss: 0.538271
[epoch 29, batch  1299] avg loss: 0.538481
[epoch 29, batch  1399] avg loss: 0.524873
[epoch 29, batch  1499] avg loss: 0.527660
[epoch 29, batch  1599] avg loss: 0.527906
[epoch 29, batch  1699] avg loss: 0.502059
[epoch 29, batch  1799] avg loss: 0.516828
[epoch 29, batch  1899] avg loss: 0.507018
[epoch 29, batch  1999] avg loss: 0.514960
[epoch 29, batch  2099] avg loss: 0.543066
[epoch 29, batch  2199] avg loss: 0.520387
[epoch 29, batch  2299] avg loss: 0.507577
[epoch 29, batch  2399] avg loss: 0.538270
[epoch 30, batch    99] avg loss: 0.513264
[epoch 30, batch   199] avg loss: 0.531984
[epoch 30, batch   299] avg loss: 0.526852
[epoch 30, batch   399] avg loss: 0.515387
[epoch 30, batch   499] avg loss: 0.512419
[epoch 30, batch   599] avg loss: 0.526046
[epoch 30, batch   699] avg loss: 0.532069
[epoch 30, batch   799] avg loss: 0.542304
[epoch 30, batch   899] avg loss: 0.506639
[epoch 30, batch   999] avg loss: 0.520772
[epoch 30, batch  1099] avg loss: 0.501118
[epoch 30, batch  1199] avg loss: 0.511466
[epoch 30, batch  1299] avg loss: 0.499922
[epoch 30, batch  1399] avg loss: 0.527774
[epoch 30, batch  1499] avg loss: 0.520175
[epoch 30, batch  1599] avg loss: 0.523821
[epoch 30, batch  1699] avg loss: 0.504462
[epoch 30, batch  1799] avg loss: 0.508749
[epoch 30, batch  1899] avg loss: 0.529243
[epoch 30, batch  1999] avg loss: 0.510114
[epoch 30, batch  2099] avg loss: 0.518722
[epoch 30, batch  2199] avg loss: 0.520546
[epoch 30, batch  2299] avg loss: 0.532854
[epoch 30, batch  2399] avg loss: 0.557627
[epoch 31, batch    99] avg loss: 0.519927
[epoch 31, batch   199] avg loss: 0.505214
[epoch 31, batch   299] avg loss: 0.507750
[epoch 31, batch   399] avg loss: 0.499135
[epoch 31, batch   499] avg loss: 0.552318
[epoch 31, batch   599] avg loss: 0.507546
[epoch 31, batch   699] avg loss: 0.502818
[epoch 31, batch   799] avg loss: 0.504651
[epoch 31, batch   899] avg loss: 0.516234
[epoch 31, batch   999] avg loss: 0.526685
[epoch 31, batch  1099] avg loss: 0.510453
[epoch 31, batch  1199] avg loss: 0.537820
[epoch 31, batch  1299] avg loss: 0.511177
[epoch 31, batch  1399] avg loss: 0.521443
[epoch 31, batch  1499] avg loss: 0.515808
[epoch 31, batch  1599] avg loss: 0.516520
[epoch 31, batch  1699] avg loss: 0.547315
[epoch 31, batch  1799] avg loss: 0.495608
[epoch 31, batch  1899] avg loss: 0.501220
[epoch 31, batch  1999] avg loss: 0.524816
[epoch 31, batch  2099] avg loss: 0.519435
[epoch 31, batch  2199] avg loss: 0.520492
[epoch 31, batch  2299] avg loss: 0.523861
[epoch 31, batch  2399] avg loss: 0.524587
[epoch 32, batch    99] avg loss: 0.508976
[epoch 32, batch   199] avg loss: 0.502147
[epoch 32, batch   299] avg loss: 0.520527
[epoch 32, batch   399] avg loss: 0.518152
[epoch 32, batch   499] avg loss: 0.502505
[epoch 32, batch   599] avg loss: 0.529827
[epoch 32, batch   699] avg loss: 0.519713
[epoch 32, batch   799] avg loss: 0.504456
[epoch 32, batch   899] avg loss: 0.519590
[epoch 32, batch   999] avg loss: 0.524001
[epoch 32, batch  1099] avg loss: 0.500158
[epoch 32, batch  1199] avg loss: 0.515801
[epoch 32, batch  1299] avg loss: 0.518122
[epoch 32, batch  1399] avg loss: 0.537269
[epoch 32, batch  1499] avg loss: 0.508672
[epoch 32, batch  1599] avg loss: 0.495825
[epoch 32, batch  1699] avg loss: 0.491900
[epoch 32, batch  1799] avg loss: 0.521820
[epoch 32, batch  1899] avg loss: 0.500781
[epoch 32, batch  1999] avg loss: 0.532512
[epoch 32, batch  2099] avg loss: 0.500122
[epoch 32, batch  2199] avg loss: 0.517372
[epoch 32, batch  2299] avg loss: 0.526444
[epoch 32, batch  2399] avg loss: 0.510618
[epoch 33, batch    99] avg loss: 0.510316
[epoch 33, batch   199] avg loss: 0.506414
[epoch 33, batch   299] avg loss: 0.515821
[epoch 33, batch   399] avg loss: 0.497463
[epoch 33, batch   499] avg loss: 0.511922
[epoch 33, batch   599] avg loss: 0.511358
[epoch 33, batch   699] avg loss: 0.512286
[epoch 33, batch   799] avg loss: 0.509796
[epoch 33, batch   899] avg loss: 0.502462
[epoch 33, batch   999] avg loss: 0.498428
[epoch 33, batch  1099] avg loss: 0.513767
[epoch 33, batch  1199] avg loss: 0.510145
[epoch 33, batch  1299] avg loss: 0.488660
[epoch 33, batch  1399] avg loss: 0.517712
[epoch 33, batch  1499] avg loss: 0.505268
[epoch 33, batch  1599] avg loss: 0.501777
[epoch 33, batch  1699] avg loss: 0.525689
[epoch 33, batch  1799] avg loss: 0.507730
[epoch 33, batch  1899] avg loss: 0.496665
[epoch 33, batch  1999] avg loss: 0.508069
[epoch 33, batch  2099] avg loss: 0.534470
[epoch 33, batch  2199] avg loss: 0.495695
[epoch 33, batch  2299] avg loss: 0.509639
[epoch 33, batch  2399] avg loss: 0.511331
[epoch 34, batch    99] avg loss: 0.506564
[epoch 34, batch   199] avg loss: 0.522267
[epoch 34, batch   299] avg loss: 0.513184
[epoch 34, batch   399] avg loss: 0.503847
[epoch 34, batch   499] avg loss: 0.491225
[epoch 34, batch   599] avg loss: 0.500338
[epoch 34, batch   699] avg loss: 0.506319
[epoch 34, batch   799] avg loss: 0.529786
[epoch 34, batch   899] avg loss: 0.511024
[epoch 34, batch   999] avg loss: 0.505955
[epoch 34, batch  1099] avg loss: 0.520223
[epoch 34, batch  1199] avg loss: 0.530925
[epoch 34, batch  1299] avg loss: 0.485007
[epoch 34, batch  1399] avg loss: 0.500673
[epoch 34, batch  1499] avg loss: 0.521578
[epoch 34, batch  1599] avg loss: 0.503987
[epoch 34, batch  1699] avg loss: 0.488890
[epoch 34, batch  1799] avg loss: 0.517834
[epoch 34, batch  1899] avg loss: 0.563188
[epoch 34, batch  1999] avg loss: 0.513544
[epoch 34, batch  2099] avg loss: 0.519244
[epoch 34, batch  2199] avg loss: 0.491333
[epoch 34, batch  2299] avg loss: 0.481945
[epoch 34, batch  2399] avg loss: 0.489361
[epoch 35, batch    99] avg loss: 0.514839
[epoch 35, batch   199] avg loss: 0.492196
[epoch 35, batch   299] avg loss: 0.506555
[epoch 35, batch   399] avg loss: 0.503390
[epoch 35, batch   499] avg loss: 0.510075
[epoch 35, batch   599] avg loss: 0.521033
[epoch 35, batch   699] avg loss: 0.512185
[epoch 35, batch   799] avg loss: 0.482681
[epoch 35, batch   899] avg loss: 0.528176
[epoch 35, batch   999] avg loss: 0.492267
[epoch 35, batch  1099] avg loss: 0.498818
[epoch 35, batch  1199] avg loss: 0.514227
[epoch 35, batch  1299] avg loss: 0.521820
[epoch 35, batch  1399] avg loss: 0.498266
[epoch 35, batch  1499] avg loss: 0.495281
[epoch 35, batch  1599] avg loss: 0.490083
[epoch 35, batch  1699] avg loss: 0.509916
[epoch 35, batch  1799] avg loss: 0.495810
[epoch 35, batch  1899] avg loss: 0.519229
[epoch 35, batch  1999] avg loss: 0.516200
[epoch 35, batch  2099] avg loss: 0.492627
[epoch 35, batch  2199] avg loss: 0.494198
[epoch 35, batch  2299] avg loss: 0.500883
[epoch 35, batch  2399] avg loss: 0.492370
[epoch 36, batch    99] avg loss: 0.502499
[epoch 36, batch   199] avg loss: 0.487791
[epoch 36, batch   299] avg loss: 0.485097
[epoch 36, batch   399] avg loss: 0.506280
[epoch 36, batch   499] avg loss: 0.494389
[epoch 36, batch   599] avg loss: 0.500502
[epoch 36, batch   699] avg loss: 0.525275
[epoch 36, batch   799] avg loss: 0.522795
[epoch 36, batch   899] avg loss: 0.507716
[epoch 36, batch   999] avg loss: 0.500685
[epoch 36, batch  1099] avg loss: 0.484538
[epoch 36, batch  1199] avg loss: 0.520450
[epoch 36, batch  1299] avg loss: 0.502946
[epoch 36, batch  1399] avg loss: 0.494755
[epoch 36, batch  1499] avg loss: 0.499857
[epoch 36, batch  1599] avg loss: 0.507344
[epoch 36, batch  1699] avg loss: 0.498240
[epoch 36, batch  1799] avg loss: 0.518467
[epoch 36, batch  1899] avg loss: 0.500613
[epoch 36, batch  1999] avg loss: 0.499614
[epoch 36, batch  2099] avg loss: 0.495503
[epoch 36, batch  2199] avg loss: 0.488205
[epoch 36, batch  2299] avg loss: 0.509394
[epoch 36, batch  2399] avg loss: 0.491269
[epoch 37, batch    99] avg loss: 0.538190
[epoch 37, batch   199] avg loss: 0.498575
[epoch 37, batch   299] avg loss: 0.532332
[epoch 37, batch   399] avg loss: 0.510833
[epoch 37, batch   499] avg loss: 0.477093
[epoch 37, batch   599] avg loss: 0.489121
[epoch 37, batch   699] avg loss: 0.512797
[epoch 37, batch   799] avg loss: 0.493555
[epoch 37, batch   899] avg loss: 0.487612
[epoch 37, batch   999] avg loss: 0.484266
[epoch 37, batch  1099] avg loss: 0.500417
[epoch 37, batch  1199] avg loss: 0.487388
[epoch 37, batch  1299] avg loss: 0.493177
[epoch 37, batch  1399] avg loss: 0.504788
[epoch 37, batch  1499] avg loss: 0.507660
[epoch 37, batch  1599] avg loss: 0.509748
[epoch 37, batch  1699] avg loss: 0.506720
[epoch 37, batch  1799] avg loss: 0.493784
[epoch 37, batch  1899] avg loss: 0.490145
[epoch 37, batch  1999] avg loss: 0.540972
[epoch 37, batch  2099] avg loss: 0.497213
[epoch 37, batch  2199] avg loss: 0.485689
[epoch 37, batch  2299] avg loss: 0.514191
[epoch 37, batch  2399] avg loss: 0.491479
[epoch 38, batch    99] avg loss: 0.497060
[epoch 38, batch   199] avg loss: 0.487874
[epoch 38, batch   299] avg loss: 0.478905
[epoch 38, batch   399] avg loss: 0.479512
[epoch 38, batch   499] avg loss: 0.492801
[epoch 38, batch   599] avg loss: 0.493016
[epoch 38, batch   699] avg loss: 0.511692
[epoch 38, batch   799] avg loss: 0.499210
[epoch 38, batch   899] avg loss: 0.491595
[epoch 38, batch   999] avg loss: 0.499468
[epoch 38, batch  1099] avg loss: 0.486366
[epoch 38, batch  1199] avg loss: 0.496930
[epoch 38, batch  1299] avg loss: 0.517455
[epoch 38, batch  1399] avg loss: 0.508701
[epoch 38, batch  1499] avg loss: 0.496176
[epoch 38, batch  1599] avg loss: 0.492513
[epoch 38, batch  1699] avg loss: 0.487504
[epoch 38, batch  1799] avg loss: 0.501983
[epoch 38, batch  1899] avg loss: 0.480050
[epoch 38, batch  1999] avg loss: 0.497231
[epoch 38, batch  2099] avg loss: 0.476195
[epoch 38, batch  2199] avg loss: 0.481415
[epoch 38, batch  2299] avg loss: 0.479837
[epoch 38, batch  2399] avg loss: 0.474403
[epoch 39, batch    99] avg loss: 0.523537
[epoch 39, batch   199] avg loss: 0.505125
[epoch 39, batch   299] avg loss: 0.497378
[epoch 39, batch   399] avg loss: 0.482176
[epoch 39, batch   499] avg loss: 0.482766
[epoch 39, batch   599] avg loss: 0.488777
[epoch 39, batch   699] avg loss: 0.477480
[epoch 39, batch   799] avg loss: 0.489272
[epoch 39, batch   899] avg loss: 0.485150
[epoch 39, batch   999] avg loss: 0.489369
[epoch 39, batch  1099] avg loss: 0.490325
[epoch 39, batch  1199] avg loss: 0.496436
[epoch 39, batch  1299] avg loss: 0.522350
[epoch 39, batch  1399] avg loss: 0.481151
[epoch 39, batch  1499] avg loss: 0.516072
[epoch 39, batch  1599] avg loss: 0.509124
[epoch 39, batch  1699] avg loss: 0.498142
[epoch 39, batch  1799] avg loss: 0.482724
[epoch 39, batch  1899] avg loss: 0.496328
[epoch 39, batch  1999] avg loss: 0.476715
[epoch 39, batch  2099] avg loss: 0.512391
[epoch 39, batch  2199] avg loss: 0.500547
[epoch 39, batch  2299] avg loss: 0.492014
[epoch 39, batch  2399] avg loss: 0.482223
Model saved to model/20200502-215833.pth.
accuracy/TriangPrismIsosc : 0.606
n_examples/TriangPrismIsosc : 500.0
accuracy/parallelepiped : 0.454
n_examples/parallelepiped : 500.0
accuracy/sphere : 1.0
n_examples/sphere : 102.0
accuracy/wire : 0.855
n_examples/wire : 200.0
accuracy/avg_geom : 0.6167434715821812
loss/validation_geom : 0.8547703146751392
accuracy/Au : 0.0
n_examples/Au : 1302.0
accuracy/SiN : 0.0
n_examples/SiN : 0.0
accuracy/SiO2 : 0.0
n_examples/SiO2 : 0.0
accuracy/avg_mat : 0.0
loss/validation_mat : 1.5234884827550839
MSE/ShortestDim : 1.151119005112421
MAE/ShortestDim : 0.6653539259129772
MSE/MiddleDim : 8.302753524663078
MAE/MiddleDim : 2.199449905418947
MSE/LongDim : 110.79769930275538
MAE/LongDim : 6.504135377945438
MSE/log Area/Vol : 11.299266059277794
MAE/log Area/Vol : 3.161479240921419
loss/validation_dim : 131.55083789180867
loss/validation : 133.9290966892389
Metrics saved to model/20200502-215833_metrics.csv.
Parsed 2604 rows from data/sim_train_spectrum_Au.
Parsed 2604 rows from data/sim_train_labels_Au.
Parsed 9765 rows from data/gen_spectrum_Au_00-of-16.
Parsed 9765 rows from data/gen_labels_Au_00-of-16.
Parsed 9765 rows from data/gen_spectrum_Au_01-of-16.
Parsed 9765 rows from data/gen_labels_Au_01-of-16.
Parsed 9765 rows from data/gen_spectrum_Au_02-of-16.
Parsed 9765 rows from data/gen_labels_Au_02-of-16.
Parsed 9765 rows from data/gen_spectrum_Au_03-of-16.
Parsed 9765 rows from data/gen_labels_Au_03-of-16.
Parsed 9765 rows from data/gen_spectrum_Au_04-of-16.
Parsed 9765 rows from data/gen_labels_Au_04-of-16.
Parsed 9765 rows from data/gen_spectrum_Au_05-of-16.
Parsed 9765 rows from data/gen_labels_Au_05-of-16.
Parsed 9765 rows from data/gen_spectrum_Au_06-of-16.
Parsed 9765 rows from data/gen_labels_Au_06-of-16.
Parsed 9765 rows from data/gen_spectrum_Au_07-of-16.
Parsed 9765 rows from data/gen_labels_Au_07-of-16.
Parsed 9765 rows from data/gen_spectrum_Au_08-of-16.
Parsed 9765 rows from data/gen_labels_Au_08-of-16.
Parsed 9765 rows from data/gen_spectrum_Au_09-of-16.
Parsed 9765 rows from data/gen_labels_Au_09-of-16.
Parsed 9765 rows from data/gen_spectrum_Au_10-of-16.
Parsed 9765 rows from data/gen_labels_Au_10-of-16.
Parsed 9765 rows from data/gen_spectrum_Au_11-of-16.
Parsed 9765 rows from data/gen_labels_Au_11-of-16.
Parsed 9765 rows from data/gen_spectrum_Au_12-of-16.
Parsed 9765 rows from data/gen_labels_Au_12-of-16.
Parsed 9765 rows from data/gen_spectrum_Au_13-of-16.
Parsed 9765 rows from data/gen_labels_Au_13-of-16.
Parsed 9765 rows from data/gen_spectrum_Au_14-of-16.
Parsed 9765 rows from data/gen_labels_Au_14-of-16.
Parsed 9765 rows from data/gen_spectrum_Au_15-of-16.
Parsed 9765 rows from data/gen_labels_Au_15-of-16.
Parsed 1302 rows from data/sim_validation_spectrum_Au.
Parsed 1302 rows from data/sim_validation_labels_Au.
Logging training progress to tensorboard dir runs/alexnet-Au-lr_0.000010-trainsize_158844-05_02_2020_21:59-joint.
[epoch 0, batch    99] avg loss: 1.386440
[epoch 0, batch   199] avg loss: 1.386100
[epoch 0, batch   299] avg loss: 1.386387
[epoch 0, batch   399] avg loss: 1.385953
[epoch 0, batch   499] avg loss: 1.385636
[epoch 0, batch   599] avg loss: 1.385429
[epoch 0, batch   699] avg loss: 1.384218
[epoch 0, batch   799] avg loss: 1.382598
[epoch 0, batch   899] avg loss: 1.378543
[epoch 0, batch   999] avg loss: 1.371022
[epoch 0, batch  1099] avg loss: 1.354961
[epoch 0, batch  1199] avg loss: 1.321681
[epoch 0, batch  1299] avg loss: 1.270466
[epoch 0, batch  1399] avg loss: 1.217123
[epoch 0, batch  1499] avg loss: 1.192680
[epoch 0, batch  1599] avg loss: 1.165039
[epoch 0, batch  1699] avg loss: 1.152199
[epoch 0, batch  1799] avg loss: 1.132645
[epoch 0, batch  1899] avg loss: 1.123472
[epoch 0, batch  1999] avg loss: 1.108165
[epoch 0, batch  2099] avg loss: 1.100210
[epoch 0, batch  2199] avg loss: 1.102966
[epoch 0, batch  2299] avg loss: 1.085513
[epoch 0, batch  2399] avg loss: 1.072293
[epoch 1, batch    99] avg loss: 1.057482
[epoch 1, batch   199] avg loss: 1.054152
[epoch 1, batch   299] avg loss: 1.052529
[epoch 1, batch   399] avg loss: 1.055296
[epoch 1, batch   499] avg loss: 1.039556
[epoch 1, batch   599] avg loss: 1.046851
[epoch 1, batch   699] avg loss: 1.029510
[epoch 1, batch   799] avg loss: 1.040731
[epoch 1, batch   899] avg loss: 1.029531
[epoch 1, batch   999] avg loss: 1.029635
[epoch 1, batch  1099] avg loss: 1.016732
[epoch 1, batch  1199] avg loss: 1.003278
[epoch 1, batch  1299] avg loss: 1.022428
[epoch 1, batch  1399] avg loss: 0.998441
[epoch 1, batch  1499] avg loss: 1.022578
[epoch 1, batch  1599] avg loss: 0.998774
[epoch 1, batch  1699] avg loss: 0.990836
[epoch 1, batch  1799] avg loss: 0.981006
[epoch 1, batch  1899] avg loss: 0.984174
[epoch 1, batch  1999] avg loss: 0.995254
[epoch 1, batch  2099] avg loss: 0.988440
[epoch 1, batch  2199] avg loss: 0.993144
[epoch 1, batch  2299] avg loss: 0.983065
[epoch 1, batch  2399] avg loss: 0.976205
[epoch 2, batch    99] avg loss: 0.972267
[epoch 2, batch   199] avg loss: 0.960841
[epoch 2, batch   299] avg loss: 0.965655
[epoch 2, batch   399] avg loss: 0.958603
[epoch 2, batch   499] avg loss: 0.957757
[epoch 2, batch   599] avg loss: 0.965805
[epoch 2, batch   699] avg loss: 0.962291
[epoch 2, batch   799] avg loss: 0.962437
[epoch 2, batch   899] avg loss: 0.968680
[epoch 2, batch   999] avg loss: 0.960060
[epoch 2, batch  1099] avg loss: 0.947771
[epoch 2, batch  1199] avg loss: 0.953903
[epoch 2, batch  1299] avg loss: 0.953945
[epoch 2, batch  1399] avg loss: 0.950217
[epoch 2, batch  1499] avg loss: 0.951500
[epoch 2, batch  1599] avg loss: 0.948015
[epoch 2, batch  1699] avg loss: 0.933071
[epoch 2, batch  1799] avg loss: 0.935090
[epoch 2, batch  1899] avg loss: 0.944082
[epoch 2, batch  1999] avg loss: 0.939097
[epoch 2, batch  2099] avg loss: 0.934310
[epoch 2, batch  2199] avg loss: 0.955795
[epoch 2, batch  2299] avg loss: 0.939713
[epoch 2, batch  2399] avg loss: 0.930315
[epoch 3, batch    99] avg loss: 0.931183
[epoch 3, batch   199] avg loss: 0.949275
[epoch 3, batch   299] avg loss: 0.934699
[epoch 3, batch   399] avg loss: 0.940397
[epoch 3, batch   499] avg loss: 0.926696
[epoch 3, batch   599] avg loss: 0.917247
[epoch 3, batch   699] avg loss: 0.926151
[epoch 3, batch   799] avg loss: 0.922794
[epoch 3, batch   899] avg loss: 0.913106
[epoch 3, batch   999] avg loss: 0.921053
[epoch 3, batch  1099] avg loss: 0.922543
[epoch 3, batch  1199] avg loss: 0.903450
[epoch 3, batch  1299] avg loss: 0.916134
[epoch 3, batch  1399] avg loss: 0.921310
[epoch 3, batch  1499] avg loss: 0.900854
[epoch 3, batch  1599] avg loss: 0.922595
[epoch 3, batch  1699] avg loss: 0.923224
[epoch 3, batch  1799] avg loss: 0.912471
[epoch 3, batch  1899] avg loss: 0.915524
[epoch 3, batch  1999] avg loss: 0.909391
[epoch 3, batch  2099] avg loss: 0.914380
[epoch 3, batch  2199] avg loss: 0.904423
[epoch 3, batch  2299] avg loss: 0.908371
[epoch 3, batch  2399] avg loss: 0.907712
[epoch 4, batch    99] avg loss: 0.899222
[epoch 4, batch   199] avg loss: 0.900048
[epoch 4, batch   299] avg loss: 0.901279
[epoch 4, batch   399] avg loss: 0.902519
[epoch 4, batch   499] avg loss: 0.897483
[epoch 4, batch   599] avg loss: 0.905808
[epoch 4, batch   699] avg loss: 0.900831
[epoch 4, batch   799] avg loss: 0.885870
[epoch 4, batch   899] avg loss: 0.887990
[epoch 4, batch   999] avg loss: 0.901590
[epoch 4, batch  1099] avg loss: 0.889880
[epoch 4, batch  1199] avg loss: 0.901619
[epoch 4, batch  1299] avg loss: 0.893156
[epoch 4, batch  1399] avg loss: 0.898694
[epoch 4, batch  1499] avg loss: 0.909779
[epoch 4, batch  1599] avg loss: 0.889965
[epoch 4, batch  1699] avg loss: 0.898250
[epoch 4, batch  1799] avg loss: 0.898986
[epoch 4, batch  1899] avg loss: 0.893071
[epoch 4, batch  1999] avg loss: 0.900691
[epoch 4, batch  2099] avg loss: 0.901312
[epoch 4, batch  2199] avg loss: 0.892908
[epoch 4, batch  2299] avg loss: 0.895285
[epoch 4, batch  2399] avg loss: 0.897090
[epoch 5, batch    99] avg loss: 0.900424
[epoch 5, batch   199] avg loss: 0.880905
[epoch 5, batch   299] avg loss: 0.887908
[epoch 5, batch   399] avg loss: 0.884905
[epoch 5, batch   499] avg loss: 0.882693
[epoch 5, batch   599] avg loss: 0.888924
[epoch 5, batch   699] avg loss: 0.881060
[epoch 5, batch   799] avg loss: 0.888653
[epoch 5, batch   899] avg loss: 0.878814
[epoch 5, batch   999] avg loss: 0.866255
[epoch 5, batch  1099] avg loss: 0.904068
[epoch 5, batch  1199] avg loss: 0.886120
[epoch 5, batch  1299] avg loss: 0.882860
[epoch 5, batch  1399] avg loss: 0.885467
[epoch 5, batch  1499] avg loss: 0.886145
[epoch 5, batch  1599] avg loss: 0.863075
[epoch 5, batch  1699] avg loss: 0.875004
[epoch 5, batch  1799] avg loss: 0.869203
[epoch 5, batch  1899] avg loss: 0.871289
[epoch 5, batch  1999] avg loss: 0.874811
[epoch 5, batch  2099] avg loss: 0.879383
[epoch 5, batch  2199] avg loss: 0.878010
[epoch 5, batch  2299] avg loss: 0.896004
[epoch 5, batch  2399] avg loss: 0.875140
[epoch 6, batch    99] avg loss: 0.865153
[epoch 6, batch   199] avg loss: 0.890660
[epoch 6, batch   299] avg loss: 0.875300
[epoch 6, batch   399] avg loss: 0.871012
[epoch 6, batch   499] avg loss: 0.871689
[epoch 6, batch   599] avg loss: 0.865721
[epoch 6, batch   699] avg loss: 0.875678
[epoch 6, batch   799] avg loss: 0.868694
[epoch 6, batch   899] avg loss: 0.855413
[epoch 6, batch   999] avg loss: 0.855920
[epoch 6, batch  1099] avg loss: 0.851326
[epoch 6, batch  1199] avg loss: 0.854407
[epoch 6, batch  1299] avg loss: 0.873596
[epoch 6, batch  1399] avg loss: 0.874118
[epoch 6, batch  1499] avg loss: 0.868017
[epoch 6, batch  1599] avg loss: 0.852744
[epoch 6, batch  1699] avg loss: 0.856621
[epoch 6, batch  1799] avg loss: 0.852445
[epoch 6, batch  1899] avg loss: 0.850738
[epoch 6, batch  1999] avg loss: 0.863670
[epoch 6, batch  2099] avg loss: 0.851242
[epoch 6, batch  2199] avg loss: 0.856975
[epoch 6, batch  2299] avg loss: 0.865796
[epoch 6, batch  2399] avg loss: 0.850160
[epoch 7, batch    99] avg loss: 0.846130
[epoch 7, batch   199] avg loss: 0.850364
[epoch 7, batch   299] avg loss: 0.845122
[epoch 7, batch   399] avg loss: 0.856531
[epoch 7, batch   499] avg loss: 0.864633
[epoch 7, batch   599] avg loss: 0.846303
[epoch 7, batch   699] avg loss: 0.842799
[epoch 7, batch   799] avg loss: 0.845206
[epoch 7, batch   899] avg loss: 0.853249
[epoch 7, batch   999] avg loss: 0.843336
[epoch 7, batch  1099] avg loss: 0.844815
[epoch 7, batch  1199] avg loss: 0.853283
[epoch 7, batch  1299] avg loss: 0.852703
[epoch 7, batch  1399] avg loss: 0.846978
[epoch 7, batch  1499] avg loss: 0.858835
[epoch 7, batch  1599] avg loss: 0.847124
[epoch 7, batch  1699] avg loss: 0.851393
[epoch 7, batch  1799] avg loss: 0.836940
[epoch 7, batch  1899] avg loss: 0.862188
[epoch 7, batch  1999] avg loss: 0.847996
[epoch 7, batch  2099] avg loss: 0.842506
[epoch 7, batch  2199] avg loss: 0.841500
[epoch 7, batch  2299] avg loss: 0.851715
[epoch 7, batch  2399] avg loss: 0.836934
[epoch 8, batch    99] avg loss: 0.846390
[epoch 8, batch   199] avg loss: 0.825868
[epoch 8, batch   299] avg loss: 0.846599
[epoch 8, batch   399] avg loss: 0.821022
[epoch 8, batch   499] avg loss: 0.844169
[epoch 8, batch   599] avg loss: 0.838446
[epoch 8, batch   699] avg loss: 0.834034
[epoch 8, batch   799] avg loss: 0.839144
[epoch 8, batch   899] avg loss: 0.835941
[epoch 8, batch   999] avg loss: 0.851418
[epoch 8, batch  1099] avg loss: 0.829959
[epoch 8, batch  1199] avg loss: 0.838836
[epoch 8, batch  1299] avg loss: 0.835852
[epoch 8, batch  1399] avg loss: 0.842386
[epoch 8, batch  1499] avg loss: 0.847759
[epoch 8, batch  1599] avg loss: 0.842291
[epoch 8, batch  1699] avg loss: 0.839021
[epoch 8, batch  1799] avg loss: 0.832353
[epoch 8, batch  1899] avg loss: 0.840050
[epoch 8, batch  1999] avg loss: 0.822645
[epoch 8, batch  2099] avg loss: 0.839035
[epoch 8, batch  2199] avg loss: 0.837357
[epoch 8, batch  2299] avg loss: 0.841081
[epoch 8, batch  2399] avg loss: 0.836771
[epoch 9, batch    99] avg loss: 0.825207
[epoch 9, batch   199] avg loss: 0.839053
[epoch 9, batch   299] avg loss: 0.840945
[epoch 9, batch   399] avg loss: 0.831085
[epoch 9, batch   499] avg loss: 0.823455
[epoch 9, batch   599] avg loss: 0.826497
[epoch 9, batch   699] avg loss: 0.836784
[epoch 9, batch   799] avg loss: 0.832814
[epoch 9, batch   899] avg loss: 0.849169
[epoch 9, batch   999] avg loss: 0.839354
[epoch 9, batch  1099] avg loss: 0.823619
[epoch 9, batch  1199] avg loss: 0.816527
[epoch 9, batch  1299] avg loss: 0.816453
[epoch 9, batch  1399] avg loss: 0.838145
[epoch 9, batch  1499] avg loss: 0.835991
[epoch 9, batch  1599] avg loss: 0.823769
[epoch 9, batch  1699] avg loss: 0.832667
[epoch 9, batch  1799] avg loss: 0.834451
[epoch 9, batch  1899] avg loss: 0.837893
[epoch 9, batch  1999] avg loss: 0.830536
[epoch 9, batch  2099] avg loss: 0.832105
[epoch 9, batch  2199] avg loss: 0.821578
[epoch 9, batch  2299] avg loss: 0.831013
[epoch 9, batch  2399] avg loss: 0.818647
[epoch 10, batch    99] avg loss: 0.823508
[epoch 10, batch   199] avg loss: 0.811761
[epoch 10, batch   299] avg loss: 0.823376
[epoch 10, batch   399] avg loss: 0.829948
[epoch 10, batch   499] avg loss: 0.816564
[epoch 10, batch   599] avg loss: 0.826186
[epoch 10, batch   699] avg loss: 0.829644
[epoch 10, batch   799] avg loss: 0.832119
[epoch 10, batch   899] avg loss: 0.819519
[epoch 10, batch   999] avg loss: 0.836701
[epoch 10, batch  1099] avg loss: 0.831395
[epoch 10, batch  1199] avg loss: 0.838844
[epoch 10, batch  1299] avg loss: 0.816399
[epoch 10, batch  1399] avg loss: 0.834942
[epoch 10, batch  1499] avg loss: 0.808120
[epoch 10, batch  1599] avg loss: 0.813760
[epoch 10, batch  1699] avg loss: 0.817086
[epoch 10, batch  1799] avg loss: 0.820869
[epoch 10, batch  1899] avg loss: 0.819305
[epoch 10, batch  1999] avg loss: 0.821850
[epoch 10, batch  2099] avg loss: 0.821714
[epoch 10, batch  2199] avg loss: 0.826485
[epoch 10, batch  2299] avg loss: 0.823353
[epoch 10, batch  2399] avg loss: 0.822582
[epoch 11, batch    99] avg loss: 0.810237
[epoch 11, batch   199] avg loss: 0.811769
[epoch 11, batch   299] avg loss: 0.815013
[epoch 11, batch   399] avg loss: 0.816581
[epoch 11, batch   499] avg loss: 0.819964
[epoch 11, batch   599] avg loss: 0.828062
[epoch 11, batch   699] avg loss: 0.810136
[epoch 11, batch   799] avg loss: 0.816776
[epoch 11, batch   899] avg loss: 0.820569
[epoch 11, batch   999] avg loss: 0.806031
[epoch 11, batch  1099] avg loss: 0.828212
[epoch 11, batch  1199] avg loss: 0.825236
[epoch 11, batch  1299] avg loss: 0.807485
[epoch 11, batch  1399] avg loss: 0.809827
[epoch 11, batch  1499] avg loss: 0.836634
[epoch 11, batch  1599] avg loss: 0.817114
[epoch 11, batch  1699] avg loss: 0.805704
[epoch 11, batch  1799] avg loss: 0.824838
[epoch 11, batch  1899] avg loss: 0.810200
[epoch 11, batch  1999] avg loss: 0.800178
[epoch 11, batch  2099] avg loss: 0.807899
[epoch 11, batch  2199] avg loss: 0.811428
[epoch 11, batch  2299] avg loss: 0.821933
[epoch 11, batch  2399] avg loss: 0.824503
[epoch 12, batch    99] avg loss: 0.804763
[epoch 12, batch   199] avg loss: 0.807705
[epoch 12, batch   299] avg loss: 0.822066
[epoch 12, batch   399] avg loss: 0.815674
[epoch 12, batch   499] avg loss: 0.821495
[epoch 12, batch   599] avg loss: 0.810078
[epoch 12, batch   699] avg loss: 0.805508
[epoch 12, batch   799] avg loss: 0.808729
[epoch 12, batch   899] avg loss: 0.805441
[epoch 12, batch   999] avg loss: 0.808985
[epoch 12, batch  1099] avg loss: 0.826669
[epoch 12, batch  1199] avg loss: 0.804289
[epoch 12, batch  1299] avg loss: 0.812750
[epoch 12, batch  1399] avg loss: 0.807608
[epoch 12, batch  1499] avg loss: 0.809500
[epoch 12, batch  1599] avg loss: 0.811550
[epoch 12, batch  1699] avg loss: 0.804866
[epoch 12, batch  1799] avg loss: 0.812869
[epoch 12, batch  1899] avg loss: 0.807773
[epoch 12, batch  1999] avg loss: 0.799792
[epoch 12, batch  2099] avg loss: 0.817890
[epoch 12, batch  2199] avg loss: 0.799492
[epoch 12, batch  2299] avg loss: 0.809918
[epoch 12, batch  2399] avg loss: 0.815098
[epoch 13, batch    99] avg loss: 0.815110
[epoch 13, batch   199] avg loss: 0.812225
[epoch 13, batch   299] avg loss: 0.794580
[epoch 13, batch   399] avg loss: 0.804329
[epoch 13, batch   499] avg loss: 0.806254
[epoch 13, batch   599] avg loss: 0.810886
[epoch 13, batch   699] avg loss: 0.811576
[epoch 13, batch   799] avg loss: 0.807500
[epoch 13, batch   899] avg loss: 0.805111
[epoch 13, batch   999] avg loss: 0.807596
[epoch 13, batch  1099] avg loss: 0.814065
[epoch 13, batch  1199] avg loss: 0.804415
[epoch 13, batch  1299] avg loss: 0.797314
[epoch 13, batch  1399] avg loss: 0.812755
[epoch 13, batch  1499] avg loss: 0.808393
[epoch 13, batch  1599] avg loss: 0.813836
[epoch 13, batch  1699] avg loss: 0.799993
[epoch 13, batch  1799] avg loss: 0.802626
[epoch 13, batch  1899] avg loss: 0.810152
[epoch 13, batch  1999] avg loss: 0.798359
[epoch 13, batch  2099] avg loss: 0.817867
[epoch 13, batch  2199] avg loss: 0.807316
[epoch 13, batch  2299] avg loss: 0.809563
[epoch 13, batch  2399] avg loss: 0.782266
[epoch 14, batch    99] avg loss: 0.806922
[epoch 14, batch   199] avg loss: 0.814032
[epoch 14, batch   299] avg loss: 0.805215
[epoch 14, batch   399] avg loss: 0.817355
[epoch 14, batch   499] avg loss: 0.808921
[epoch 14, batch   599] avg loss: 0.810939
[epoch 14, batch   699] avg loss: 0.801870
[epoch 14, batch   799] avg loss: 0.804444
[epoch 14, batch   899] avg loss: 0.806220
[epoch 14, batch   999] avg loss: 0.804298
[epoch 14, batch  1099] avg loss: 0.811785
[epoch 14, batch  1199] avg loss: 0.811226
[epoch 14, batch  1299] avg loss: 0.790954
[epoch 14, batch  1399] avg loss: 0.801533
[epoch 14, batch  1499] avg loss: 0.791112
[epoch 14, batch  1599] avg loss: 0.797185
[epoch 14, batch  1699] avg loss: 0.788390
[epoch 14, batch  1799] avg loss: 0.792034
[epoch 14, batch  1899] avg loss: 0.797251
[epoch 14, batch  1999] avg loss: 0.792251
[epoch 14, batch  2099] avg loss: 0.793644
[epoch 14, batch  2199] avg loss: 0.797442
[epoch 14, batch  2299] avg loss: 0.791036
[epoch 14, batch  2399] avg loss: 0.798215
[epoch 15, batch    99] avg loss: 0.808849
[epoch 15, batch   199] avg loss: 0.800106
[epoch 15, batch   299] avg loss: 0.797070
[epoch 15, batch   399] avg loss: 0.801188
[epoch 15, batch   499] avg loss: 0.794159
[epoch 15, batch   599] avg loss: 0.798769
[epoch 15, batch   699] avg loss: 0.794914
[epoch 15, batch   799] avg loss: 0.806396
[epoch 15, batch   899] avg loss: 0.799413
[epoch 15, batch   999] avg loss: 0.792656
[epoch 15, batch  1099] avg loss: 0.803347
[epoch 15, batch  1199] avg loss: 0.795179
[epoch 15, batch  1299] avg loss: 0.800117
[epoch 15, batch  1399] avg loss: 0.808051
[epoch 15, batch  1499] avg loss: 0.795102
[epoch 15, batch  1599] avg loss: 0.815405
[epoch 15, batch  1699] avg loss: 0.798747
[epoch 15, batch  1799] avg loss: 0.786963
[epoch 15, batch  1899] avg loss: 0.801935
[epoch 15, batch  1999] avg loss: 0.794803
[epoch 15, batch  2099] avg loss: 0.799021
[epoch 15, batch  2199] avg loss: 0.793917
[epoch 15, batch  2299] avg loss: 0.791644
[epoch 15, batch  2399] avg loss: 0.790646
[epoch 16, batch    99] avg loss: 0.801705
[epoch 16, batch   199] avg loss: 0.794422
[epoch 16, batch   299] avg loss: 0.787500
[epoch 16, batch   399] avg loss: 0.810842
[epoch 16, batch   499] avg loss: 0.795844
[epoch 16, batch   599] avg loss: 0.798073
[epoch 16, batch   699] avg loss: 0.794727
[epoch 16, batch   799] avg loss: 0.801189
[epoch 16, batch   899] avg loss: 0.794354
[epoch 16, batch   999] avg loss: 0.806920
[epoch 16, batch  1099] avg loss: 0.791427
[epoch 16, batch  1199] avg loss: 0.799478
[epoch 16, batch  1299] avg loss: 0.791875
[epoch 16, batch  1399] avg loss: 0.790856
[epoch 16, batch  1499] avg loss: 0.800451
[epoch 16, batch  1599] avg loss: 0.786262
[epoch 16, batch  1699] avg loss: 0.792702
[epoch 16, batch  1799] avg loss: 0.792611
[epoch 16, batch  1899] avg loss: 0.796203
[epoch 16, batch  1999] avg loss: 0.791593
[epoch 16, batch  2099] avg loss: 0.790598
[epoch 16, batch  2199] avg loss: 0.789200
[epoch 16, batch  2299] avg loss: 0.801940
[epoch 16, batch  2399] avg loss: 0.780802
[epoch 17, batch    99] avg loss: 0.790107
[epoch 17, batch   199] avg loss: 0.776181
[epoch 17, batch   299] avg loss: 0.780208
[epoch 17, batch   399] avg loss: 0.782941
[epoch 17, batch   499] avg loss: 0.796829
[epoch 17, batch   599] avg loss: 0.793940
[epoch 17, batch   699] avg loss: 0.800446
[epoch 17, batch   799] avg loss: 0.800815
[epoch 17, batch   899] avg loss: 0.792226
[epoch 17, batch   999] avg loss: 0.782939
[epoch 17, batch  1099] avg loss: 0.786374
[epoch 17, batch  1199] avg loss: 0.798985
[epoch 17, batch  1299] avg loss: 0.801749
[epoch 17, batch  1399] avg loss: 0.788581
[epoch 17, batch  1499] avg loss: 0.793312
[epoch 17, batch  1599] avg loss: 0.779635
[epoch 17, batch  1699] avg loss: 0.786698
[epoch 17, batch  1799] avg loss: 0.794442
[epoch 17, batch  1899] avg loss: 0.781786
[epoch 17, batch  1999] avg loss: 0.779406
[epoch 17, batch  2099] avg loss: 0.776134
[epoch 17, batch  2199] avg loss: 0.795261
[epoch 17, batch  2299] avg loss: 0.795476
[epoch 17, batch  2399] avg loss: 0.787942
[epoch 18, batch    99] avg loss: 0.790729
[epoch 18, batch   199] avg loss: 0.786445
[epoch 18, batch   299] avg loss: 0.794411
[epoch 18, batch   399] avg loss: 0.788340
[epoch 18, batch   499] avg loss: 0.795794
[epoch 18, batch   599] avg loss: 0.780280
[epoch 18, batch   699] avg loss: 0.776119
[epoch 18, batch   799] avg loss: 0.791669
[epoch 18, batch   899] avg loss: 0.777471
[epoch 18, batch   999] avg loss: 0.785564
[epoch 18, batch  1099] avg loss: 0.781536
[epoch 18, batch  1199] avg loss: 0.784751
[epoch 18, batch  1299] avg loss: 0.780680
[epoch 18, batch  1399] avg loss: 0.779808
[epoch 18, batch  1499] avg loss: 0.782520
[epoch 18, batch  1599] avg loss: 0.793351
[epoch 18, batch  1699] avg loss: 0.790597
[epoch 18, batch  1799] avg loss: 0.785498
[epoch 18, batch  1899] avg loss: 0.793015
[epoch 18, batch  1999] avg loss: 0.776455
[epoch 18, batch  2099] avg loss: 0.804659
[epoch 18, batch  2199] avg loss: 0.779089
[epoch 18, batch  2299] avg loss: 0.789636
[epoch 18, batch  2399] avg loss: 0.786333
[epoch 19, batch    99] avg loss: 0.784926
[epoch 19, batch   199] avg loss: 0.785685
[epoch 19, batch   299] avg loss: 0.784231
[epoch 19, batch   399] avg loss: 0.786091
[epoch 19, batch   499] avg loss: 0.787594
[epoch 19, batch   599] avg loss: 0.787138
[epoch 19, batch   699] avg loss: 0.781601
[epoch 19, batch   799] avg loss: 0.778016
[epoch 19, batch   899] avg loss: 0.792771
[epoch 19, batch   999] avg loss: 0.770057
[epoch 19, batch  1099] avg loss: 0.776057
[epoch 19, batch  1199] avg loss: 0.786175
[epoch 19, batch  1299] avg loss: 0.781858
[epoch 19, batch  1399] avg loss: 0.792066
[epoch 19, batch  1499] avg loss: 0.785917
[epoch 19, batch  1599] avg loss: 0.776940
[epoch 19, batch  1699] avg loss: 0.775590
[epoch 19, batch  1799] avg loss: 0.787584
[epoch 19, batch  1899] avg loss: 0.779479
[epoch 19, batch  1999] avg loss: 0.775780
[epoch 19, batch  2099] avg loss: 0.782384
[epoch 19, batch  2199] avg loss: 0.777580
[epoch 19, batch  2299] avg loss: 0.780954
[epoch 19, batch  2399] avg loss: 0.775010
[epoch 20, batch    99] avg loss: 0.761971
[epoch 20, batch   199] avg loss: 0.767651
[epoch 20, batch   299] avg loss: 0.791074
[epoch 20, batch   399] avg loss: 0.788084
[epoch 20, batch   499] avg loss: 0.775263
[epoch 20, batch   599] avg loss: 0.781301
[epoch 20, batch   699] avg loss: 0.765738
[epoch 20, batch   799] avg loss: 0.782537
[epoch 20, batch   899] avg loss: 0.776904
[epoch 20, batch   999] avg loss: 0.797389
[epoch 20, batch  1099] avg loss: 0.785550
[epoch 20, batch  1199] avg loss: 0.783648
[epoch 20, batch  1299] avg loss: 0.786606
[epoch 20, batch  1399] avg loss: 0.771484
[epoch 20, batch  1499] avg loss: 0.772701
[epoch 20, batch  1599] avg loss: 0.762322
[epoch 20, batch  1699] avg loss: 0.782314
[epoch 20, batch  1799] avg loss: 0.788235
[epoch 20, batch  1899] avg loss: 0.783457
[epoch 20, batch  1999] avg loss: 0.795710
[epoch 20, batch  2099] avg loss: 0.784291
[epoch 20, batch  2199] avg loss: 0.775448
[epoch 20, batch  2299] avg loss: 0.768853
[epoch 20, batch  2399] avg loss: 0.779050
[epoch 21, batch    99] avg loss: 0.774124
[epoch 21, batch   199] avg loss: 0.787875
[epoch 21, batch   299] avg loss: 0.766524
[epoch 21, batch   399] avg loss: 0.792946
[epoch 21, batch   499] avg loss: 0.765243
[epoch 21, batch   599] avg loss: 0.783450
[epoch 21, batch   699] avg loss: 0.769332
[epoch 21, batch   799] avg loss: 0.786390
[epoch 21, batch   899] avg loss: 0.789624
[epoch 21, batch   999] avg loss: 0.767646
[epoch 21, batch  1099] avg loss: 0.770124
[epoch 21, batch  1199] avg loss: 0.787152
[epoch 21, batch  1299] avg loss: 0.769696
[epoch 21, batch  1399] avg loss: 0.760170
[epoch 21, batch  1499] avg loss: 0.774482
[epoch 21, batch  1599] avg loss: 0.775363
[epoch 21, batch  1699] avg loss: 0.772297
[epoch 21, batch  1799] avg loss: 0.768991
[epoch 21, batch  1899] avg loss: 0.768560
[epoch 21, batch  1999] avg loss: 0.764075
[epoch 21, batch  2099] avg loss: 0.776558
[epoch 21, batch  2199] avg loss: 0.771950
[epoch 21, batch  2299] avg loss: 0.759084
[epoch 21, batch  2399] avg loss: 0.777730
[epoch 22, batch    99] avg loss: 0.774930
[epoch 22, batch   199] avg loss: 0.771345
[epoch 22, batch   299] avg loss: 0.754151
[epoch 22, batch   399] avg loss: 0.763369
[epoch 22, batch   499] avg loss: 0.755842
[epoch 22, batch   599] avg loss: 0.767018
[epoch 22, batch   699] avg loss: 0.782498
[epoch 22, batch   799] avg loss: 0.762353
[epoch 22, batch   899] avg loss: 0.785340
[epoch 22, batch   999] avg loss: 0.774977
[epoch 22, batch  1099] avg loss: 0.777089
[epoch 22, batch  1199] avg loss: 0.772740
[epoch 22, batch  1299] avg loss: 0.769336
[epoch 22, batch  1399] avg loss: 0.767562
[epoch 22, batch  1499] avg loss: 0.768767
[epoch 22, batch  1599] avg loss: 0.772173
[epoch 22, batch  1699] avg loss: 0.766614
[epoch 22, batch  1799] avg loss: 0.771359
[epoch 22, batch  1899] avg loss: 0.767881
[epoch 22, batch  1999] avg loss: 0.759251
[epoch 22, batch  2099] avg loss: 0.775955
[epoch 22, batch  2199] avg loss: 0.772947
[epoch 22, batch  2299] avg loss: 0.775564
[epoch 22, batch  2399] avg loss: 0.766052
[epoch 23, batch    99] avg loss: 0.763979
[epoch 23, batch   199] avg loss: 0.769657
[epoch 23, batch   299] avg loss: 0.757724
[epoch 23, batch   399] avg loss: 0.758398
[epoch 23, batch   499] avg loss: 0.775597
[epoch 23, batch   599] avg loss: 0.770815
[epoch 23, batch   699] avg loss: 0.770055
[epoch 23, batch   799] avg loss: 0.764889
[epoch 23, batch   899] avg loss: 0.768078
[epoch 23, batch   999] avg loss: 0.750647
[epoch 23, batch  1099] avg loss: 0.760514
[epoch 23, batch  1199] avg loss: 0.763662
[epoch 23, batch  1299] avg loss: 0.768143
[epoch 23, batch  1399] avg loss: 0.762635
[epoch 23, batch  1499] avg loss: 0.760095
[epoch 23, batch  1599] avg loss: 0.768101
[epoch 23, batch  1699] avg loss: 0.764071
[epoch 23, batch  1799] avg loss: 0.754121
[epoch 23, batch  1899] avg loss: 0.774147
[epoch 23, batch  1999] avg loss: 0.760679
[epoch 23, batch  2099] avg loss: 0.762946
[epoch 23, batch  2199] avg loss: 0.774713
[epoch 23, batch  2299] avg loss: 0.761973
[epoch 23, batch  2399] avg loss: 0.764283
[epoch 24, batch    99] avg loss: 0.751055
[epoch 24, batch   199] avg loss: 0.758948
[epoch 24, batch   299] avg loss: 0.753572
[epoch 24, batch   399] avg loss: 0.754301
[epoch 24, batch   499] avg loss: 0.764625
[epoch 24, batch   599] avg loss: 0.770176
[epoch 24, batch   699] avg loss: 0.777133
[epoch 24, batch   799] avg loss: 0.760372
[epoch 24, batch   899] avg loss: 0.764091
[epoch 24, batch   999] avg loss: 0.769018
[epoch 24, batch  1099] avg loss: 0.758416
[epoch 24, batch  1199] avg loss: 0.759810
[epoch 24, batch  1299] avg loss: 0.765275
[epoch 24, batch  1399] avg loss: 0.771572
[epoch 24, batch  1499] avg loss: 0.764627
[epoch 24, batch  1599] avg loss: 0.755669
[epoch 24, batch  1699] avg loss: 0.770845
[epoch 24, batch  1799] avg loss: 0.763754
[epoch 24, batch  1899] avg loss: 0.758960
[epoch 24, batch  1999] avg loss: 0.752048
[epoch 24, batch  2099] avg loss: 0.766458
[epoch 24, batch  2199] avg loss: 0.757187
[epoch 24, batch  2299] avg loss: 0.764176
[epoch 24, batch  2399] avg loss: 0.756602
[epoch 25, batch    99] avg loss: 0.763029
[epoch 25, batch   199] avg loss: 0.754332
[epoch 25, batch   299] avg loss: 0.746680
[epoch 25, batch   399] avg loss: 0.762531
[epoch 25, batch   499] avg loss: 0.761994
[epoch 25, batch   599] avg loss: 0.757314
[epoch 25, batch   699] avg loss: 0.759437
[epoch 25, batch   799] avg loss: 0.756467
[epoch 25, batch   899] avg loss: 0.753680
[epoch 25, batch   999] avg loss: 0.755361
[epoch 25, batch  1099] avg loss: 0.763214
[epoch 25, batch  1199] avg loss: 0.751448
[epoch 25, batch  1299] avg loss: 0.755983
[epoch 25, batch  1399] avg loss: 0.756162
[epoch 25, batch  1499] avg loss: 0.756119
[epoch 25, batch  1599] avg loss: 0.755326
[epoch 25, batch  1699] avg loss: 0.766400
[epoch 25, batch  1799] avg loss: 0.756037
[epoch 25, batch  1899] avg loss: 0.765689
[epoch 25, batch  1999] avg loss: 0.765774
[epoch 25, batch  2099] avg loss: 0.756920
[epoch 25, batch  2199] avg loss: 0.756447
[epoch 25, batch  2299] avg loss: 0.753274
[epoch 25, batch  2399] avg loss: 0.750087
[epoch 26, batch    99] avg loss: 0.763613
[epoch 26, batch   199] avg loss: 0.750276
[epoch 26, batch   299] avg loss: 0.758753
[epoch 26, batch   399] avg loss: 0.753573
[epoch 26, batch   499] avg loss: 0.759174
[epoch 26, batch   599] avg loss: 0.766213
[epoch 26, batch   699] avg loss: 0.750362
[epoch 26, batch   799] avg loss: 0.749828
[epoch 26, batch   899] avg loss: 0.756064
[epoch 26, batch   999] avg loss: 0.747165
[epoch 26, batch  1099] avg loss: 0.757869
[epoch 26, batch  1199] avg loss: 0.759038
[epoch 26, batch  1299] avg loss: 0.757521
[epoch 26, batch  1399] avg loss: 0.745825
[epoch 26, batch  1499] avg loss: 0.746824
[epoch 26, batch  1599] avg loss: 0.759202
[epoch 26, batch  1699] avg loss: 0.759354
[epoch 26, batch  1799] avg loss: 0.756763
[epoch 26, batch  1899] avg loss: 0.753361
[epoch 26, batch  1999] avg loss: 0.743719
[epoch 26, batch  2099] avg loss: 0.737585
[epoch 26, batch  2199] avg loss: 0.747135
[epoch 26, batch  2299] avg loss: 0.755793
[epoch 26, batch  2399] avg loss: 0.749724
[epoch 27, batch    99] avg loss: 0.760024
[epoch 27, batch   199] avg loss: 0.752337
[epoch 27, batch   299] avg loss: 0.758093
[epoch 27, batch   399] avg loss: 0.756788
[epoch 27, batch   499] avg loss: 0.749598
[epoch 27, batch   599] avg loss: 0.759254
[epoch 27, batch   699] avg loss: 0.740588
[epoch 27, batch   799] avg loss: 0.743756
[epoch 27, batch   899] avg loss: 0.763439
[epoch 27, batch   999] avg loss: 0.747974
[epoch 27, batch  1099] avg loss: 0.760759
[epoch 27, batch  1199] avg loss: 0.748944
[epoch 27, batch  1299] avg loss: 0.757377
[epoch 27, batch  1399] avg loss: 0.742512
[epoch 27, batch  1499] avg loss: 0.753292
[epoch 27, batch  1599] avg loss: 0.732689
[epoch 27, batch  1699] avg loss: 0.763289
[epoch 27, batch  1799] avg loss: 0.739326
[epoch 27, batch  1899] avg loss: 0.752206
[epoch 27, batch  1999] avg loss: 0.756349
[epoch 27, batch  2099] avg loss: 0.731110
[epoch 27, batch  2199] avg loss: 0.749393
[epoch 27, batch  2299] avg loss: 0.740231
[epoch 27, batch  2399] avg loss: 0.746167
[epoch 28, batch    99] avg loss: 0.744622
[epoch 28, batch   199] avg loss: 0.738991
[epoch 28, batch   299] avg loss: 0.746578
[epoch 28, batch   399] avg loss: 0.769373
[epoch 28, batch   499] avg loss: 0.735332
[epoch 28, batch   599] avg loss: 0.745619
[epoch 28, batch   699] avg loss: 0.768714
[epoch 28, batch   799] avg loss: 0.734319
[epoch 28, batch   899] avg loss: 0.754234
[epoch 28, batch   999] avg loss: 0.739057
[epoch 28, batch  1099] avg loss: 0.740954
[epoch 28, batch  1199] avg loss: 0.751499
[epoch 28, batch  1299] avg loss: 0.734221
[epoch 28, batch  1399] avg loss: 0.739688
[epoch 28, batch  1499] avg loss: 0.753401
[epoch 28, batch  1599] avg loss: 0.740163
[epoch 28, batch  1699] avg loss: 0.746145
[epoch 28, batch  1799] avg loss: 0.759974
[epoch 28, batch  1899] avg loss: 0.748485
[epoch 28, batch  1999] avg loss: 0.746090
[epoch 28, batch  2099] avg loss: 0.756222
[epoch 28, batch  2199] avg loss: 0.751739
[epoch 28, batch  2299] avg loss: 0.735021
[epoch 28, batch  2399] avg loss: 0.741573
[epoch 29, batch    99] avg loss: 0.748250
[epoch 29, batch   199] avg loss: 0.751339
[epoch 29, batch   299] avg loss: 0.743077
[epoch 29, batch   399] avg loss: 0.750317
[epoch 29, batch   499] avg loss: 0.750660
[epoch 29, batch   599] avg loss: 0.735342
[epoch 29, batch   699] avg loss: 0.748793
[epoch 29, batch   799] avg loss: 0.744936
[epoch 29, batch   899] avg loss: 0.740628
[epoch 29, batch   999] avg loss: 0.741874
[epoch 29, batch  1099] avg loss: 0.734784
[epoch 29, batch  1199] avg loss: 0.738050
[epoch 29, batch  1299] avg loss: 0.734643
[epoch 29, batch  1399] avg loss: 0.739150
[epoch 29, batch  1499] avg loss: 0.741544
[epoch 29, batch  1599] avg loss: 0.735814
[epoch 29, batch  1699] avg loss: 0.753767
[epoch 29, batch  1799] avg loss: 0.734970
[epoch 29, batch  1899] avg loss: 0.745343
[epoch 29, batch  1999] avg loss: 0.745861
[epoch 29, batch  2099] avg loss: 0.733544
[epoch 29, batch  2199] avg loss: 0.746595
[epoch 29, batch  2299] avg loss: 0.752061
[epoch 29, batch  2399] avg loss: 0.744856
[epoch 30, batch    99] avg loss: 0.741889
[epoch 30, batch   199] avg loss: 0.737467
[epoch 30, batch   299] avg loss: 0.760826
[epoch 30, batch   399] avg loss: 0.725989
[epoch 30, batch   499] avg loss: 0.739127
[epoch 30, batch   599] avg loss: 0.741679
[epoch 30, batch   699] avg loss: 0.745509
[epoch 30, batch   799] avg loss: 0.735903
[epoch 30, batch   899] avg loss: 0.744207
[epoch 30, batch   999] avg loss: 0.747384
[epoch 30, batch  1099] avg loss: 0.755806
[epoch 30, batch  1199] avg loss: 0.740939
[epoch 30, batch  1299] avg loss: 0.755078
[epoch 30, batch  1399] avg loss: 0.743727
[epoch 30, batch  1499] avg loss: 0.737104
[epoch 30, batch  1599] avg loss: 0.737355
[epoch 30, batch  1699] avg loss: 0.740604
[epoch 30, batch  1799] avg loss: 0.740699
[epoch 30, batch  1899] avg loss: 0.733331
[epoch 30, batch  1999] avg loss: 0.727702
[epoch 30, batch  2099] avg loss: 0.738494
[epoch 30, batch  2199] avg loss: 0.741438
[epoch 30, batch  2299] avg loss: 0.727023
[epoch 30, batch  2399] avg loss: 0.732228
[epoch 31, batch    99] avg loss: 0.732256
[epoch 31, batch   199] avg loss: 0.730804
[epoch 31, batch   299] avg loss: 0.733186
[epoch 31, batch   399] avg loss: 0.722833
[epoch 31, batch   499] avg loss: 0.736976
[epoch 31, batch   599] avg loss: 0.741579
[epoch 31, batch   699] avg loss: 0.739889
[epoch 31, batch   799] avg loss: 0.732219
[epoch 31, batch   899] avg loss: 0.741296
[epoch 31, batch   999] avg loss: 0.738341
[epoch 31, batch  1099] avg loss: 0.742599
[epoch 31, batch  1199] avg loss: 0.736881
[epoch 31, batch  1299] avg loss: 0.736063
[epoch 31, batch  1399] avg loss: 0.736314
[epoch 31, batch  1499] avg loss: 0.750607
[epoch 31, batch  1599] avg loss: 0.736390
[epoch 31, batch  1699] avg loss: 0.748466
[epoch 31, batch  1799] avg loss: 0.741129
[epoch 31, batch  1899] avg loss: 0.747086
[epoch 31, batch  1999] avg loss: 0.726528
[epoch 31, batch  2099] avg loss: 0.728426
[epoch 31, batch  2199] avg loss: 0.738145
[epoch 31, batch  2299] avg loss: 0.736629
[epoch 31, batch  2399] avg loss: 0.732819
[epoch 32, batch    99] avg loss: 0.728442
[epoch 32, batch   199] avg loss: 0.732903
[epoch 32, batch   299] avg loss: 0.738948
[epoch 32, batch   399] avg loss: 0.738737
[epoch 32, batch   499] avg loss: 0.736989
[epoch 32, batch   599] avg loss: 0.738383
[epoch 32, batch   699] avg loss: 0.733427
[epoch 32, batch   799] avg loss: 0.725845
[epoch 32, batch   899] avg loss: 0.743934
[epoch 32, batch   999] avg loss: 0.739263
[epoch 32, batch  1099] avg loss: 0.741297
[epoch 32, batch  1199] avg loss: 0.725663
[epoch 32, batch  1299] avg loss: 0.735186
[epoch 32, batch  1399] avg loss: 0.742867
[epoch 32, batch  1499] avg loss: 0.725623
[epoch 32, batch  1599] avg loss: 0.745490
[epoch 32, batch  1699] avg loss: 0.741482
[epoch 32, batch  1799] avg loss: 0.739100
[epoch 32, batch  1899] avg loss: 0.734273
[epoch 32, batch  1999] avg loss: 0.733321
[epoch 32, batch  2099] avg loss: 0.746691
[epoch 32, batch  2199] avg loss: 0.732157
[epoch 32, batch  2299] avg loss: 0.730006
[epoch 32, batch  2399] avg loss: 0.737156
[epoch 33, batch    99] avg loss: 0.732913
[epoch 33, batch   199] avg loss: 0.737233
[epoch 33, batch   299] avg loss: 0.722040
[epoch 33, batch   399] avg loss: 0.739377
[epoch 33, batch   499] avg loss: 0.731275
[epoch 33, batch   599] avg loss: 0.743358
[epoch 33, batch   699] avg loss: 0.734129
[epoch 33, batch   799] avg loss: 0.720188
[epoch 33, batch   899] avg loss: 0.738273
[epoch 33, batch   999] avg loss: 0.749262
[epoch 33, batch  1099] avg loss: 0.726228
[epoch 33, batch  1199] avg loss: 0.739482
[epoch 33, batch  1299] avg loss: 0.735524
[epoch 33, batch  1399] avg loss: 0.730099
[epoch 33, batch  1499] avg loss: 0.727131
[epoch 33, batch  1599] avg loss: 0.744735
[epoch 33, batch  1699] avg loss: 0.724832
[epoch 33, batch  1799] avg loss: 0.742641
[epoch 33, batch  1899] avg loss: 0.720982
[epoch 33, batch  1999] avg loss: 0.733632
[epoch 33, batch  2099] avg loss: 0.733216
[epoch 33, batch  2199] avg loss: 0.723390
[epoch 33, batch  2299] avg loss: 0.735254
[epoch 33, batch  2399] avg loss: 0.744140
[epoch 34, batch    99] avg loss: 0.735947
[epoch 34, batch   199] avg loss: 0.744295
[epoch 34, batch   299] avg loss: 0.734065
[epoch 34, batch   399] avg loss: 0.746659
[epoch 34, batch   499] avg loss: 0.727202
[epoch 34, batch   599] avg loss: 0.730625
[epoch 34, batch   699] avg loss: 0.733724
[epoch 34, batch   799] avg loss: 0.732055
[epoch 34, batch   899] avg loss: 0.715047
[epoch 34, batch   999] avg loss: 0.724250
[epoch 34, batch  1099] avg loss: 0.736680
[epoch 34, batch  1199] avg loss: 0.730030
[epoch 34, batch  1299] avg loss: 0.724813
[epoch 34, batch  1399] avg loss: 0.731511
[epoch 34, batch  1499] avg loss: 0.726140
[epoch 34, batch  1599] avg loss: 0.730856
[epoch 34, batch  1699] avg loss: 0.725454
[epoch 34, batch  1799] avg loss: 0.724075
[epoch 34, batch  1899] avg loss: 0.729134
[epoch 34, batch  1999] avg loss: 0.733806
[epoch 34, batch  2099] avg loss: 0.710232
[epoch 34, batch  2199] avg loss: 0.728448
[epoch 34, batch  2299] avg loss: 0.724849
[epoch 34, batch  2399] avg loss: 0.738263
[epoch 35, batch    99] avg loss: 0.738633
[epoch 35, batch   199] avg loss: 0.732462
[epoch 35, batch   299] avg loss: 0.736420
[epoch 35, batch   399] avg loss: 0.753153
[epoch 35, batch   499] avg loss: 0.724385
[epoch 35, batch   599] avg loss: 0.725983
[epoch 35, batch   699] avg loss: 0.728072
[epoch 35, batch   799] avg loss: 0.731908
[epoch 35, batch   899] avg loss: 0.728752
[epoch 35, batch   999] avg loss: 0.732388
[epoch 35, batch  1099] avg loss: 0.728226
[epoch 35, batch  1199] avg loss: 0.729806
[epoch 35, batch  1299] avg loss: 0.717739
[epoch 35, batch  1399] avg loss: 0.729599
[epoch 35, batch  1499] avg loss: 0.727892
[epoch 35, batch  1599] avg loss: 0.708116
[epoch 35, batch  1699] avg loss: 0.737390
[epoch 35, batch  1799] avg loss: 0.734319
[epoch 35, batch  1899] avg loss: 0.720088
[epoch 35, batch  1999] avg loss: 0.722861
[epoch 35, batch  2099] avg loss: 0.735413
[epoch 35, batch  2199] avg loss: 0.714271
[epoch 35, batch  2299] avg loss: 0.712311
[epoch 35, batch  2399] avg loss: 0.725847
[epoch 36, batch    99] avg loss: 0.725355
[epoch 36, batch   199] avg loss: 0.732578
[epoch 36, batch   299] avg loss: 0.723474
[epoch 36, batch   399] avg loss: 0.729882
[epoch 36, batch   499] avg loss: 0.725596
[epoch 36, batch   599] avg loss: 0.729000
[epoch 36, batch   699] avg loss: 0.726955
[epoch 36, batch   799] avg loss: 0.727195
[epoch 36, batch   899] avg loss: 0.737677
[epoch 36, batch   999] avg loss: 0.733335
[epoch 36, batch  1099] avg loss: 0.718683
[epoch 36, batch  1199] avg loss: 0.744190
[epoch 36, batch  1299] avg loss: 0.717437
[epoch 36, batch  1399] avg loss: 0.716351
[epoch 36, batch  1499] avg loss: 0.725750
[epoch 36, batch  1599] avg loss: 0.710293
[epoch 36, batch  1699] avg loss: 0.726659
[epoch 36, batch  1799] avg loss: 0.732757
[epoch 36, batch  1899] avg loss: 0.733405
[epoch 36, batch  1999] avg loss: 0.729923
[epoch 36, batch  2099] avg loss: 0.717523
[epoch 36, batch  2199] avg loss: 0.738227
[epoch 36, batch  2299] avg loss: 0.717473
[epoch 36, batch  2399] avg loss: 0.728162
[epoch 37, batch    99] avg loss: 0.727606
[epoch 37, batch   199] avg loss: 0.731169
[epoch 37, batch   299] avg loss: 0.719736
[epoch 37, batch   399] avg loss: 0.725764
[epoch 37, batch   499] avg loss: 0.732205
[epoch 37, batch   599] avg loss: 0.742191
[epoch 37, batch   699] avg loss: 0.728755
[epoch 37, batch   799] avg loss: 0.731306
[epoch 37, batch   899] avg loss: 0.722300
[epoch 37, batch   999] avg loss: 0.723252
[epoch 37, batch  1099] avg loss: 0.730532
[epoch 37, batch  1199] avg loss: 0.727242
[epoch 37, batch  1299] avg loss: 0.722299
[epoch 37, batch  1399] avg loss: 0.718236
[epoch 37, batch  1499] avg loss: 0.709366
[epoch 37, batch  1599] avg loss: 0.722018
[epoch 37, batch  1699] avg loss: 0.722897
[epoch 37, batch  1799] avg loss: 0.715804
[epoch 37, batch  1899] avg loss: 0.731258
[epoch 37, batch  1999] avg loss: 0.712034
[epoch 37, batch  2099] avg loss: 0.729299
[epoch 37, batch  2199] avg loss: 0.726107
[epoch 37, batch  2299] avg loss: 0.734064
[epoch 37, batch  2399] avg loss: 0.725371
[epoch 38, batch    99] avg loss: 0.725723
[epoch 38, batch   199] avg loss: 0.715750
[epoch 38, batch   299] avg loss: 0.720290
[epoch 38, batch   399] avg loss: 0.723169
[epoch 38, batch   499] avg loss: 0.733286
[epoch 38, batch   599] avg loss: 0.722936
[epoch 38, batch   699] avg loss: 0.716851
[epoch 38, batch   799] avg loss: 0.726173
[epoch 38, batch   899] avg loss: 0.721665
[epoch 38, batch   999] avg loss: 0.714658
[epoch 38, batch  1099] avg loss: 0.724713
[epoch 38, batch  1199] avg loss: 0.721062
[epoch 38, batch  1299] avg loss: 0.715768
[epoch 38, batch  1399] avg loss: 0.730811
[epoch 38, batch  1499] avg loss: 0.732593
[epoch 38, batch  1599] avg loss: 0.736594
[epoch 38, batch  1699] avg loss: 0.732748
[epoch 38, batch  1799] avg loss: 0.738889
[epoch 38, batch  1899] avg loss: 0.719347
[epoch 38, batch  1999] avg loss: 0.732896
[epoch 38, batch  2099] avg loss: 0.713748
[epoch 38, batch  2199] avg loss: 0.712277
[epoch 38, batch  2299] avg loss: 0.717087
[epoch 38, batch  2399] avg loss: 0.720117
[epoch 39, batch    99] avg loss: 0.729288
[epoch 39, batch   199] avg loss: 0.724979
[epoch 39, batch   299] avg loss: 0.729037
[epoch 39, batch   399] avg loss: 0.714020
[epoch 39, batch   499] avg loss: 0.709812
[epoch 39, batch   599] avg loss: 0.720793
[epoch 39, batch   699] avg loss: 0.732140
[epoch 39, batch   799] avg loss: 0.722739
[epoch 39, batch   899] avg loss: 0.711620
[epoch 39, batch   999] avg loss: 0.727761
[epoch 39, batch  1099] avg loss: 0.730305
[epoch 39, batch  1199] avg loss: 0.719753
[epoch 39, batch  1299] avg loss: 0.729108
[epoch 39, batch  1399] avg loss: 0.733041
[epoch 39, batch  1499] avg loss: 0.724642
[epoch 39, batch  1599] avg loss: 0.727469
[epoch 39, batch  1699] avg loss: 0.714631
[epoch 39, batch  1799] avg loss: 0.715543
[epoch 39, batch  1899] avg loss: 0.707090
[epoch 39, batch  1999] avg loss: 0.718361
[epoch 39, batch  2099] avg loss: 0.732520
[epoch 39, batch  2199] avg loss: 0.711082
[epoch 39, batch  2299] avg loss: 0.736344
[epoch 39, batch  2399] avg loss: 0.721362
Model saved to model/20200502-223025.pth.
accuracy/TriangPrismIsosc : 0.306
n_examples/TriangPrismIsosc : 500.0
accuracy/parallelepiped : 0.426
n_examples/parallelepiped : 500.0
accuracy/sphere : 1.0
n_examples/sphere : 102.0
accuracy/wire : 0.685
n_examples/wire : 200.0
accuracy/avg_geom : 0.4646697388632873
loss/validation_geom : 1.0074186500865743
accuracy/Au : 0.1989247311827957
n_examples/Au : 1302.0
accuracy/SiN : 0.0
n_examples/SiN : 0.0
accuracy/SiO2 : 0.0
n_examples/SiO2 : 0.0
accuracy/avg_mat : 0.1989247311827957
loss/validation_mat : 1.214826008141865
MSE/ShortestDim : 2.9028331182336293
MAE/ShortestDim : 1.4572043953586467
MSE/MiddleDim : 3.5681218793315272
MAE/MiddleDim : 1.1606396207794798
MSE/LongDim : 103.27840150519633
MAE/LongDim : 6.001473340387535
MSE/log Area/Vol : 15.551180603683635
MAE/log Area/Vol : 3.6618444264026646
loss/validation_dim : 125.30053710644512
loss/validation : 127.52278176467355
Metrics saved to model/20200502-223025_metrics.csv.
Parsed 2604 rows from data/sim_train_spectrum_Au.
Parsed 2604 rows from data/sim_train_labels_Au.
Parsed 9765 rows from data/gen_spectrum_Au_00-of-16.
Parsed 9765 rows from data/gen_labels_Au_00-of-16.
Parsed 9765 rows from data/gen_spectrum_Au_01-of-16.
Parsed 9765 rows from data/gen_labels_Au_01-of-16.
Parsed 9765 rows from data/gen_spectrum_Au_02-of-16.
Parsed 9765 rows from data/gen_labels_Au_02-of-16.
Parsed 9765 rows from data/gen_spectrum_Au_03-of-16.
Parsed 9765 rows from data/gen_labels_Au_03-of-16.
Parsed 9765 rows from data/gen_spectrum_Au_04-of-16.
Parsed 9765 rows from data/gen_labels_Au_04-of-16.
Parsed 9765 rows from data/gen_spectrum_Au_05-of-16.
Parsed 9765 rows from data/gen_labels_Au_05-of-16.
Parsed 9765 rows from data/gen_spectrum_Au_06-of-16.
Parsed 9765 rows from data/gen_labels_Au_06-of-16.
Parsed 9765 rows from data/gen_spectrum_Au_07-of-16.
Parsed 9765 rows from data/gen_labels_Au_07-of-16.
Parsed 9765 rows from data/gen_spectrum_Au_08-of-16.
Parsed 9765 rows from data/gen_labels_Au_08-of-16.
Parsed 9765 rows from data/gen_spectrum_Au_09-of-16.
Parsed 9765 rows from data/gen_labels_Au_09-of-16.
Parsed 9765 rows from data/gen_spectrum_Au_10-of-16.
Parsed 9765 rows from data/gen_labels_Au_10-of-16.
Parsed 9765 rows from data/gen_spectrum_Au_11-of-16.
Parsed 9765 rows from data/gen_labels_Au_11-of-16.
Parsed 9765 rows from data/gen_spectrum_Au_12-of-16.
Parsed 9765 rows from data/gen_labels_Au_12-of-16.
Parsed 9765 rows from data/gen_spectrum_Au_13-of-16.
Parsed 9765 rows from data/gen_labels_Au_13-of-16.
Parsed 9765 rows from data/gen_spectrum_Au_14-of-16.
Parsed 9765 rows from data/gen_labels_Au_14-of-16.
Parsed 9765 rows from data/gen_spectrum_Au_15-of-16.
Parsed 9765 rows from data/gen_labels_Au_15-of-16.
Parsed 1302 rows from data/sim_validation_spectrum_Au.
Parsed 1302 rows from data/sim_validation_labels_Au.
Logging training progress to tensorboard dir runs/alexnet-Au-lr_0.000050-trainsize_158844-05_02_2020_22:31-joint.
[epoch 0, batch    99] avg loss: 1.387694
[epoch 0, batch   199] avg loss: 1.384180
[epoch 0, batch   299] avg loss: 1.338112
[epoch 0, batch   399] avg loss: 1.175180
[epoch 0, batch   499] avg loss: 1.096427
[epoch 0, batch   599] avg loss: 1.057900
[epoch 0, batch   699] avg loss: 1.044502
[epoch 0, batch   799] avg loss: 1.018116
[epoch 0, batch   899] avg loss: 0.988892
[epoch 0, batch   999] avg loss: 0.980192
[epoch 0, batch  1099] avg loss: 0.977905
[epoch 0, batch  1199] avg loss: 0.954479
[epoch 0, batch  1299] avg loss: 0.949529
[epoch 0, batch  1399] avg loss: 0.944511
[epoch 0, batch  1499] avg loss: 0.934630
[epoch 0, batch  1599] avg loss: 0.926852
[epoch 0, batch  1699] avg loss: 0.938369
[epoch 0, batch  1799] avg loss: 0.931947
[epoch 0, batch  1899] avg loss: 0.921175
[epoch 0, batch  1999] avg loss: 0.930768
[epoch 0, batch  2099] avg loss: 0.923053
[epoch 0, batch  2199] avg loss: 0.921958
[epoch 0, batch  2299] avg loss: 0.916541
[epoch 0, batch  2399] avg loss: 0.893506
[epoch 1, batch    99] avg loss: 0.900302
[epoch 1, batch   199] avg loss: 0.869920
[epoch 1, batch   299] avg loss: 0.884484
[epoch 1, batch   399] avg loss: 0.878487
[epoch 1, batch   499] avg loss: 0.874281
[epoch 1, batch   599] avg loss: 0.883104
[epoch 1, batch   699] avg loss: 0.869579
[epoch 1, batch   799] avg loss: 0.869950
[epoch 1, batch   899] avg loss: 0.861786
[epoch 1, batch   999] avg loss: 0.857424
[epoch 1, batch  1099] avg loss: 0.852344
[epoch 1, batch  1199] avg loss: 0.847950
[epoch 1, batch  1299] avg loss: 0.853936
[epoch 1, batch  1399] avg loss: 0.858321
[epoch 1, batch  1499] avg loss: 0.846525
[epoch 1, batch  1599] avg loss: 0.853168
[epoch 1, batch  1699] avg loss: 0.832665
[epoch 1, batch  1799] avg loss: 0.831477
[epoch 1, batch  1899] avg loss: 0.852336
[epoch 1, batch  1999] avg loss: 0.847267
[epoch 1, batch  2099] avg loss: 0.829377
[epoch 1, batch  2199] avg loss: 0.831513
[epoch 1, batch  2299] avg loss: 0.838164
[epoch 1, batch  2399] avg loss: 0.826477
[epoch 2, batch    99] avg loss: 0.830435
[epoch 2, batch   199] avg loss: 0.808570
[epoch 2, batch   299] avg loss: 0.818544
[epoch 2, batch   399] avg loss: 0.819237
[epoch 2, batch   499] avg loss: 0.829767
[epoch 2, batch   599] avg loss: 0.822827
[epoch 2, batch   699] avg loss: 0.834495
[epoch 2, batch   799] avg loss: 0.814879
[epoch 2, batch   899] avg loss: 0.817443
[epoch 2, batch   999] avg loss: 0.821905
[epoch 2, batch  1099] avg loss: 0.807174
[epoch 2, batch  1199] avg loss: 0.818745
[epoch 2, batch  1299] avg loss: 0.802688
[epoch 2, batch  1399] avg loss: 0.793782
[epoch 2, batch  1499] avg loss: 0.796696
[epoch 2, batch  1599] avg loss: 0.797210
[epoch 2, batch  1699] avg loss: 0.808698
[epoch 2, batch  1799] avg loss: 0.784108
[epoch 2, batch  1899] avg loss: 0.799218
[epoch 2, batch  1999] avg loss: 0.800020
[epoch 2, batch  2099] avg loss: 0.805186
[epoch 2, batch  2199] avg loss: 0.793666
[epoch 2, batch  2299] avg loss: 0.804157
[epoch 2, batch  2399] avg loss: 0.800696
[epoch 3, batch    99] avg loss: 0.803322
[epoch 3, batch   199] avg loss: 0.813289
[epoch 3, batch   299] avg loss: 0.808959
[epoch 3, batch   399] avg loss: 0.785155
[epoch 3, batch   499] avg loss: 0.799768
[epoch 3, batch   599] avg loss: 0.781114
[epoch 3, batch   699] avg loss: 0.799872
[epoch 3, batch   799] avg loss: 0.827804
[epoch 3, batch   899] avg loss: 0.787214
[epoch 3, batch   999] avg loss: 0.790801
[epoch 3, batch  1099] avg loss: 0.800907
[epoch 3, batch  1199] avg loss: 0.781595
[epoch 3, batch  1299] avg loss: 0.794543
[epoch 3, batch  1399] avg loss: 0.801084
[epoch 3, batch  1499] avg loss: 0.803077
[epoch 3, batch  1599] avg loss: 0.782651
[epoch 3, batch  1699] avg loss: 0.796561
[epoch 3, batch  1799] avg loss: 0.776351
[epoch 3, batch  1899] avg loss: 0.784318
[epoch 3, batch  1999] avg loss: 0.779954
[epoch 3, batch  2099] avg loss: 0.782592
[epoch 3, batch  2199] avg loss: 0.784467
[epoch 3, batch  2299] avg loss: 0.786815
[epoch 3, batch  2399] avg loss: 0.779822
[epoch 4, batch    99] avg loss: 0.772000
[epoch 4, batch   199] avg loss: 0.770472
[epoch 4, batch   299] avg loss: 0.782851
[epoch 4, batch   399] avg loss: 0.780727
[epoch 4, batch   499] avg loss: 0.774514
[epoch 4, batch   599] avg loss: 0.763115
[epoch 4, batch   699] avg loss: 0.782188
[epoch 4, batch   799] avg loss: 0.764114
[epoch 4, batch   899] avg loss: 0.763222
[epoch 4, batch   999] avg loss: 0.778149
[epoch 4, batch  1099] avg loss: 0.784075
[epoch 4, batch  1199] avg loss: 0.800587
[epoch 4, batch  1299] avg loss: 0.780211
[epoch 4, batch  1399] avg loss: 0.777665
[epoch 4, batch  1499] avg loss: 0.771851
[epoch 4, batch  1599] avg loss: 0.771652
[epoch 4, batch  1699] avg loss: 0.776916
[epoch 4, batch  1799] avg loss: 0.777169
[epoch 4, batch  1899] avg loss: 0.768924
[epoch 4, batch  1999] avg loss: 0.773543
[epoch 4, batch  2099] avg loss: 0.765957
[epoch 4, batch  2199] avg loss: 0.751796
[epoch 4, batch  2299] avg loss: 0.761851
[epoch 4, batch  2399] avg loss: 0.778076
[epoch 5, batch    99] avg loss: 0.774240
[epoch 5, batch   199] avg loss: 0.747618
[epoch 5, batch   299] avg loss: 0.766073
[epoch 5, batch   399] avg loss: 0.757607
[epoch 5, batch   499] avg loss: 0.755740
[epoch 5, batch   599] avg loss: 0.747431
[epoch 5, batch   699] avg loss: 0.763096
[epoch 5, batch   799] avg loss: 0.772421
[epoch 5, batch   899] avg loss: 0.744010
[epoch 5, batch   999] avg loss: 0.803719
[epoch 5, batch  1099] avg loss: 0.772171
[epoch 5, batch  1199] avg loss: 0.768827
[epoch 5, batch  1299] avg loss: 0.768469
[epoch 5, batch  1399] avg loss: 0.759135
[epoch 5, batch  1499] avg loss: 0.772851
[epoch 5, batch  1599] avg loss: 0.760414
[epoch 5, batch  1699] avg loss: 0.753616
[epoch 5, batch  1799] avg loss: 0.764271
[epoch 5, batch  1899] avg loss: 0.751429
[epoch 5, batch  1999] avg loss: 0.752421
[epoch 5, batch  2099] avg loss: 0.757347
[epoch 5, batch  2199] avg loss: 0.741872
[epoch 5, batch  2299] avg loss: 0.741322
[epoch 5, batch  2399] avg loss: 0.765665
[epoch 6, batch    99] avg loss: 0.754925
[epoch 6, batch   199] avg loss: 0.758337
[epoch 6, batch   299] avg loss: 0.754163
[epoch 6, batch   399] avg loss: 0.754736
[epoch 6, batch   499] avg loss: 0.742080
[epoch 6, batch   599] avg loss: 0.751055
[epoch 6, batch   699] avg loss: 0.764393
[epoch 6, batch   799] avg loss: 0.751648
[epoch 6, batch   899] avg loss: 0.728666
[epoch 6, batch   999] avg loss: 0.774250
[epoch 6, batch  1099] avg loss: 0.755372
[epoch 6, batch  1199] avg loss: 0.755253
[epoch 6, batch  1299] avg loss: 0.756192
[epoch 6, batch  1399] avg loss: 0.742356
[epoch 6, batch  1499] avg loss: 0.747099
[epoch 6, batch  1599] avg loss: 0.749309
[epoch 6, batch  1699] avg loss: 0.743996
[epoch 6, batch  1799] avg loss: 0.766572
[epoch 6, batch  1899] avg loss: 0.751944
[epoch 6, batch  1999] avg loss: 0.760897
[epoch 6, batch  2099] avg loss: 0.758515
[epoch 6, batch  2199] avg loss: 0.741183
[epoch 6, batch  2299] avg loss: 0.756995
[epoch 6, batch  2399] avg loss: 0.755404
[epoch 7, batch    99] avg loss: 0.743196
[epoch 7, batch   199] avg loss: 0.740844
[epoch 7, batch   299] avg loss: 0.747189
[epoch 7, batch   399] avg loss: 0.743028
[epoch 7, batch   499] avg loss: 0.754825
[epoch 7, batch   599] avg loss: 0.730575
[epoch 7, batch   699] avg loss: 0.754372
[epoch 7, batch   799] avg loss: 0.735763
[epoch 7, batch   899] avg loss: 0.739005
[epoch 7, batch   999] avg loss: 0.730281
[epoch 7, batch  1099] avg loss: 0.738545
[epoch 7, batch  1199] avg loss: 0.732736
[epoch 7, batch  1299] avg loss: 0.741286
[epoch 7, batch  1399] avg loss: 0.748123
[epoch 7, batch  1499] avg loss: 0.760329
[epoch 7, batch  1599] avg loss: 0.729814
[epoch 7, batch  1699] avg loss: 0.730920
[epoch 7, batch  1799] avg loss: 0.743800
[epoch 7, batch  1899] avg loss: 0.754507
[epoch 7, batch  1999] avg loss: 0.746541
[epoch 7, batch  2099] avg loss: 0.746090
[epoch 7, batch  2199] avg loss: 0.745299
[epoch 7, batch  2299] avg loss: 0.721851
[epoch 7, batch  2399] avg loss: 0.732429
[epoch 8, batch    99] avg loss: 0.710161
[epoch 8, batch   199] avg loss: 0.735952
[epoch 8, batch   299] avg loss: 0.729541
[epoch 8, batch   399] avg loss: 0.742750
[epoch 8, batch   499] avg loss: 0.745052
[epoch 8, batch   599] avg loss: 0.740194
[epoch 8, batch   699] avg loss: 0.734284
[epoch 8, batch   799] avg loss: 0.727586
[epoch 8, batch   899] avg loss: 0.723405
[epoch 8, batch   999] avg loss: 0.738727
[epoch 8, batch  1099] avg loss: 0.753954
[epoch 8, batch  1199] avg loss: 0.718573
[epoch 8, batch  1299] avg loss: 0.740387
[epoch 8, batch  1399] avg loss: 0.723480
[epoch 8, batch  1499] avg loss: 0.731922
[epoch 8, batch  1599] avg loss: 0.732905
[epoch 8, batch  1699] avg loss: 0.735217
[epoch 8, batch  1799] avg loss: 0.732668
[epoch 8, batch  1899] avg loss: 0.729095
[epoch 8, batch  1999] avg loss: 0.770808
[epoch 8, batch  2099] avg loss: 0.735139
[epoch 8, batch  2199] avg loss: 0.727577
[epoch 8, batch  2299] avg loss: 0.740634
[epoch 8, batch  2399] avg loss: 0.743207
[epoch 9, batch    99] avg loss: 0.725024
[epoch 9, batch   199] avg loss: 0.719211
[epoch 9, batch   299] avg loss: 0.746880
[epoch 9, batch   399] avg loss: 0.741938
[epoch 9, batch   499] avg loss: 0.720094
[epoch 9, batch   599] avg loss: 0.735345
[epoch 9, batch   699] avg loss: 0.730682
[epoch 9, batch   799] avg loss: 0.723159
[epoch 9, batch   899] avg loss: 0.734519
[epoch 9, batch   999] avg loss: 0.714844
[epoch 9, batch  1099] avg loss: 0.719183
[epoch 9, batch  1199] avg loss: 0.722567
[epoch 9, batch  1299] avg loss: 0.732129
[epoch 9, batch  1399] avg loss: 0.728124
[epoch 9, batch  1499] avg loss: 0.726109
[epoch 9, batch  1599] avg loss: 0.714835
[epoch 9, batch  1699] avg loss: 0.717201
[epoch 9, batch  1799] avg loss: 0.738529
[epoch 9, batch  1899] avg loss: 0.740744
[epoch 9, batch  1999] avg loss: 0.712902
[epoch 9, batch  2099] avg loss: 0.721384
[epoch 9, batch  2199] avg loss: 0.769843
[epoch 9, batch  2299] avg loss: 0.732494
[epoch 9, batch  2399] avg loss: 0.718995
[epoch 10, batch    99] avg loss: 0.718090
[epoch 10, batch   199] avg loss: 0.724993
[epoch 10, batch   299] avg loss: 0.736567
[epoch 10, batch   399] avg loss: 0.720061
[epoch 10, batch   499] avg loss: 0.708009
[epoch 10, batch   599] avg loss: 0.710824
[epoch 10, batch   699] avg loss: 0.743842
[epoch 10, batch   799] avg loss: 0.719828
[epoch 10, batch   899] avg loss: 0.720843
[epoch 10, batch   999] avg loss: 0.726273
[epoch 10, batch  1099] avg loss: 0.705476
[epoch 10, batch  1199] avg loss: 0.727733
[epoch 10, batch  1299] avg loss: 0.737145
[epoch 10, batch  1399] avg loss: 0.711627
[epoch 10, batch  1499] avg loss: 0.725746
[epoch 10, batch  1599] avg loss: 0.720349
[epoch 10, batch  1699] avg loss: 0.725197
[epoch 10, batch  1799] avg loss: 0.725009
[epoch 10, batch  1899] avg loss: 0.697311
[epoch 10, batch  1999] avg loss: 0.723780
[epoch 10, batch  2099] avg loss: 0.709483
[epoch 10, batch  2199] avg loss: 0.719860
[epoch 10, batch  2299] avg loss: 0.731105
[epoch 10, batch  2399] avg loss: 0.732094
[epoch 11, batch    99] avg loss: 0.721792
[epoch 11, batch   199] avg loss: 0.701254
[epoch 11, batch   299] avg loss: 0.716698
[epoch 11, batch   399] avg loss: 0.762635
[epoch 11, batch   499] avg loss: 0.716066
[epoch 11, batch   599] avg loss: 0.706465
[epoch 11, batch   699] avg loss: 0.716768
[epoch 11, batch   799] avg loss: 0.708959
[epoch 11, batch   899] avg loss: 0.702270
[epoch 11, batch   999] avg loss: 0.697187
[epoch 11, batch  1099] avg loss: 0.729440
[epoch 11, batch  1199] avg loss: 0.738073
[epoch 11, batch  1299] avg loss: 0.714625
[epoch 11, batch  1399] avg loss: 0.707967
[epoch 11, batch  1499] avg loss: 0.715732
[epoch 11, batch  1599] avg loss: 0.695811
[epoch 11, batch  1699] avg loss: 0.739001
[epoch 11, batch  1799] avg loss: 0.703812
[epoch 11, batch  1899] avg loss: 0.717005
[epoch 11, batch  1999] avg loss: 0.714180
[epoch 11, batch  2099] avg loss: 0.712133
[epoch 11, batch  2199] avg loss: 0.712725
[epoch 11, batch  2299] avg loss: 0.725892
[epoch 11, batch  2399] avg loss: 0.745413
[epoch 12, batch    99] avg loss: 0.706737
[epoch 12, batch   199] avg loss: 0.721686
[epoch 12, batch   299] avg loss: 0.720714
[epoch 12, batch   399] avg loss: 0.716963
[epoch 12, batch   499] avg loss: 0.718910
[epoch 12, batch   599] avg loss: 0.724252
[epoch 12, batch   699] avg loss: 0.718373
[epoch 12, batch   799] avg loss: 0.722544
[epoch 12, batch   899] avg loss: 0.723152
[epoch 12, batch   999] avg loss: 0.710770
[epoch 12, batch  1099] avg loss: 0.724866
[epoch 12, batch  1199] avg loss: 0.701440
[epoch 12, batch  1299] avg loss: 0.706109
[epoch 12, batch  1399] avg loss: 0.700680
[epoch 12, batch  1499] avg loss: 0.700909
[epoch 12, batch  1599] avg loss: 0.704201
[epoch 12, batch  1699] avg loss: 0.710455
[epoch 12, batch  1799] avg loss: 0.707616
[epoch 12, batch  1899] avg loss: 0.696428
[epoch 12, batch  1999] avg loss: 0.723354
[epoch 12, batch  2099] avg loss: 0.719449
[epoch 12, batch  2199] avg loss: 0.697991
[epoch 12, batch  2299] avg loss: 0.699189
[epoch 12, batch  2399] avg loss: 0.713063
[epoch 13, batch    99] avg loss: 0.715227
[epoch 13, batch   199] avg loss: 0.739685
[epoch 13, batch   299] avg loss: 0.715338
[epoch 13, batch   399] avg loss: 0.695201
[epoch 13, batch   499] avg loss: 0.704399
[epoch 13, batch   599] avg loss: 0.684141
[epoch 13, batch   699] avg loss: 0.694027
[epoch 13, batch   799] avg loss: 0.693913
[epoch 13, batch   899] avg loss: 0.729398
[epoch 13, batch   999] avg loss: 0.704992
[epoch 13, batch  1099] avg loss: 0.697655
[epoch 13, batch  1199] avg loss: 0.708390
[epoch 13, batch  1299] avg loss: 0.711977
[epoch 13, batch  1399] avg loss: 0.710841
[epoch 13, batch  1499] avg loss: 0.676622
[epoch 13, batch  1599] avg loss: 0.730021
[epoch 13, batch  1699] avg loss: 0.685975
[epoch 13, batch  1799] avg loss: 0.710605
[epoch 13, batch  1899] avg loss: 0.700783
[epoch 13, batch  1999] avg loss: 0.682359
[epoch 13, batch  2099] avg loss: 0.707386
[epoch 13, batch  2199] avg loss: 0.716594
[epoch 13, batch  2299] avg loss: 0.716412
[epoch 13, batch  2399] avg loss: 0.698467
[epoch 14, batch    99] avg loss: 0.704492
[epoch 14, batch   199] avg loss: 0.685423
[epoch 14, batch   299] avg loss: 0.699970
[epoch 14, batch   399] avg loss: 0.698503
[epoch 14, batch   499] avg loss: 0.701249
[epoch 14, batch   599] avg loss: 0.709981
[epoch 14, batch   699] avg loss: 0.712132
[epoch 14, batch   799] avg loss: 0.724363
[epoch 14, batch   899] avg loss: 0.708084
[epoch 14, batch   999] avg loss: 0.696815
[epoch 14, batch  1099] avg loss: 0.692482
[epoch 14, batch  1199] avg loss: 0.724236
[epoch 14, batch  1299] avg loss: 0.705989
[epoch 14, batch  1399] avg loss: 0.686759
[epoch 14, batch  1499] avg loss: 0.695169
[epoch 14, batch  1599] avg loss: 0.718404
[epoch 14, batch  1699] avg loss: 0.697533
[epoch 14, batch  1799] avg loss: 0.707991
[epoch 14, batch  1899] avg loss: 0.713908
[epoch 14, batch  1999] avg loss: 0.704042
[epoch 14, batch  2099] avg loss: 0.691556
[epoch 14, batch  2199] avg loss: 0.715671
[epoch 14, batch  2299] avg loss: 0.694399
[epoch 14, batch  2399] avg loss: 0.701696
[epoch 15, batch    99] avg loss: 0.688979
[epoch 15, batch   199] avg loss: 0.686742
[epoch 15, batch   299] avg loss: 0.713509
[epoch 15, batch   399] avg loss: 0.697106
[epoch 15, batch   499] avg loss: 0.715410
[epoch 15, batch   599] avg loss: 0.686204
[epoch 15, batch   699] avg loss: 0.710559
[epoch 15, batch   799] avg loss: 0.699643
[epoch 15, batch   899] avg loss: 0.709817
[epoch 15, batch   999] avg loss: 0.713882
[epoch 15, batch  1099] avg loss: 0.675448
[epoch 15, batch  1199] avg loss: 0.713370
[epoch 15, batch  1299] avg loss: 0.696086
[epoch 15, batch  1399] avg loss: 0.702919
[epoch 15, batch  1499] avg loss: 0.709363
[epoch 15, batch  1599] avg loss: 0.697277
[epoch 15, batch  1699] avg loss: 0.706114
[epoch 15, batch  1799] avg loss: 0.709266
[epoch 15, batch  1899] avg loss: 0.683594
[epoch 15, batch  1999] avg loss: 0.702802
[epoch 15, batch  2099] avg loss: 0.680461
[epoch 15, batch  2199] avg loss: 0.691677
[epoch 15, batch  2299] avg loss: 0.688671
[epoch 15, batch  2399] avg loss: 0.691019
[epoch 16, batch    99] avg loss: 0.684463
[epoch 16, batch   199] avg loss: 0.672677
[epoch 16, batch   299] avg loss: 0.696168
[epoch 16, batch   399] avg loss: 0.666630
[epoch 16, batch   499] avg loss: 0.693970
[epoch 16, batch   599] avg loss: 0.689794
[epoch 16, batch   699] avg loss: 0.671572
[epoch 16, batch   799] avg loss: 0.714163
[epoch 16, batch   899] avg loss: 0.682138
[epoch 16, batch   999] avg loss: 0.701825
[epoch 16, batch  1099] avg loss: 0.696816
[epoch 16, batch  1199] avg loss: 0.700865
[epoch 16, batch  1299] avg loss: 0.693970
[epoch 16, batch  1399] avg loss: 0.693612
[epoch 16, batch  1499] avg loss: 0.669815
[epoch 16, batch  1599] avg loss: 0.691458
[epoch 16, batch  1699] avg loss: 0.683142
[epoch 16, batch  1799] avg loss: 0.670947
[epoch 16, batch  1899] avg loss: 0.685578
[epoch 16, batch  1999] avg loss: 0.694885
[epoch 16, batch  2099] avg loss: 0.679806
[epoch 16, batch  2199] avg loss: 0.688646
[epoch 16, batch  2299] avg loss: 0.696404
[epoch 16, batch  2399] avg loss: 0.698134
[epoch 17, batch    99] avg loss: 0.686774
[epoch 17, batch   199] avg loss: 0.689124
[epoch 17, batch   299] avg loss: 0.697872
[epoch 17, batch   399] avg loss: 0.702017
[epoch 17, batch   499] avg loss: 0.691515
[epoch 17, batch   599] avg loss: 0.692970
[epoch 17, batch   699] avg loss: 0.681936
[epoch 17, batch   799] avg loss: 0.694124
[epoch 17, batch   899] avg loss: 0.688965
[epoch 17, batch   999] avg loss: 0.676186
[epoch 17, batch  1099] avg loss: 0.689840
[epoch 17, batch  1199] avg loss: 0.697788
[epoch 17, batch  1299] avg loss: 0.678572
[epoch 17, batch  1399] avg loss: 0.692387
[epoch 17, batch  1499] avg loss: 0.665055
[epoch 17, batch  1599] avg loss: 0.678915
[epoch 17, batch  1699] avg loss: 0.675985
[epoch 17, batch  1799] avg loss: 0.688391
[epoch 17, batch  1899] avg loss: 0.676938
[epoch 17, batch  1999] avg loss: 0.685722
[epoch 17, batch  2099] avg loss: 0.676830
[epoch 17, batch  2199] avg loss: 0.666362
[epoch 17, batch  2299] avg loss: 0.695827
[epoch 17, batch  2399] avg loss: 0.688060
[epoch 18, batch    99] avg loss: 0.689677
[epoch 18, batch   199] avg loss: 0.672807
[epoch 18, batch   299] avg loss: 0.682713
[epoch 18, batch   399] avg loss: 0.681014
[epoch 18, batch   499] avg loss: 0.702186
[epoch 18, batch   599] avg loss: 0.666937
[epoch 18, batch   699] avg loss: 0.682146
[epoch 18, batch   799] avg loss: 0.703411
[epoch 18, batch   899] avg loss: 0.692780
[epoch 18, batch   999] avg loss: 0.693614
[epoch 18, batch  1099] avg loss: 0.691389
[epoch 18, batch  1199] avg loss: 0.683846
[epoch 18, batch  1299] avg loss: 0.681183
[epoch 18, batch  1399] avg loss: 0.685133
[epoch 18, batch  1499] avg loss: 0.673095
[epoch 18, batch  1599] avg loss: 0.683732
[epoch 18, batch  1699] avg loss: 0.680582
[epoch 18, batch  1799] avg loss: 0.696005
[epoch 18, batch  1899] avg loss: 0.677221
[epoch 18, batch  1999] avg loss: 0.673528
[epoch 18, batch  2099] avg loss: 0.652755
[epoch 18, batch  2199] avg loss: 0.680960
[epoch 18, batch  2299] avg loss: 0.747095
[epoch 18, batch  2399] avg loss: 0.685099
[epoch 19, batch    99] avg loss: 0.681096
[epoch 19, batch   199] avg loss: 0.669937
[epoch 19, batch   299] avg loss: 0.679409
[epoch 19, batch   399] avg loss: 0.674766
[epoch 19, batch   499] avg loss: 0.699817
[epoch 19, batch   599] avg loss: 0.671218
[epoch 19, batch   699] avg loss: 0.665467
[epoch 19, batch   799] avg loss: 0.678993
[epoch 19, batch   899] avg loss: 0.681403
[epoch 19, batch   999] avg loss: 0.684546
[epoch 19, batch  1099] avg loss: 0.686888
[epoch 19, batch  1199] avg loss: 0.685582
[epoch 19, batch  1299] avg loss: 0.674103
[epoch 19, batch  1399] avg loss: 0.683989
[epoch 19, batch  1499] avg loss: 0.675799
[epoch 19, batch  1599] avg loss: 0.663757
[epoch 19, batch  1699] avg loss: 0.698395
[epoch 19, batch  1799] avg loss: 0.690543
[epoch 19, batch  1899] avg loss: 0.685100
[epoch 19, batch  1999] avg loss: 0.663985
[epoch 19, batch  2099] avg loss: 0.684040
[epoch 19, batch  2199] avg loss: 0.679683
[epoch 19, batch  2299] avg loss: 0.651831
[epoch 19, batch  2399] avg loss: 0.690313
[epoch 20, batch    99] avg loss: 0.675534
[epoch 20, batch   199] avg loss: 0.680410
[epoch 20, batch   299] avg loss: 0.668009
[epoch 20, batch   399] avg loss: 0.660401
[epoch 20, batch   499] avg loss: 0.661042
[epoch 20, batch   599] avg loss: 0.657715
[epoch 20, batch   699] avg loss: 0.666142
[epoch 20, batch   799] avg loss: 0.677350
[epoch 20, batch   899] avg loss: 0.675181
[epoch 20, batch   999] avg loss: 0.668848
[epoch 20, batch  1099] avg loss: 0.674700
[epoch 20, batch  1199] avg loss: 0.672293
[epoch 20, batch  1299] avg loss: 0.675615
[epoch 20, batch  1399] avg loss: 0.670396
[epoch 20, batch  1499] avg loss: 0.655008
[epoch 20, batch  1599] avg loss: 0.671932
[epoch 20, batch  1699] avg loss: 0.690248
[epoch 20, batch  1799] avg loss: 0.675780
[epoch 20, batch  1899] avg loss: 0.688852
[epoch 20, batch  1999] avg loss: 0.681141
[epoch 20, batch  2099] avg loss: 0.686785
[epoch 20, batch  2199] avg loss: 0.664044
[epoch 20, batch  2299] avg loss: 0.669818
[epoch 20, batch  2399] avg loss: 0.666453
[epoch 21, batch    99] avg loss: 0.661004
[epoch 21, batch   199] avg loss: 0.661218
[epoch 21, batch   299] avg loss: 0.690447
[epoch 21, batch   399] avg loss: 0.658522
[epoch 21, batch   499] avg loss: 0.668486
[epoch 21, batch   599] avg loss: 0.686911
[epoch 21, batch   699] avg loss: 0.672518
[epoch 21, batch   799] avg loss: 0.674626
[epoch 21, batch   899] avg loss: 0.682916
[epoch 21, batch   999] avg loss: 0.651664
[epoch 21, batch  1099] avg loss: 0.654743
[epoch 21, batch  1199] avg loss: 0.679114
[epoch 21, batch  1299] avg loss: 0.678821
[epoch 21, batch  1399] avg loss: 0.658404
[epoch 21, batch  1499] avg loss: 0.698725
[epoch 21, batch  1599] avg loss: 0.687358
[epoch 21, batch  1699] avg loss: 0.675738
[epoch 21, batch  1799] avg loss: 0.666037
[epoch 21, batch  1899] avg loss: 0.701855
[epoch 21, batch  1999] avg loss: 0.653943
[epoch 21, batch  2099] avg loss: 0.656590
[epoch 21, batch  2199] avg loss: 0.670169
[epoch 21, batch  2299] avg loss: 0.665811
[epoch 21, batch  2399] avg loss: 0.670027
[epoch 22, batch    99] avg loss: 0.667532
[epoch 22, batch   199] avg loss: 0.677884
[epoch 22, batch   299] avg loss: 0.670043
[epoch 22, batch   399] avg loss: 0.656680
[epoch 22, batch   499] avg loss: 0.664992
[epoch 22, batch   599] avg loss: 0.671241
[epoch 22, batch   699] avg loss: 0.667378
[epoch 22, batch   799] avg loss: 0.665417
[epoch 22, batch   899] avg loss: 0.677276
[epoch 22, batch   999] avg loss: 0.657181
[epoch 22, batch  1099] avg loss: 0.674925
[epoch 22, batch  1199] avg loss: 0.661996
[epoch 22, batch  1299] avg loss: 0.657955
[epoch 22, batch  1399] avg loss: 0.654377
[epoch 22, batch  1499] avg loss: 0.667882
[epoch 22, batch  1599] avg loss: 0.664048
[epoch 22, batch  1699] avg loss: 0.670432
[epoch 22, batch  1799] avg loss: 0.677365
[epoch 22, batch  1899] avg loss: 0.651021
[epoch 22, batch  1999] avg loss: 0.672291
[epoch 22, batch  2099] avg loss: 0.673579
[epoch 22, batch  2199] avg loss: 0.654252
[epoch 22, batch  2299] avg loss: 0.667601
[epoch 22, batch  2399] avg loss: 0.663776
[epoch 23, batch    99] avg loss: 0.657955
[epoch 23, batch   199] avg loss: 0.655171
[epoch 23, batch   299] avg loss: 0.654151
[epoch 23, batch   399] avg loss: 0.678918
[epoch 23, batch   499] avg loss: 0.662178
[epoch 23, batch   599] avg loss: 0.667159
[epoch 23, batch   699] avg loss: 0.653162
[epoch 23, batch   799] avg loss: 0.662158
[epoch 23, batch   899] avg loss: 0.657685
[epoch 23, batch   999] avg loss: 0.662088
[epoch 23, batch  1099] avg loss: 0.644360
[epoch 23, batch  1199] avg loss: 0.656884
[epoch 23, batch  1299] avg loss: 0.659982
[epoch 23, batch  1399] avg loss: 0.686208
[epoch 23, batch  1499] avg loss: 0.679219
[epoch 23, batch  1599] avg loss: 0.667542
[epoch 23, batch  1699] avg loss: 0.652503
[epoch 23, batch  1799] avg loss: 0.666501
[epoch 23, batch  1899] avg loss: 0.659803
[epoch 23, batch  1999] avg loss: 0.659381
[epoch 23, batch  2099] avg loss: 0.645176
[epoch 23, batch  2199] avg loss: 0.647053
[epoch 23, batch  2299] avg loss: 0.649040
[epoch 23, batch  2399] avg loss: 0.665254
[epoch 24, batch    99] avg loss: 0.657986
[epoch 24, batch   199] avg loss: 0.668124
[epoch 24, batch   299] avg loss: 0.677972
[epoch 24, batch   399] avg loss: 0.658111
[epoch 24, batch   499] avg loss: 0.666254
[epoch 24, batch   599] avg loss: 0.656829
[epoch 24, batch   699] avg loss: 0.671637
[epoch 24, batch   799] avg loss: 0.665333
[epoch 24, batch   899] avg loss: 0.656381
[epoch 24, batch   999] avg loss: 0.654056
[epoch 24, batch  1099] avg loss: 0.648869
[epoch 24, batch  1199] avg loss: 0.638778
[epoch 24, batch  1299] avg loss: 0.647879
[epoch 24, batch  1399] avg loss: 0.657661
[epoch 24, batch  1499] avg loss: 0.655412
[epoch 24, batch  1599] avg loss: 0.657603
[epoch 24, batch  1699] avg loss: 0.656659
[epoch 24, batch  1799] avg loss: 0.642429
[epoch 24, batch  1899] avg loss: 0.654817
[epoch 24, batch  1999] avg loss: 0.659021
[epoch 24, batch  2099] avg loss: 0.661459
[epoch 24, batch  2199] avg loss: 0.662450
[epoch 24, batch  2299] avg loss: 0.641212
[epoch 24, batch  2399] avg loss: 0.668655
[epoch 25, batch    99] avg loss: 0.644316
[epoch 25, batch   199] avg loss: 0.649602
[epoch 25, batch   299] avg loss: 0.645374
[epoch 25, batch   399] avg loss: 0.653088
[epoch 25, batch   499] avg loss: 0.646446
[epoch 25, batch   599] avg loss: 0.673751
[epoch 25, batch   699] avg loss: 0.653269
[epoch 25, batch   799] avg loss: 0.655927
[epoch 25, batch   899] avg loss: 0.659443
[epoch 25, batch   999] avg loss: 0.662865
[epoch 25, batch  1099] avg loss: 0.637345
[epoch 25, batch  1199] avg loss: 0.678564
[epoch 25, batch  1299] avg loss: 0.643827
[epoch 25, batch  1399] avg loss: 0.635416
[epoch 25, batch  1499] avg loss: 0.651979
[epoch 25, batch  1599] avg loss: 0.654018
[epoch 25, batch  1699] avg loss: 0.662320
[epoch 25, batch  1799] avg loss: 0.665080
[epoch 25, batch  1899] avg loss: 0.649472
[epoch 25, batch  1999] avg loss: 0.638496
[epoch 25, batch  2099] avg loss: 0.668613
[epoch 25, batch  2199] avg loss: 0.633728
[epoch 25, batch  2299] avg loss: 0.649396
[epoch 25, batch  2399] avg loss: 0.641029
[epoch 26, batch    99] avg loss: 0.646868
[epoch 26, batch   199] avg loss: 0.652025
[epoch 26, batch   299] avg loss: 0.656278
[epoch 26, batch   399] avg loss: 0.653889
[epoch 26, batch   499] avg loss: 0.643622
[epoch 26, batch   599] avg loss: 0.641743
[epoch 26, batch   699] avg loss: 0.640272
[epoch 26, batch   799] avg loss: 0.655812
[epoch 26, batch   899] avg loss: 0.641959
[epoch 26, batch   999] avg loss: 0.661863
[epoch 26, batch  1099] avg loss: 0.646052
[epoch 26, batch  1199] avg loss: 0.627287
[epoch 26, batch  1299] avg loss: 0.670696
[epoch 26, batch  1399] avg loss: 0.686003
[epoch 26, batch  1499] avg loss: 0.670221
[epoch 26, batch  1599] avg loss: 0.641943
[epoch 26, batch  1699] avg loss: 0.639394
[epoch 26, batch  1799] avg loss: 0.659708
[epoch 26, batch  1899] avg loss: 0.647814
[epoch 26, batch  1999] avg loss: 0.657242
[epoch 26, batch  2099] avg loss: 0.638219
[epoch 26, batch  2199] avg loss: 0.636270
[epoch 26, batch  2299] avg loss: 0.660133
[epoch 26, batch  2399] avg loss: 0.650873
[epoch 27, batch    99] avg loss: 0.651054
[epoch 27, batch   199] avg loss: 0.649815
[epoch 27, batch   299] avg loss: 0.661156
[epoch 27, batch   399] avg loss: 0.651043
[epoch 27, batch   499] avg loss: 0.638384
[epoch 27, batch   599] avg loss: 0.661384
[epoch 27, batch   699] avg loss: 0.636400
[epoch 27, batch   799] avg loss: 0.652490
[epoch 27, batch   899] avg loss: 0.641129
[epoch 27, batch   999] avg loss: 0.636900
[epoch 27, batch  1099] avg loss: 0.670555
[epoch 27, batch  1199] avg loss: 0.636465
[epoch 27, batch  1299] avg loss: 0.643507
[epoch 27, batch  1399] avg loss: 0.623757
[epoch 27, batch  1499] avg loss: 0.627342
[epoch 27, batch  1599] avg loss: 0.630875
[epoch 27, batch  1699] avg loss: 0.646180
[epoch 27, batch  1799] avg loss: 0.666647
[epoch 27, batch  1899] avg loss: 0.640427
[epoch 27, batch  1999] avg loss: 0.631453
[epoch 27, batch  2099] avg loss: 0.631560
[epoch 27, batch  2199] avg loss: 0.641357
[epoch 27, batch  2299] avg loss: 0.652355
[epoch 27, batch  2399] avg loss: 0.653934
[epoch 28, batch    99] avg loss: 0.657536
[epoch 28, batch   199] avg loss: 0.639354
[epoch 28, batch   299] avg loss: 0.634893
[epoch 28, batch   399] avg loss: 0.654295
[epoch 28, batch   499] avg loss: 0.644355
[epoch 28, batch   599] avg loss: 0.639111
[epoch 28, batch   699] avg loss: 0.646556
[epoch 28, batch   799] avg loss: 0.621825
[epoch 28, batch   899] avg loss: 0.617826
[epoch 28, batch   999] avg loss: 0.642762
[epoch 28, batch  1099] avg loss: 0.647653
[epoch 28, batch  1199] avg loss: 0.633724
[epoch 28, batch  1299] avg loss: 0.638969
[epoch 28, batch  1399] avg loss: 0.648324
[epoch 28, batch  1499] avg loss: 0.649544
[epoch 28, batch  1599] avg loss: 0.640043
[epoch 28, batch  1699] avg loss: 0.641924
[epoch 28, batch  1799] avg loss: 0.664000
[epoch 28, batch  1899] avg loss: 0.658083
[epoch 28, batch  1999] avg loss: 0.653778
[epoch 28, batch  2099] avg loss: 0.615961
[epoch 28, batch  2199] avg loss: 0.641144
[epoch 28, batch  2299] avg loss: 0.633053
[epoch 28, batch  2399] avg loss: 0.636613
[epoch 29, batch    99] avg loss: 0.647213
[epoch 29, batch   199] avg loss: 0.648860
[epoch 29, batch   299] avg loss: 0.629779
[epoch 29, batch   399] avg loss: 0.638910
[epoch 29, batch   499] avg loss: 0.638489
[epoch 29, batch   599] avg loss: 0.641343
[epoch 29, batch   699] avg loss: 0.650965
[epoch 29, batch   799] avg loss: 0.643268
[epoch 29, batch   899] avg loss: 0.633489
[epoch 29, batch   999] avg loss: 0.649819
[epoch 29, batch  1099] avg loss: 0.640527
[epoch 29, batch  1199] avg loss: 0.627416
[epoch 29, batch  1299] avg loss: 0.650750
[epoch 29, batch  1399] avg loss: 0.627263
[epoch 29, batch  1499] avg loss: 0.631105
[epoch 29, batch  1599] avg loss: 0.630823
[epoch 29, batch  1699] avg loss: 0.637175
[epoch 29, batch  1799] avg loss: 0.633868
[epoch 29, batch  1899] avg loss: 0.637883
[epoch 29, batch  1999] avg loss: 0.626915
[epoch 29, batch  2099] avg loss: 0.635102
[epoch 29, batch  2199] avg loss: 0.627552
[epoch 29, batch  2299] avg loss: 0.642603
[epoch 29, batch  2399] avg loss: 0.647852
[epoch 30, batch    99] avg loss: 0.638009
[epoch 30, batch   199] avg loss: 0.623329
[epoch 30, batch   299] avg loss: 0.639074
[epoch 30, batch   399] avg loss: 0.635451
[epoch 30, batch   499] avg loss: 0.640175
[epoch 30, batch   599] avg loss: 0.626708
[epoch 30, batch   699] avg loss: 0.627489
[epoch 30, batch   799] avg loss: 0.634331
[epoch 30, batch   899] avg loss: 0.618598
[epoch 30, batch   999] avg loss: 0.629859
[epoch 30, batch  1099] avg loss: 0.660664
[epoch 30, batch  1199] avg loss: 0.634485
[epoch 30, batch  1299] avg loss: 0.665794
[epoch 30, batch  1399] avg loss: 0.635980
[epoch 30, batch  1499] avg loss: 0.636118
[epoch 30, batch  1599] avg loss: 0.626572
[epoch 30, batch  1699] avg loss: 0.624142
[epoch 30, batch  1799] avg loss: 0.639973
[epoch 30, batch  1899] avg loss: 0.627335
[epoch 30, batch  1999] avg loss: 0.629399
[epoch 30, batch  2099] avg loss: 0.620628
[epoch 30, batch  2199] avg loss: 0.622549
[epoch 30, batch  2299] avg loss: 0.637805
[epoch 30, batch  2399] avg loss: 0.625350
[epoch 31, batch    99] avg loss: 0.638300
[epoch 31, batch   199] avg loss: 0.652903
[epoch 31, batch   299] avg loss: 0.645117
[epoch 31, batch   399] avg loss: 0.642160
[epoch 31, batch   499] avg loss: 0.625947
[epoch 31, batch   599] avg loss: 0.626903
[epoch 31, batch   699] avg loss: 0.622854
[epoch 31, batch   799] avg loss: 0.628375
[epoch 31, batch   899] avg loss: 0.641718
[epoch 31, batch   999] avg loss: 0.604557
[epoch 31, batch  1099] avg loss: 0.618458
[epoch 31, batch  1199] avg loss: 0.643962
[epoch 31, batch  1299] avg loss: 0.647834
[epoch 31, batch  1399] avg loss: 0.610097
[epoch 31, batch  1499] avg loss: 0.620865
[epoch 31, batch  1599] avg loss: 0.598299
[epoch 31, batch  1699] avg loss: 0.619126
[epoch 31, batch  1799] avg loss: 0.616026
[epoch 31, batch  1899] avg loss: 0.620192
[epoch 31, batch  1999] avg loss: 0.636978
[epoch 31, batch  2099] avg loss: 0.623143
[epoch 31, batch  2199] avg loss: 0.619152
[epoch 31, batch  2299] avg loss: 0.616909
[epoch 31, batch  2399] avg loss: 0.621274
[epoch 32, batch    99] avg loss: 0.615976
[epoch 32, batch   199] avg loss: 0.620820
[epoch 32, batch   299] avg loss: 0.641523
[epoch 32, batch   399] avg loss: 0.635319
[epoch 32, batch   499] avg loss: 0.632262
[epoch 32, batch   599] avg loss: 0.626454
[epoch 32, batch   699] avg loss: 0.630596
[epoch 32, batch   799] avg loss: 0.628153
[epoch 32, batch   899] avg loss: 0.623581
[epoch 32, batch   999] avg loss: 0.622445
[epoch 32, batch  1099] avg loss: 0.640032
[epoch 32, batch  1199] avg loss: 0.604766
[epoch 32, batch  1299] avg loss: 0.629391
[epoch 32, batch  1399] avg loss: 0.618090
[epoch 32, batch  1499] avg loss: 0.616259
[epoch 32, batch  1599] avg loss: 0.630522
[epoch 32, batch  1699] avg loss: 0.619529
[epoch 32, batch  1799] avg loss: 0.631083
[epoch 32, batch  1899] avg loss: 0.630995
[epoch 32, batch  1999] avg loss: 0.638595
[epoch 32, batch  2099] avg loss: 0.612473
[epoch 32, batch  2199] avg loss: 0.615289
[epoch 32, batch  2299] avg loss: 0.625817
[epoch 32, batch  2399] avg loss: 0.627862
[epoch 33, batch    99] avg loss: 0.626638
[epoch 33, batch   199] avg loss: 0.628909
[epoch 33, batch   299] avg loss: 0.618760
[epoch 33, batch   399] avg loss: 0.640827
[epoch 33, batch   499] avg loss: 0.630877
[epoch 33, batch   599] avg loss: 0.616446
[epoch 33, batch   699] avg loss: 0.621366
[epoch 33, batch   799] avg loss: 0.616669
[epoch 33, batch   899] avg loss: 0.600451
[epoch 33, batch   999] avg loss: 0.607729
[epoch 33, batch  1099] avg loss: 0.634608
[epoch 33, batch  1199] avg loss: 0.617883
[epoch 33, batch  1299] avg loss: 0.607821
[epoch 33, batch  1399] avg loss: 0.617871
[epoch 33, batch  1499] avg loss: 0.611558
[epoch 33, batch  1599] avg loss: 0.605598
[epoch 33, batch  1699] avg loss: 0.632076
[epoch 33, batch  1799] avg loss: 0.616438
[epoch 33, batch  1899] avg loss: 0.634188
[epoch 33, batch  1999] avg loss: 0.623819
[epoch 33, batch  2099] avg loss: 0.605821
[epoch 33, batch  2199] avg loss: 0.619181
[epoch 33, batch  2299] avg loss: 0.620555
[epoch 33, batch  2399] avg loss: 0.637983
[epoch 34, batch    99] avg loss: 0.620281
[epoch 34, batch   199] avg loss: 0.620467
[epoch 34, batch   299] avg loss: 0.619932
[epoch 34, batch   399] avg loss: 0.618238
[epoch 34, batch   499] avg loss: 0.606100
[epoch 34, batch   599] avg loss: 0.626429
[epoch 34, batch   699] avg loss: 0.587670
[epoch 34, batch   799] avg loss: 0.620810
[epoch 34, batch   899] avg loss: 0.631923
[epoch 34, batch   999] avg loss: 0.611956
[epoch 34, batch  1099] avg loss: 0.611681
[epoch 34, batch  1199] avg loss: 0.604214
[epoch 34, batch  1299] avg loss: 0.608650
[epoch 34, batch  1399] avg loss: 0.611699
[epoch 34, batch  1499] avg loss: 0.619312
[epoch 34, batch  1599] avg loss: 0.616816
[epoch 34, batch  1699] avg loss: 0.624605
[epoch 34, batch  1799] avg loss: 0.634229
[epoch 34, batch  1899] avg loss: 0.613240
[epoch 34, batch  1999] avg loss: 0.604621
[epoch 34, batch  2099] avg loss: 0.613169
[epoch 34, batch  2199] avg loss: 0.612082
[epoch 34, batch  2299] avg loss: 0.612458
[epoch 34, batch  2399] avg loss: 0.621301
[epoch 35, batch    99] avg loss: 0.623506
[epoch 35, batch   199] avg loss: 0.612517
[epoch 35, batch   299] avg loss: 0.605075
[epoch 35, batch   399] avg loss: 0.611565
[epoch 35, batch   499] avg loss: 0.605764
[epoch 35, batch   599] avg loss: 0.648900
[epoch 35, batch   699] avg loss: 0.610027
[epoch 35, batch   799] avg loss: 0.622800
[epoch 35, batch   899] avg loss: 0.626432
[epoch 35, batch   999] avg loss: 0.620147
[epoch 35, batch  1099] avg loss: 0.627734
[epoch 35, batch  1199] avg loss: 0.606432
[epoch 35, batch  1299] avg loss: 0.608335
[epoch 35, batch  1399] avg loss: 0.588074
[epoch 35, batch  1499] avg loss: 0.621941
[epoch 35, batch  1599] avg loss: 0.604286
[epoch 35, batch  1699] avg loss: 0.609759
[epoch 35, batch  1799] avg loss: 0.594367
[epoch 35, batch  1899] avg loss: 0.623873
[epoch 35, batch  1999] avg loss: 0.613553
[epoch 35, batch  2099] avg loss: 0.600497
[epoch 35, batch  2199] avg loss: 0.612960
[epoch 35, batch  2299] avg loss: 0.625218
[epoch 35, batch  2399] avg loss: 0.596597
[epoch 36, batch    99] avg loss: 0.606422
[epoch 36, batch   199] avg loss: 0.652955
[epoch 36, batch   299] avg loss: 0.626867
[epoch 36, batch   399] avg loss: 0.616203
[epoch 36, batch   499] avg loss: 0.587396
[epoch 36, batch   599] avg loss: 0.610769
[epoch 36, batch   699] avg loss: 0.616007
[epoch 36, batch   799] avg loss: 0.590004
[epoch 36, batch   899] avg loss: 0.603692
[epoch 36, batch   999] avg loss: 0.597493
[epoch 36, batch  1099] avg loss: 0.594668
[epoch 36, batch  1199] avg loss: 0.592721
[epoch 36, batch  1299] avg loss: 0.617547
[epoch 36, batch  1399] avg loss: 0.597348
[epoch 36, batch  1499] avg loss: 0.619680
[epoch 36, batch  1599] avg loss: 0.620486
[epoch 36, batch  1699] avg loss: 0.617109
[epoch 36, batch  1799] avg loss: 0.613100
[epoch 36, batch  1899] avg loss: 0.599780
[epoch 36, batch  1999] avg loss: 0.606399
[epoch 36, batch  2099] avg loss: 0.648197
[epoch 36, batch  2199] avg loss: 0.597533
[epoch 36, batch  2299] avg loss: 0.601809
[epoch 36, batch  2399] avg loss: 0.590623
[epoch 37, batch    99] avg loss: 0.612188
[epoch 37, batch   199] avg loss: 0.614444
[epoch 37, batch   299] avg loss: 0.624500
[epoch 37, batch   399] avg loss: 0.600876
[epoch 37, batch   499] avg loss: 0.614849
[epoch 37, batch   599] avg loss: 0.615501
[epoch 37, batch   699] avg loss: 0.599960
[epoch 37, batch   799] avg loss: 0.582397
[epoch 37, batch   899] avg loss: 0.632693
[epoch 37, batch   999] avg loss: 0.612709
[epoch 37, batch  1099] avg loss: 0.600151
[epoch 37, batch  1199] avg loss: 0.617095
[epoch 37, batch  1299] avg loss: 0.588144
[epoch 37, batch  1399] avg loss: 0.596398
[epoch 37, batch  1499] avg loss: 0.598810
[epoch 37, batch  1599] avg loss: 0.587185
[epoch 37, batch  1699] avg loss: 0.620781
[epoch 37, batch  1799] avg loss: 0.616156
[epoch 37, batch  1899] avg loss: 0.611153
[epoch 37, batch  1999] avg loss: 0.618019
[epoch 37, batch  2099] avg loss: 0.614566
[epoch 37, batch  2199] avg loss: 0.613579
[epoch 37, batch  2299] avg loss: 0.608322
[epoch 37, batch  2399] avg loss: 0.596098
[epoch 38, batch    99] avg loss: 0.584312
[epoch 38, batch   199] avg loss: 0.598767
[epoch 38, batch   299] avg loss: 0.596410
[epoch 38, batch   399] avg loss: 0.602861
[epoch 38, batch   499] avg loss: 0.599170
[epoch 38, batch   599] avg loss: 0.585620
[epoch 38, batch   699] avg loss: 0.599426
[epoch 38, batch   799] avg loss: 0.587805
[epoch 38, batch   899] avg loss: 0.616665
[epoch 38, batch   999] avg loss: 0.589674
[epoch 38, batch  1099] avg loss: 0.587949
[epoch 38, batch  1199] avg loss: 0.605469
[epoch 38, batch  1299] avg loss: 0.610299
[epoch 38, batch  1399] avg loss: 0.614674
[epoch 38, batch  1499] avg loss: 0.600806
[epoch 38, batch  1599] avg loss: 0.596969
[epoch 38, batch  1699] avg loss: 0.599520
[epoch 38, batch  1799] avg loss: 0.617032
[epoch 38, batch  1899] avg loss: 0.597702
[epoch 38, batch  1999] avg loss: 0.591641
[epoch 38, batch  2099] avg loss: 0.617274
[epoch 38, batch  2199] avg loss: 0.578929
[epoch 38, batch  2299] avg loss: 0.625466
[epoch 38, batch  2399] avg loss: 0.594711
[epoch 39, batch    99] avg loss: 0.634059
[epoch 39, batch   199] avg loss: 0.622553
[epoch 39, batch   299] avg loss: 0.588456
[epoch 39, batch   399] avg loss: 0.601058
[epoch 39, batch   499] avg loss: 0.623441
[epoch 39, batch   599] avg loss: 0.597456
[epoch 39, batch   699] avg loss: 0.592796
[epoch 39, batch   799] avg loss: 0.590164
[epoch 39, batch   899] avg loss: 0.585440
[epoch 39, batch   999] avg loss: 0.586680
[epoch 39, batch  1099] avg loss: 0.612927
[epoch 39, batch  1199] avg loss: 0.585578
[epoch 39, batch  1299] avg loss: 0.584100
[epoch 39, batch  1399] avg loss: 0.595152
[epoch 39, batch  1499] avg loss: 0.596765
[epoch 39, batch  1599] avg loss: 0.592362
[epoch 39, batch  1699] avg loss: 0.614763
[epoch 39, batch  1799] avg loss: 0.606379
[epoch 39, batch  1899] avg loss: 0.599842
[epoch 39, batch  1999] avg loss: 0.618784
[epoch 39, batch  2099] avg loss: 0.572847
[epoch 39, batch  2199] avg loss: 0.599864
[epoch 39, batch  2299] avg loss: 0.587761
[epoch 39, batch  2399] avg loss: 0.598213
Model saved to model/20200502-230308.pth.
accuracy/TriangPrismIsosc : 0.494
n_examples/TriangPrismIsosc : 500.0
accuracy/parallelepiped : 0.378
n_examples/parallelepiped : 500.0
accuracy/sphere : 1.0
n_examples/sphere : 102.0
accuracy/wire : 0.755
n_examples/wire : 200.0
accuracy/avg_geom : 0.5291858678955453
loss/validation_geom : 0.9701519510529923
accuracy/Au : 0.059907834101382486
n_examples/Au : 1302.0
accuracy/SiN : 0.0
n_examples/SiN : 0.0
accuracy/SiO2 : 0.0
n_examples/SiO2 : 0.0
accuracy/avg_mat : 0.059907834101382486
loss/validation_mat : 1.3310747512840821
MSE/ShortestDim : 1.121223376460156
MAE/ShortestDim : 0.6383242292155136
MSE/MiddleDim : 6.445837264053649
MAE/MiddleDim : 1.8970479540378085
MSE/LongDim : 103.63763418358775
MAE/LongDim : 6.039380555511803
MSE/log Area/Vol : 7.455240861977667
MAE/log Area/Vol : 2.5093880197785783
loss/validation_dim : 118.65993568607922
loss/validation : 120.9611623884163
Metrics saved to model/20200502-230308_metrics.csv.
Parsed 2604 rows from data/sim_train_spectrum_SiN.
Parsed 2604 rows from data/sim_train_labels_SiN.
Parsed 9765 rows from data/gen_spectrum_SiN_00-of-16.
Parsed 9765 rows from data/gen_labels_SiN_00-of-16.
Parsed 9765 rows from data/gen_spectrum_SiN_01-of-16.
Parsed 9765 rows from data/gen_labels_SiN_01-of-16.
Parsed 9765 rows from data/gen_spectrum_SiN_02-of-16.
Parsed 9765 rows from data/gen_labels_SiN_02-of-16.
Parsed 9765 rows from data/gen_spectrum_SiN_03-of-16.
Parsed 9765 rows from data/gen_labels_SiN_03-of-16.
Parsed 9765 rows from data/gen_spectrum_SiN_04-of-16.
Parsed 9765 rows from data/gen_labels_SiN_04-of-16.
Parsed 9765 rows from data/gen_spectrum_SiN_05-of-16.
Parsed 9765 rows from data/gen_labels_SiN_05-of-16.
Parsed 9765 rows from data/gen_spectrum_SiN_06-of-16.
Parsed 9765 rows from data/gen_labels_SiN_06-of-16.
Parsed 9765 rows from data/gen_spectrum_SiN_07-of-16.
Parsed 9765 rows from data/gen_labels_SiN_07-of-16.
Parsed 9765 rows from data/gen_spectrum_SiN_08-of-16.
Parsed 9765 rows from data/gen_labels_SiN_08-of-16.
Parsed 9765 rows from data/gen_spectrum_SiN_09-of-16.
Parsed 9765 rows from data/gen_labels_SiN_09-of-16.
Parsed 9765 rows from data/gen_spectrum_SiN_10-of-16.
Parsed 9765 rows from data/gen_labels_SiN_10-of-16.
Parsed 9765 rows from data/gen_spectrum_SiN_11-of-16.
Parsed 9765 rows from data/gen_labels_SiN_11-of-16.
Parsed 9765 rows from data/gen_spectrum_SiN_12-of-16.
Parsed 9765 rows from data/gen_labels_SiN_12-of-16.
Parsed 9765 rows from data/gen_spectrum_SiN_13-of-16.
Parsed 9765 rows from data/gen_labels_SiN_13-of-16.
Parsed 9765 rows from data/gen_spectrum_SiN_14-of-16.
Parsed 9765 rows from data/gen_labels_SiN_14-of-16.
Parsed 9765 rows from data/gen_spectrum_SiN_15-of-16.
Parsed 9765 rows from data/gen_labels_SiN_15-of-16.
Parsed 1302 rows from data/sim_validation_spectrum_SiN.
Parsed 1302 rows from data/sim_validation_labels_SiN.
Logging training progress to tensorboard dir runs/alexnet-SiN-lr_0.000100-trainsize_158844-05_02_2020_23:03-joint.
[epoch 0, batch    99] avg loss: 1.386468
[epoch 0, batch   199] avg loss: 1.382237
[epoch 0, batch   299] avg loss: 1.309204
[epoch 0, batch   399] avg loss: 1.093942
[epoch 0, batch   499] avg loss: 0.920860
[epoch 0, batch   599] avg loss: 0.871112
[epoch 0, batch   699] avg loss: 0.851568
[epoch 0, batch   799] avg loss: 0.828539
[epoch 0, batch   899] avg loss: 0.820095
[epoch 0, batch   999] avg loss: 0.825486
[epoch 0, batch  1099] avg loss: 0.810366
[epoch 0, batch  1199] avg loss: 0.804033
[epoch 0, batch  1299] avg loss: 0.791758
[epoch 0, batch  1399] avg loss: 0.776024
[epoch 0, batch  1499] avg loss: 0.805420
[epoch 0, batch  1599] avg loss: 0.787710
[epoch 0, batch  1699] avg loss: 0.784212
[epoch 0, batch  1799] avg loss: 0.778562
[epoch 0, batch  1899] avg loss: 0.773329
[epoch 0, batch  1999] avg loss: 0.751041
[epoch 0, batch  2099] avg loss: 0.768157
[epoch 0, batch  2199] avg loss: 0.764435
[epoch 0, batch  2299] avg loss: 0.780718
[epoch 0, batch  2399] avg loss: 0.742908
[epoch 1, batch    99] avg loss: 0.753390
[epoch 1, batch   199] avg loss: 0.768691
[epoch 1, batch   299] avg loss: 0.761064
[epoch 1, batch   399] avg loss: 0.756265
[epoch 1, batch   499] avg loss: 0.768322
[epoch 1, batch   599] avg loss: 0.760140
[epoch 1, batch   699] avg loss: 0.749623
[epoch 1, batch   799] avg loss: 0.767945
[epoch 1, batch   899] avg loss: 0.754796
[epoch 1, batch   999] avg loss: 0.746662
[epoch 1, batch  1099] avg loss: 0.752149
[epoch 1, batch  1199] avg loss: 0.750826
[epoch 1, batch  1299] avg loss: 0.742687
[epoch 1, batch  1399] avg loss: 0.726501
[epoch 1, batch  1499] avg loss: 0.734891
[epoch 1, batch  1599] avg loss: 0.736725
[epoch 1, batch  1699] avg loss: 0.748124
[epoch 1, batch  1799] avg loss: 0.720882
[epoch 1, batch  1899] avg loss: 0.727410
[epoch 1, batch  1999] avg loss: 0.720804
[epoch 1, batch  2099] avg loss: 0.719402
[epoch 1, batch  2199] avg loss: 0.718952
[epoch 1, batch  2299] avg loss: 0.710307
[epoch 1, batch  2399] avg loss: 0.705630
[epoch 2, batch    99] avg loss: 0.698684
[epoch 2, batch   199] avg loss: 0.703024
[epoch 2, batch   299] avg loss: 0.704973
[epoch 2, batch   399] avg loss: 0.686734
[epoch 2, batch   499] avg loss: 0.682481
[epoch 2, batch   599] avg loss: 0.682472
[epoch 2, batch   699] avg loss: 0.682294
[epoch 2, batch   799] avg loss: 0.677882
[epoch 2, batch   899] avg loss: 0.682547
[epoch 2, batch   999] avg loss: 0.662352
[epoch 2, batch  1099] avg loss: 0.654160
[epoch 2, batch  1199] avg loss: 0.669045
[epoch 2, batch  1299] avg loss: 0.655135
[epoch 2, batch  1399] avg loss: 0.673948
[epoch 2, batch  1499] avg loss: 0.675101
[epoch 2, batch  1599] avg loss: 0.646333
[epoch 2, batch  1699] avg loss: 0.647111
[epoch 2, batch  1799] avg loss: 0.647577
[epoch 2, batch  1899] avg loss: 0.638819
[epoch 2, batch  1999] avg loss: 0.654955
[epoch 2, batch  2099] avg loss: 0.640464
[epoch 2, batch  2199] avg loss: 0.645625
[epoch 2, batch  2299] avg loss: 0.647166
[epoch 2, batch  2399] avg loss: 0.622572
[epoch 3, batch    99] avg loss: 0.635427
[epoch 3, batch   199] avg loss: 0.630380
[epoch 3, batch   299] avg loss: 0.620024
[epoch 3, batch   399] avg loss: 0.646049
[epoch 3, batch   499] avg loss: 0.638526
[epoch 3, batch   599] avg loss: 0.648990
[epoch 3, batch   699] avg loss: 0.641539
[epoch 3, batch   799] avg loss: 0.627085
[epoch 3, batch   899] avg loss: 0.609687
[epoch 3, batch   999] avg loss: 0.635938
[epoch 3, batch  1099] avg loss: 0.641804
[epoch 3, batch  1199] avg loss: 0.630675
[epoch 3, batch  1299] avg loss: 0.616715
[epoch 3, batch  1399] avg loss: 0.626072
[epoch 3, batch  1499] avg loss: 0.615584
[epoch 3, batch  1599] avg loss: 0.615461
[epoch 3, batch  1699] avg loss: 0.624463
[epoch 3, batch  1799] avg loss: 0.611420
[epoch 3, batch  1899] avg loss: 0.605232
[epoch 3, batch  1999] avg loss: 0.612871
[epoch 3, batch  2099] avg loss: 0.619718
[epoch 3, batch  2199] avg loss: 0.603652
[epoch 3, batch  2299] avg loss: 0.604473
[epoch 3, batch  2399] avg loss: 0.608510
[epoch 4, batch    99] avg loss: 0.605572
[epoch 4, batch   199] avg loss: 0.614222
[epoch 4, batch   299] avg loss: 0.608282
[epoch 4, batch   399] avg loss: 0.606587
[epoch 4, batch   499] avg loss: 0.590055
[epoch 4, batch   599] avg loss: 0.602201
[epoch 4, batch   699] avg loss: 0.608347
[epoch 4, batch   799] avg loss: 0.606319
[epoch 4, batch   899] avg loss: 0.604219
[epoch 4, batch   999] avg loss: 0.604879
[epoch 4, batch  1099] avg loss: 0.608655
[epoch 4, batch  1199] avg loss: 0.598188
[epoch 4, batch  1299] avg loss: 0.630541
[epoch 4, batch  1399] avg loss: 0.602141
[epoch 4, batch  1499] avg loss: 0.597113
[epoch 4, batch  1599] avg loss: 0.602507
[epoch 4, batch  1699] avg loss: 0.614752
[epoch 4, batch  1799] avg loss: 0.589068
[epoch 4, batch  1899] avg loss: 0.586632
[epoch 4, batch  1999] avg loss: 0.578633
[epoch 4, batch  2099] avg loss: 0.590083
[epoch 4, batch  2199] avg loss: 0.611290
[epoch 4, batch  2299] avg loss: 0.588977
[epoch 4, batch  2399] avg loss: 0.583919
[epoch 5, batch    99] avg loss: 0.593450
[epoch 5, batch   199] avg loss: 0.594269
[epoch 5, batch   299] avg loss: 0.590239
[epoch 5, batch   399] avg loss: 0.596270
[epoch 5, batch   499] avg loss: 0.587341
[epoch 5, batch   599] avg loss: 0.596336
[epoch 5, batch   699] avg loss: 0.593726
[epoch 5, batch   799] avg loss: 0.588155
[epoch 5, batch   899] avg loss: 0.581775
[epoch 5, batch   999] avg loss: 0.596741
[epoch 5, batch  1099] avg loss: 0.580101
[epoch 5, batch  1199] avg loss: 0.577452
[epoch 5, batch  1299] avg loss: 0.580770
[epoch 5, batch  1399] avg loss: 0.576485
[epoch 5, batch  1499] avg loss: 0.570120
[epoch 5, batch  1599] avg loss: 0.594870
[epoch 5, batch  1699] avg loss: 0.582012
[epoch 5, batch  1799] avg loss: 0.588418
[epoch 5, batch  1899] avg loss: 0.581288
[epoch 5, batch  1999] avg loss: 0.588715
[epoch 5, batch  2099] avg loss: 0.568889
[epoch 5, batch  2199] avg loss: 0.584340
[epoch 5, batch  2299] avg loss: 0.592914
[epoch 5, batch  2399] avg loss: 0.587510
[epoch 6, batch    99] avg loss: 0.574955
[epoch 6, batch   199] avg loss: 0.566219
[epoch 6, batch   299] avg loss: 0.575145
[epoch 6, batch   399] avg loss: 0.581590
[epoch 6, batch   499] avg loss: 0.571923
[epoch 6, batch   599] avg loss: 0.573195
[epoch 6, batch   699] avg loss: 0.585016
[epoch 6, batch   799] avg loss: 0.557884
[epoch 6, batch   899] avg loss: 0.562873
[epoch 6, batch   999] avg loss: 0.595409
[epoch 6, batch  1099] avg loss: 0.573626
[epoch 6, batch  1199] avg loss: 0.557053
[epoch 6, batch  1299] avg loss: 0.569303
[epoch 6, batch  1399] avg loss: 0.570348
[epoch 6, batch  1499] avg loss: 0.593570
[epoch 6, batch  1599] avg loss: 0.570801
[epoch 6, batch  1699] avg loss: 0.550763
[epoch 6, batch  1799] avg loss: 0.578582
[epoch 6, batch  1899] avg loss: 0.572545
[epoch 6, batch  1999] avg loss: 0.581366
[epoch 6, batch  2099] avg loss: 0.564338
[epoch 6, batch  2199] avg loss: 0.566757
[epoch 6, batch  2299] avg loss: 0.564909
[epoch 6, batch  2399] avg loss: 0.561541
[epoch 7, batch    99] avg loss: 0.564661
[epoch 7, batch   199] avg loss: 0.557726
[epoch 7, batch   299] avg loss: 0.576993
[epoch 7, batch   399] avg loss: 0.570858
[epoch 7, batch   499] avg loss: 0.572641
[epoch 7, batch   599] avg loss: 0.559253
[epoch 7, batch   699] avg loss: 0.552956
[epoch 7, batch   799] avg loss: 0.549799
[epoch 7, batch   899] avg loss: 0.588256
[epoch 7, batch   999] avg loss: 0.569482
[epoch 7, batch  1099] avg loss: 0.581558
[epoch 7, batch  1199] avg loss: 0.583190
[epoch 7, batch  1299] avg loss: 0.558360
[epoch 7, batch  1399] avg loss: 0.562190
[epoch 7, batch  1499] avg loss: 0.568873
[epoch 7, batch  1599] avg loss: 0.573977
[epoch 7, batch  1699] avg loss: 0.552730
[epoch 7, batch  1799] avg loss: 0.548235
[epoch 7, batch  1899] avg loss: 0.559480
[epoch 7, batch  1999] avg loss: 0.547853
[epoch 7, batch  2099] avg loss: 0.557119
[epoch 7, batch  2199] avg loss: 0.566108
[epoch 7, batch  2299] avg loss: 0.568699
[epoch 7, batch  2399] avg loss: 0.561153
[epoch 8, batch    99] avg loss: 0.557136
[epoch 8, batch   199] avg loss: 0.547220
[epoch 8, batch   299] avg loss: 0.548789
[epoch 8, batch   399] avg loss: 0.554440
[epoch 8, batch   499] avg loss: 0.577398
[epoch 8, batch   599] avg loss: 0.549276
[epoch 8, batch   699] avg loss: 0.552774
[epoch 8, batch   799] avg loss: 0.559041
[epoch 8, batch   899] avg loss: 0.554883
[epoch 8, batch   999] avg loss: 0.554562
[epoch 8, batch  1099] avg loss: 0.549200
[epoch 8, batch  1199] avg loss: 0.550912
[epoch 8, batch  1299] avg loss: 0.560375
[epoch 8, batch  1399] avg loss: 0.564786
[epoch 8, batch  1499] avg loss: 0.555635
[epoch 8, batch  1599] avg loss: 0.558611
[epoch 8, batch  1699] avg loss: 0.558136
[epoch 8, batch  1799] avg loss: 0.555696
[epoch 8, batch  1899] avg loss: 0.558577
[epoch 8, batch  1999] avg loss: 0.541348
[epoch 8, batch  2099] avg loss: 0.550992
[epoch 8, batch  2199] avg loss: 0.546013
[epoch 8, batch  2299] avg loss: 0.558816
[epoch 8, batch  2399] avg loss: 0.552327
[epoch 9, batch    99] avg loss: 0.552237
[epoch 9, batch   199] avg loss: 0.546018
[epoch 9, batch   299] avg loss: 0.546754
[epoch 9, batch   399] avg loss: 0.545142
[epoch 9, batch   499] avg loss: 0.563745
[epoch 9, batch   599] avg loss: 0.560765
[epoch 9, batch   699] avg loss: 0.556967
[epoch 9, batch   799] avg loss: 0.527981
[epoch 9, batch   899] avg loss: 0.555900
[epoch 9, batch   999] avg loss: 0.559206
[epoch 9, batch  1099] avg loss: 0.549259
[epoch 9, batch  1199] avg loss: 0.543008
[epoch 9, batch  1299] avg loss: 0.537783
[epoch 9, batch  1399] avg loss: 0.535316
[epoch 9, batch  1499] avg loss: 0.545408
[epoch 9, batch  1599] avg loss: 0.552869
[epoch 9, batch  1699] avg loss: 0.550320
[epoch 9, batch  1799] avg loss: 0.536568
[epoch 9, batch  1899] avg loss: 0.549186
[epoch 9, batch  1999] avg loss: 0.550788
[epoch 9, batch  2099] avg loss: 0.535423
[epoch 9, batch  2199] avg loss: 0.554595
[epoch 9, batch  2299] avg loss: 0.538138
[epoch 9, batch  2399] avg loss: 0.539295
[epoch 10, batch    99] avg loss: 0.544286
[epoch 10, batch   199] avg loss: 0.539434
[epoch 10, batch   299] avg loss: 0.536121
[epoch 10, batch   399] avg loss: 0.547834
[epoch 10, batch   499] avg loss: 0.530972
[epoch 10, batch   599] avg loss: 0.533614
[epoch 10, batch   699] avg loss: 0.539456
[epoch 10, batch   799] avg loss: 0.535707
[epoch 10, batch   899] avg loss: 0.545937
[epoch 10, batch   999] avg loss: 0.542789
[epoch 10, batch  1099] avg loss: 0.556079
[epoch 10, batch  1199] avg loss: 0.557737
[epoch 10, batch  1299] avg loss: 0.527634
[epoch 10, batch  1399] avg loss: 0.543030
[epoch 10, batch  1499] avg loss: 0.535604
[epoch 10, batch  1599] avg loss: 0.525157
[epoch 10, batch  1699] avg loss: 0.537899
[epoch 10, batch  1799] avg loss: 0.545465
[epoch 10, batch  1899] avg loss: 0.534281
[epoch 10, batch  1999] avg loss: 0.523322
[epoch 10, batch  2099] avg loss: 0.524948
[epoch 10, batch  2199] avg loss: 0.549231
[epoch 10, batch  2299] avg loss: 0.527288
[epoch 10, batch  2399] avg loss: 0.535715
[epoch 11, batch    99] avg loss: 0.548305
[epoch 11, batch   199] avg loss: 0.527196
[epoch 11, batch   299] avg loss: 0.531518
[epoch 11, batch   399] avg loss: 0.530059
[epoch 11, batch   499] avg loss: 0.534635
[epoch 11, batch   599] avg loss: 0.525774
[epoch 11, batch   699] avg loss: 0.530192
[epoch 11, batch   799] avg loss: 0.520579
[epoch 11, batch   899] avg loss: 0.546563
[epoch 11, batch   999] avg loss: 0.545141
[epoch 11, batch  1099] avg loss: 0.535143
[epoch 11, batch  1199] avg loss: 0.527414
[epoch 11, batch  1299] avg loss: 0.533316
[epoch 11, batch  1399] avg loss: 0.522826
[epoch 11, batch  1499] avg loss: 0.519054
[epoch 11, batch  1599] avg loss: 0.531830
[epoch 11, batch  1699] avg loss: 0.515264
[epoch 11, batch  1799] avg loss: 0.544744
[epoch 11, batch  1899] avg loss: 0.528342
[epoch 11, batch  1999] avg loss: 0.520593
[epoch 11, batch  2099] avg loss: 0.540489
[epoch 11, batch  2199] avg loss: 0.545290
[epoch 11, batch  2299] avg loss: 0.535872
[epoch 11, batch  2399] avg loss: 0.518442
[epoch 12, batch    99] avg loss: 0.524430
[epoch 12, batch   199] avg loss: 0.515176
[epoch 12, batch   299] avg loss: 0.531678
[epoch 12, batch   399] avg loss: 0.526685
[epoch 12, batch   499] avg loss: 0.516639
[epoch 12, batch   599] avg loss: 0.523822
[epoch 12, batch   699] avg loss: 0.542022
[epoch 12, batch   799] avg loss: 0.527598
[epoch 12, batch   899] avg loss: 0.522207
[epoch 12, batch   999] avg loss: 0.523044
[epoch 12, batch  1099] avg loss: 0.520911
[epoch 12, batch  1199] avg loss: 0.514691
[epoch 12, batch  1299] avg loss: 0.514238
[epoch 12, batch  1399] avg loss: 0.522359
[epoch 12, batch  1499] avg loss: 0.515424
[epoch 12, batch  1599] avg loss: 0.529230
[epoch 12, batch  1699] avg loss: 0.524928
[epoch 12, batch  1799] avg loss: 0.517385
[epoch 12, batch  1899] avg loss: 0.513685
[epoch 12, batch  1999] avg loss: 0.531450
[epoch 12, batch  2099] avg loss: 0.505037
[epoch 12, batch  2199] avg loss: 0.534158
[epoch 12, batch  2299] avg loss: 0.534920
[epoch 12, batch  2399] avg loss: 0.525578
[epoch 13, batch    99] avg loss: 0.511880
[epoch 13, batch   199] avg loss: 0.539122
[epoch 13, batch   299] avg loss: 0.509501
[epoch 13, batch   399] avg loss: 0.510470
[epoch 13, batch   499] avg loss: 0.525139
[epoch 13, batch   599] avg loss: 0.523589
[epoch 13, batch   699] avg loss: 0.522452
[epoch 13, batch   799] avg loss: 0.512512
[epoch 13, batch   899] avg loss: 0.524254
[epoch 13, batch   999] avg loss: 0.530938
[epoch 13, batch  1099] avg loss: 0.520222
[epoch 13, batch  1199] avg loss: 0.503659
[epoch 13, batch  1299] avg loss: 0.518487
[epoch 13, batch  1399] avg loss: 0.532195
[epoch 13, batch  1499] avg loss: 0.508054
[epoch 13, batch  1599] avg loss: 0.510499
[epoch 13, batch  1699] avg loss: 0.510828
[epoch 13, batch  1799] avg loss: 0.506940
[epoch 13, batch  1899] avg loss: 0.506584
[epoch 13, batch  1999] avg loss: 0.525032
[epoch 13, batch  2099] avg loss: 0.520084
[epoch 13, batch  2199] avg loss: 0.504972
[epoch 13, batch  2299] avg loss: 0.518374
[epoch 13, batch  2399] avg loss: 0.517171
[epoch 14, batch    99] avg loss: 0.511043
[epoch 14, batch   199] avg loss: 0.513500
[epoch 14, batch   299] avg loss: 0.509533
[epoch 14, batch   399] avg loss: 0.515472
[epoch 14, batch   499] avg loss: 0.512119
[epoch 14, batch   599] avg loss: 0.517147
[epoch 14, batch   699] avg loss: 0.510029
[epoch 14, batch   799] avg loss: 0.504233
[epoch 14, batch   899] avg loss: 0.514303
[epoch 14, batch   999] avg loss: 0.523326
[epoch 14, batch  1099] avg loss: 0.512663
[epoch 14, batch  1199] avg loss: 0.521518
[epoch 14, batch  1299] avg loss: 0.521043
[epoch 14, batch  1399] avg loss: 0.509855
[epoch 14, batch  1499] avg loss: 0.508599
[epoch 14, batch  1599] avg loss: 0.531344
[epoch 14, batch  1699] avg loss: 0.498850
[epoch 14, batch  1799] avg loss: 0.503590
[epoch 14, batch  1899] avg loss: 0.518362
[epoch 14, batch  1999] avg loss: 0.523379
[epoch 14, batch  2099] avg loss: 0.508490
[epoch 14, batch  2199] avg loss: 0.530252
[epoch 14, batch  2299] avg loss: 0.513438
[epoch 14, batch  2399] avg loss: 0.517387
[epoch 15, batch    99] avg loss: 0.510236
[epoch 15, batch   199] avg loss: 0.502982
[epoch 15, batch   299] avg loss: 0.506469
[epoch 15, batch   399] avg loss: 0.515273
[epoch 15, batch   499] avg loss: 0.503309
[epoch 15, batch   599] avg loss: 0.507147
[epoch 15, batch   699] avg loss: 0.511865
[epoch 15, batch   799] avg loss: 0.508223
[epoch 15, batch   899] avg loss: 0.538393
[epoch 15, batch   999] avg loss: 0.511335
[epoch 15, batch  1099] avg loss: 0.508777
[epoch 15, batch  1199] avg loss: 0.515753
[epoch 15, batch  1299] avg loss: 0.497608
[epoch 15, batch  1399] avg loss: 0.513229
[epoch 15, batch  1499] avg loss: 0.517228
[epoch 15, batch  1599] avg loss: 0.511806
[epoch 15, batch  1699] avg loss: 0.510602
[epoch 15, batch  1799] avg loss: 0.507521
[epoch 15, batch  1899] avg loss: 0.516235
[epoch 15, batch  1999] avg loss: 0.501865
[epoch 15, batch  2099] avg loss: 0.514606
[epoch 15, batch  2199] avg loss: 0.497588
[epoch 15, batch  2299] avg loss: 0.517164
[epoch 15, batch  2399] avg loss: 0.509096
[epoch 16, batch    99] avg loss: 0.496350
[epoch 16, batch   199] avg loss: 0.508526
[epoch 16, batch   299] avg loss: 0.509986
[epoch 16, batch   399] avg loss: 0.502597
[epoch 16, batch   499] avg loss: 0.491774
[epoch 16, batch   599] avg loss: 0.526006
[epoch 16, batch   699] avg loss: 0.495175
[epoch 16, batch   799] avg loss: 0.499847
[epoch 16, batch   899] avg loss: 0.509915
[epoch 16, batch   999] avg loss: 0.510913
[epoch 16, batch  1099] avg loss: 0.508175
[epoch 16, batch  1199] avg loss: 0.504421
[epoch 16, batch  1299] avg loss: 0.506250
[epoch 16, batch  1399] avg loss: 0.505657
[epoch 16, batch  1499] avg loss: 0.514827
[epoch 16, batch  1599] avg loss: 0.503346
[epoch 16, batch  1699] avg loss: 0.507646
[epoch 16, batch  1799] avg loss: 0.512211
[epoch 16, batch  1899] avg loss: 0.501376
[epoch 16, batch  1999] avg loss: 0.516249
[epoch 16, batch  2099] avg loss: 0.503909
[epoch 16, batch  2199] avg loss: 0.494495
[epoch 16, batch  2299] avg loss: 0.512134
[epoch 16, batch  2399] avg loss: 0.507019
[epoch 17, batch    99] avg loss: 0.516725
[epoch 17, batch   199] avg loss: 0.499722
[epoch 17, batch   299] avg loss: 0.497431
[epoch 17, batch   399] avg loss: 0.504170
[epoch 17, batch   499] avg loss: 0.507041
[epoch 17, batch   599] avg loss: 0.505977
[epoch 17, batch   699] avg loss: 0.511386
[epoch 17, batch   799] avg loss: 0.499642
[epoch 17, batch   899] avg loss: 0.494069
[epoch 17, batch   999] avg loss: 0.494864
[epoch 17, batch  1099] avg loss: 0.495024
[epoch 17, batch  1199] avg loss: 0.502175
[epoch 17, batch  1299] avg loss: 0.506449
[epoch 17, batch  1399] avg loss: 0.485659
[epoch 17, batch  1499] avg loss: 0.512253
[epoch 17, batch  1599] avg loss: 0.492750
[epoch 17, batch  1699] avg loss: 0.503518
[epoch 17, batch  1799] avg loss: 0.487648
[epoch 17, batch  1899] avg loss: 0.498846
[epoch 17, batch  1999] avg loss: 0.496181
[epoch 17, batch  2099] avg loss: 0.502854
[epoch 17, batch  2199] avg loss: 0.511415
[epoch 17, batch  2299] avg loss: 0.509860
[epoch 17, batch  2399] avg loss: 0.511342
[epoch 18, batch    99] avg loss: 0.495205
[epoch 18, batch   199] avg loss: 0.499903
[epoch 18, batch   299] avg loss: 0.490873
[epoch 18, batch   399] avg loss: 0.492787
[epoch 18, batch   499] avg loss: 0.505798
[epoch 18, batch   599] avg loss: 0.501868
[epoch 18, batch   699] avg loss: 0.489348
[epoch 18, batch   799] avg loss: 0.496985
[epoch 18, batch   899] avg loss: 0.521565
[epoch 18, batch   999] avg loss: 0.484391
[epoch 18, batch  1099] avg loss: 0.503842
[epoch 18, batch  1199] avg loss: 0.503302
[epoch 18, batch  1299] avg loss: 0.501696
[epoch 18, batch  1399] avg loss: 0.491179
[epoch 18, batch  1499] avg loss: 0.500684
[epoch 18, batch  1599] avg loss: 0.498837
[epoch 18, batch  1699] avg loss: 0.495464
[epoch 18, batch  1799] avg loss: 0.505879
[epoch 18, batch  1899] avg loss: 0.504931
[epoch 18, batch  1999] avg loss: 0.488473
[epoch 18, batch  2099] avg loss: 0.482521
[epoch 18, batch  2199] avg loss: 0.502204
[epoch 18, batch  2299] avg loss: 0.494256
[epoch 18, batch  2399] avg loss: 0.493585
[epoch 19, batch    99] avg loss: 0.499494
[epoch 19, batch   199] avg loss: 0.500666
[epoch 19, batch   299] avg loss: 0.494222
[epoch 19, batch   399] avg loss: 0.512872
[epoch 19, batch   499] avg loss: 0.505188
[epoch 19, batch   599] avg loss: 0.494010
[epoch 19, batch   699] avg loss: 0.495397
[epoch 19, batch   799] avg loss: 0.509411
[epoch 19, batch   899] avg loss: 0.484929
[epoch 19, batch   999] avg loss: 0.491899
[epoch 19, batch  1099] avg loss: 0.495019
[epoch 19, batch  1199] avg loss: 0.489208
[epoch 19, batch  1299] avg loss: 0.492395
[epoch 19, batch  1399] avg loss: 0.489843
[epoch 19, batch  1499] avg loss: 0.511977
[epoch 19, batch  1599] avg loss: 0.496767
[epoch 19, batch  1699] avg loss: 0.494057
[epoch 19, batch  1799] avg loss: 0.499491
[epoch 19, batch  1899] avg loss: 0.501444
[epoch 19, batch  1999] avg loss: 0.493069
[epoch 19, batch  2099] avg loss: 0.492459
[epoch 19, batch  2199] avg loss: 0.487801
[epoch 19, batch  2299] avg loss: 0.502789
[epoch 19, batch  2399] avg loss: 0.493552
[epoch 20, batch    99] avg loss: 0.481814
[epoch 20, batch   199] avg loss: 0.506100
[epoch 20, batch   299] avg loss: 0.499921
[epoch 20, batch   399] avg loss: 0.494678
[epoch 20, batch   499] avg loss: 0.483407
[epoch 20, batch   599] avg loss: 0.509221
[epoch 20, batch   699] avg loss: 0.484277
[epoch 20, batch   799] avg loss: 0.487610
[epoch 20, batch   899] avg loss: 0.491591
[epoch 20, batch   999] avg loss: 0.494314
[epoch 20, batch  1099] avg loss: 0.496621
[epoch 20, batch  1199] avg loss: 0.504023
[epoch 20, batch  1299] avg loss: 0.488069
[epoch 20, batch  1399] avg loss: 0.507766
[epoch 20, batch  1499] avg loss: 0.501007
[epoch 20, batch  1599] avg loss: 0.492631
[epoch 20, batch  1699] avg loss: 0.478116
[epoch 20, batch  1799] avg loss: 0.482481
[epoch 20, batch  1899] avg loss: 0.491211
[epoch 20, batch  1999] avg loss: 0.483839
[epoch 20, batch  2099] avg loss: 0.490346
[epoch 20, batch  2199] avg loss: 0.502162
[epoch 20, batch  2299] avg loss: 0.502118
[epoch 20, batch  2399] avg loss: 0.488614
[epoch 21, batch    99] avg loss: 0.503030
[epoch 21, batch   199] avg loss: 0.490690
[epoch 21, batch   299] avg loss: 0.494160
[epoch 21, batch   399] avg loss: 0.484740
[epoch 21, batch   499] avg loss: 0.490288
[epoch 21, batch   599] avg loss: 0.487309
[epoch 21, batch   699] avg loss: 0.485112
[epoch 21, batch   799] avg loss: 0.483086
[epoch 21, batch   899] avg loss: 0.500478
[epoch 21, batch   999] avg loss: 0.488520
[epoch 21, batch  1099] avg loss: 0.494893
[epoch 21, batch  1199] avg loss: 0.499319
[epoch 21, batch  1299] avg loss: 0.484246
[epoch 21, batch  1399] avg loss: 0.506139
[epoch 21, batch  1499] avg loss: 0.477724
[epoch 21, batch  1599] avg loss: 0.482871
[epoch 21, batch  1699] avg loss: 0.485719
[epoch 21, batch  1799] avg loss: 0.489693
[epoch 21, batch  1899] avg loss: 0.506539
[epoch 21, batch  1999] avg loss: 0.480627
[epoch 21, batch  2099] avg loss: 0.510632
[epoch 21, batch  2199] avg loss: 0.487142
[epoch 21, batch  2299] avg loss: 0.471189
[epoch 21, batch  2399] avg loss: 0.490203
[epoch 22, batch    99] avg loss: 0.488127
[epoch 22, batch   199] avg loss: 0.483758
[epoch 22, batch   299] avg loss: 0.501465
[epoch 22, batch   399] avg loss: 0.478470
[epoch 22, batch   499] avg loss: 0.514146
[epoch 22, batch   599] avg loss: 0.487706
[epoch 22, batch   699] avg loss: 0.485339
[epoch 22, batch   799] avg loss: 0.496275
[epoch 22, batch   899] avg loss: 0.483458
[epoch 22, batch   999] avg loss: 0.494706
[epoch 22, batch  1099] avg loss: 0.491745
[epoch 22, batch  1199] avg loss: 0.475527
[epoch 22, batch  1299] avg loss: 0.487472
[epoch 22, batch  1399] avg loss: 0.493568
[epoch 22, batch  1499] avg loss: 0.491562
[epoch 22, batch  1599] avg loss: 0.492094
[epoch 22, batch  1699] avg loss: 0.481915
[epoch 22, batch  1799] avg loss: 0.492356
[epoch 22, batch  1899] avg loss: 0.496188
[epoch 22, batch  1999] avg loss: 0.485372
[epoch 22, batch  2099] avg loss: 0.485581
[epoch 22, batch  2199] avg loss: 0.485082
[epoch 22, batch  2299] avg loss: 0.484715
[epoch 22, batch  2399] avg loss: 0.491704
[epoch 23, batch    99] avg loss: 0.485870
[epoch 23, batch   199] avg loss: 0.482641
[epoch 23, batch   299] avg loss: 0.483056
[epoch 23, batch   399] avg loss: 0.489729
[epoch 23, batch   499] avg loss: 0.471600
[epoch 23, batch   599] avg loss: 0.483896
[epoch 23, batch   699] avg loss: 0.485663
[epoch 23, batch   799] avg loss: 0.486810
[epoch 23, batch   899] avg loss: 0.476159
[epoch 23, batch   999] avg loss: 0.484174
[epoch 23, batch  1099] avg loss: 0.476237
[epoch 23, batch  1199] avg loss: 0.479387
[epoch 23, batch  1299] avg loss: 0.493581
[epoch 23, batch  1399] avg loss: 0.493450
[epoch 23, batch  1499] avg loss: 0.485112
[epoch 23, batch  1599] avg loss: 0.470582
[epoch 23, batch  1699] avg loss: 0.489308
[epoch 23, batch  1799] avg loss: 0.493466
[epoch 23, batch  1899] avg loss: 0.497492
[epoch 23, batch  1999] avg loss: 0.478890
[epoch 23, batch  2099] avg loss: 0.491503
[epoch 23, batch  2199] avg loss: 0.483011
[epoch 23, batch  2299] avg loss: 0.483538
[epoch 23, batch  2399] avg loss: 0.473568
[epoch 24, batch    99] avg loss: 0.483978
[epoch 24, batch   199] avg loss: 0.486640
[epoch 24, batch   299] avg loss: 0.485971
[epoch 24, batch   399] avg loss: 0.482707
[epoch 24, batch   499] avg loss: 0.486650
[epoch 24, batch   599] avg loss: 0.483748
[epoch 24, batch   699] avg loss: 0.491041
[epoch 24, batch   799] avg loss: 0.483371
[epoch 24, batch   899] avg loss: 0.475193
[epoch 24, batch   999] avg loss: 0.493495
[epoch 24, batch  1099] avg loss: 0.475351
[epoch 24, batch  1199] avg loss: 0.480991
[epoch 24, batch  1299] avg loss: 0.487151
[epoch 24, batch  1399] avg loss: 0.489383
[epoch 24, batch  1499] avg loss: 0.478317
[epoch 24, batch  1599] avg loss: 0.485551
[epoch 24, batch  1699] avg loss: 0.483551
[epoch 24, batch  1799] avg loss: 0.476661
[epoch 24, batch  1899] avg loss: 0.480112
[epoch 24, batch  1999] avg loss: 0.477253
[epoch 24, batch  2099] avg loss: 0.486954
[epoch 24, batch  2199] avg loss: 0.486904
[epoch 24, batch  2299] avg loss: 0.476470
[epoch 24, batch  2399] avg loss: 0.482597
[epoch 25, batch    99] avg loss: 0.465865
[epoch 25, batch   199] avg loss: 0.483809
[epoch 25, batch   299] avg loss: 0.496563
[epoch 25, batch   399] avg loss: 0.470460
[epoch 25, batch   499] avg loss: 0.461998
[epoch 25, batch   599] avg loss: 0.491988
[epoch 25, batch   699] avg loss: 0.489990
[epoch 25, batch   799] avg loss: 0.480642
[epoch 25, batch   899] avg loss: 0.483537
[epoch 25, batch   999] avg loss: 0.483356
[epoch 25, batch  1099] avg loss: 0.474748
[epoch 25, batch  1199] avg loss: 0.472946
[epoch 25, batch  1299] avg loss: 0.491021
[epoch 25, batch  1399] avg loss: 0.476800
[epoch 25, batch  1499] avg loss: 0.483518
[epoch 25, batch  1599] avg loss: 0.481700
[epoch 25, batch  1699] avg loss: 0.475771
[epoch 25, batch  1799] avg loss: 0.495858
[epoch 25, batch  1899] avg loss: 0.486703
[epoch 25, batch  1999] avg loss: 0.483894
[epoch 25, batch  2099] avg loss: 0.485606
[epoch 25, batch  2199] avg loss: 0.471827
[epoch 25, batch  2299] avg loss: 0.476371
[epoch 25, batch  2399] avg loss: 0.477516
[epoch 26, batch    99] avg loss: 0.487162
[epoch 26, batch   199] avg loss: 0.465397
[epoch 26, batch   299] avg loss: 0.479500
[epoch 26, batch   399] avg loss: 0.472841
[epoch 26, batch   499] avg loss: 0.475978
[epoch 26, batch   599] avg loss: 0.491353
[epoch 26, batch   699] avg loss: 0.481474
[epoch 26, batch   799] avg loss: 0.479248
[epoch 26, batch   899] avg loss: 0.475103
[epoch 26, batch   999] avg loss: 0.479150
[epoch 26, batch  1099] avg loss: 0.493657
[epoch 26, batch  1199] avg loss: 0.483460
[epoch 26, batch  1299] avg loss: 0.468451
[epoch 26, batch  1399] avg loss: 0.501517
[epoch 26, batch  1499] avg loss: 0.468304
[epoch 26, batch  1599] avg loss: 0.466284
[epoch 26, batch  1699] avg loss: 0.496750
[epoch 26, batch  1799] avg loss: 0.490031
[epoch 26, batch  1899] avg loss: 0.479132
[epoch 26, batch  1999] avg loss: 0.473436
[epoch 26, batch  2099] avg loss: 0.493570
[epoch 26, batch  2199] avg loss: 0.464386
[epoch 26, batch  2299] avg loss: 0.473265
[epoch 26, batch  2399] avg loss: 0.481266
[epoch 27, batch    99] avg loss: 0.486221
[epoch 27, batch   199] avg loss: 0.470979
[epoch 27, batch   299] avg loss: 0.466943
[epoch 27, batch   399] avg loss: 0.479098
[epoch 27, batch   499] avg loss: 0.477298
[epoch 27, batch   599] avg loss: 0.486789
[epoch 27, batch   699] avg loss: 0.485118
[epoch 27, batch   799] avg loss: 0.474142
[epoch 27, batch   899] avg loss: 0.471578
[epoch 27, batch   999] avg loss: 0.485119
[epoch 27, batch  1099] avg loss: 0.472091
[epoch 27, batch  1199] avg loss: 0.471293
[epoch 27, batch  1299] avg loss: 0.481205
[epoch 27, batch  1399] avg loss: 0.478770
[epoch 27, batch  1499] avg loss: 0.476024
[epoch 27, batch  1599] avg loss: 0.475481
[epoch 27, batch  1699] avg loss: 0.497368
[epoch 27, batch  1799] avg loss: 0.481560
[epoch 27, batch  1899] avg loss: 0.480639
[epoch 27, batch  1999] avg loss: 0.482843
[epoch 27, batch  2099] avg loss: 0.483442
[epoch 27, batch  2199] avg loss: 0.470363
[epoch 27, batch  2299] avg loss: 0.481600
[epoch 27, batch  2399] avg loss: 0.471834
[epoch 28, batch    99] avg loss: 0.495111
[epoch 28, batch   199] avg loss: 0.476741
[epoch 28, batch   299] avg loss: 0.482267
[epoch 28, batch   399] avg loss: 0.473691
[epoch 28, batch   499] avg loss: 0.471670
[epoch 28, batch   599] avg loss: 0.476793
[epoch 28, batch   699] avg loss: 0.476507
[epoch 28, batch   799] avg loss: 0.476019
[epoch 28, batch   899] avg loss: 0.489273
[epoch 28, batch   999] avg loss: 0.469610
[epoch 28, batch  1099] avg loss: 0.466415
[epoch 28, batch  1199] avg loss: 0.482092
[epoch 28, batch  1299] avg loss: 0.487595
[epoch 28, batch  1399] avg loss: 0.466585
[epoch 28, batch  1499] avg loss: 0.477620
[epoch 28, batch  1599] avg loss: 0.480673
[epoch 28, batch  1699] avg loss: 0.483156
[epoch 28, batch  1799] avg loss: 0.461443
[epoch 28, batch  1899] avg loss: 0.491249
[epoch 28, batch  1999] avg loss: 0.487910
[epoch 28, batch  2099] avg loss: 0.464204
[epoch 28, batch  2199] avg loss: 0.468361
[epoch 28, batch  2299] avg loss: 0.479414
[epoch 28, batch  2399] avg loss: 0.460780
[epoch 29, batch    99] avg loss: 0.473265
[epoch 29, batch   199] avg loss: 0.478620
[epoch 29, batch   299] avg loss: 0.469996
[epoch 29, batch   399] avg loss: 0.471499
[epoch 29, batch   499] avg loss: 0.462879
[epoch 29, batch   599] avg loss: 0.479715
[epoch 29, batch   699] avg loss: 0.466139
[epoch 29, batch   799] avg loss: 0.474067
[epoch 29, batch   899] avg loss: 0.483295
[epoch 29, batch   999] avg loss: 0.470792
[epoch 29, batch  1099] avg loss: 0.478230
[epoch 29, batch  1199] avg loss: 0.469565
[epoch 29, batch  1299] avg loss: 0.474410
[epoch 29, batch  1399] avg loss: 0.474137
[epoch 29, batch  1499] avg loss: 0.480766
[epoch 29, batch  1599] avg loss: 0.463272
[epoch 29, batch  1699] avg loss: 0.486611
[epoch 29, batch  1799] avg loss: 0.466645
[epoch 29, batch  1899] avg loss: 0.484821
[epoch 29, batch  1999] avg loss: 0.463996
[epoch 29, batch  2099] avg loss: 0.478056
[epoch 29, batch  2199] avg loss: 0.469768
[epoch 29, batch  2299] avg loss: 0.479619
[epoch 29, batch  2399] avg loss: 0.476159
[epoch 30, batch    99] avg loss: 0.472684
[epoch 30, batch   199] avg loss: 0.479570
[epoch 30, batch   299] avg loss: 0.475127
[epoch 30, batch   399] avg loss: 0.480440
[epoch 30, batch   499] avg loss: 0.467833
[epoch 30, batch   599] avg loss: 0.463780
[epoch 30, batch   699] avg loss: 0.461785
[epoch 30, batch   799] avg loss: 0.471892
[epoch 30, batch   899] avg loss: 0.486885
[epoch 30, batch   999] avg loss: 0.472506
[epoch 30, batch  1099] avg loss: 0.481931
[epoch 30, batch  1199] avg loss: 0.479497
[epoch 30, batch  1299] avg loss: 0.464273
[epoch 30, batch  1399] avg loss: 0.479545
[epoch 30, batch  1499] avg loss: 0.465287
[epoch 30, batch  1599] avg loss: 0.470070
[epoch 30, batch  1699] avg loss: 0.480165
[epoch 30, batch  1799] avg loss: 0.482597
[epoch 30, batch  1899] avg loss: 0.463124
[epoch 30, batch  1999] avg loss: 0.464303
[epoch 30, batch  2099] avg loss: 0.467784
[epoch 30, batch  2199] avg loss: 0.466298
[epoch 30, batch  2299] avg loss: 0.469496
[epoch 30, batch  2399] avg loss: 0.476809
[epoch 31, batch    99] avg loss: 0.465407
[epoch 31, batch   199] avg loss: 0.481036
[epoch 31, batch   299] avg loss: 0.463445
[epoch 31, batch   399] avg loss: 0.459420
[epoch 31, batch   499] avg loss: 0.473121
[epoch 31, batch   599] avg loss: 0.486091
[epoch 31, batch   699] avg loss: 0.474086
[epoch 31, batch   799] avg loss: 0.476061
[epoch 31, batch   899] avg loss: 0.471865
[epoch 31, batch   999] avg loss: 0.460705
[epoch 31, batch  1099] avg loss: 0.474156
[epoch 31, batch  1199] avg loss: 0.480542
[epoch 31, batch  1299] avg loss: 0.482060
[epoch 31, batch  1399] avg loss: 0.476117
[epoch 31, batch  1499] avg loss: 0.455882
[epoch 31, batch  1599] avg loss: 0.464023
[epoch 31, batch  1699] avg loss: 0.457480
[epoch 31, batch  1799] avg loss: 0.479537
[epoch 31, batch  1899] avg loss: 0.464766
[epoch 31, batch  1999] avg loss: 0.455228
[epoch 31, batch  2099] avg loss: 0.479230
[epoch 31, batch  2199] avg loss: 0.467969
[epoch 31, batch  2299] avg loss: 0.471218
[epoch 31, batch  2399] avg loss: 0.480197
[epoch 32, batch    99] avg loss: 0.471791
[epoch 32, batch   199] avg loss: 0.453499
[epoch 32, batch   299] avg loss: 0.471490
[epoch 32, batch   399] avg loss: 0.471592
[epoch 32, batch   499] avg loss: 0.469122
[epoch 32, batch   599] avg loss: 0.455511
[epoch 32, batch   699] avg loss: 0.460342
[epoch 32, batch   799] avg loss: 0.464535
[epoch 32, batch   899] avg loss: 0.476156
[epoch 32, batch   999] avg loss: 0.467902
[epoch 32, batch  1099] avg loss: 0.464317
[epoch 32, batch  1199] avg loss: 0.467222
[epoch 32, batch  1299] avg loss: 0.470744
[epoch 32, batch  1399] avg loss: 0.465473
[epoch 32, batch  1499] avg loss: 0.475583
[epoch 32, batch  1599] avg loss: 0.471880
[epoch 32, batch  1699] avg loss: 0.467528
[epoch 32, batch  1799] avg loss: 0.471531
[epoch 32, batch  1899] avg loss: 0.456350
[epoch 32, batch  1999] avg loss: 0.460398
[epoch 32, batch  2099] avg loss: 0.464539
[epoch 32, batch  2199] avg loss: 0.470015
[epoch 32, batch  2299] avg loss: 0.465864
[epoch 32, batch  2399] avg loss: 0.469537
[epoch 33, batch    99] avg loss: 0.463239
[epoch 33, batch   199] avg loss: 0.462154
[epoch 33, batch   299] avg loss: 0.479641
[epoch 33, batch   399] avg loss: 0.466628
[epoch 33, batch   499] avg loss: 0.475920
[epoch 33, batch   599] avg loss: 0.470470
[epoch 33, batch   699] avg loss: 0.479171
[epoch 33, batch   799] avg loss: 0.476426
[epoch 33, batch   899] avg loss: 0.475181
[epoch 33, batch   999] avg loss: 0.459299
[epoch 33, batch  1099] avg loss: 0.465036
[epoch 33, batch  1199] avg loss: 0.463526
[epoch 33, batch  1299] avg loss: 0.473085
[epoch 33, batch  1399] avg loss: 0.463939
[epoch 33, batch  1499] avg loss: 0.470584
[epoch 33, batch  1599] avg loss: 0.459537
[epoch 33, batch  1699] avg loss: 0.465513
[epoch 33, batch  1799] avg loss: 0.458187
[epoch 33, batch  1899] avg loss: 0.466433
[epoch 33, batch  1999] avg loss: 0.463294
[epoch 33, batch  2099] avg loss: 0.449744
[epoch 33, batch  2199] avg loss: 0.465333
[epoch 33, batch  2299] avg loss: 0.483890
[epoch 33, batch  2399] avg loss: 0.471096
[epoch 34, batch    99] avg loss: 0.478852
[epoch 34, batch   199] avg loss: 0.474053
[epoch 34, batch   299] avg loss: 0.479792
[epoch 34, batch   399] avg loss: 0.475248
[epoch 34, batch   499] avg loss: 0.451051
[epoch 34, batch   599] avg loss: 0.446022
[epoch 34, batch   699] avg loss: 0.474330
[epoch 34, batch   799] avg loss: 0.465309
[epoch 34, batch   899] avg loss: 0.451646
[epoch 34, batch   999] avg loss: 0.459794
[epoch 34, batch  1099] avg loss: 0.462680
[epoch 34, batch  1199] avg loss: 0.469380
[epoch 34, batch  1299] avg loss: 0.459740
[epoch 34, batch  1399] avg loss: 0.472995
[epoch 34, batch  1499] avg loss: 0.450730
[epoch 34, batch  1599] avg loss: 0.461163
[epoch 34, batch  1699] avg loss: 0.488051
[epoch 34, batch  1799] avg loss: 0.457268
[epoch 34, batch  1899] avg loss: 0.466301
[epoch 34, batch  1999] avg loss: 0.468874
[epoch 34, batch  2099] avg loss: 0.463797
[epoch 34, batch  2199] avg loss: 0.458718
[epoch 34, batch  2299] avg loss: 0.459202
[epoch 34, batch  2399] avg loss: 0.454786
[epoch 35, batch    99] avg loss: 0.460303
[epoch 35, batch   199] avg loss: 0.465233
[epoch 35, batch   299] avg loss: 0.444251
[epoch 35, batch   399] avg loss: 0.482287
[epoch 35, batch   499] avg loss: 0.454566
[epoch 35, batch   599] avg loss: 0.469271
[epoch 35, batch   699] avg loss: 0.460002
[epoch 35, batch   799] avg loss: 0.465952
[epoch 35, batch   899] avg loss: 0.466361
[epoch 35, batch   999] avg loss: 0.451589
[epoch 35, batch  1099] avg loss: 0.467290
[epoch 35, batch  1199] avg loss: 0.497983
[epoch 35, batch  1299] avg loss: 0.464101
[epoch 35, batch  1399] avg loss: 0.454310
[epoch 35, batch  1499] avg loss: 0.468673
[epoch 35, batch  1599] avg loss: 0.469115
[epoch 35, batch  1699] avg loss: 0.482946
[epoch 35, batch  1799] avg loss: 0.476393
[epoch 35, batch  1899] avg loss: 0.460311
[epoch 35, batch  1999] avg loss: 0.445743
[epoch 35, batch  2099] avg loss: 0.475048
[epoch 35, batch  2199] avg loss: 0.457881
[epoch 35, batch  2299] avg loss: 0.461861
[epoch 35, batch  2399] avg loss: 0.453148
[epoch 36, batch    99] avg loss: 0.466366
[epoch 36, batch   199] avg loss: 0.458502
[epoch 36, batch   299] avg loss: 0.466907
[epoch 36, batch   399] avg loss: 0.468883
[epoch 36, batch   499] avg loss: 0.452717
[epoch 36, batch   599] avg loss: 0.461054
[epoch 36, batch   699] avg loss: 0.460711
[epoch 36, batch   799] avg loss: 0.464059
[epoch 36, batch   899] avg loss: 0.454815
[epoch 36, batch   999] avg loss: 0.456820
[epoch 36, batch  1099] avg loss: 0.460580
[epoch 36, batch  1199] avg loss: 0.456548
[epoch 36, batch  1299] avg loss: 0.462179
[epoch 36, batch  1399] avg loss: 0.453688
[epoch 36, batch  1499] avg loss: 0.465644
[epoch 36, batch  1599] avg loss: 0.464886
[epoch 36, batch  1699] avg loss: 0.473630
[epoch 36, batch  1799] avg loss: 0.459206
[epoch 36, batch  1899] avg loss: 0.459703
[epoch 36, batch  1999] avg loss: 0.468459
[epoch 36, batch  2099] avg loss: 0.449334
[epoch 36, batch  2199] avg loss: 0.465952
[epoch 36, batch  2299] avg loss: 0.454045
[epoch 36, batch  2399] avg loss: 0.455744
[epoch 37, batch    99] avg loss: 0.470392
[epoch 37, batch   199] avg loss: 0.483731
[epoch 37, batch   299] avg loss: 0.458981
[epoch 37, batch   399] avg loss: 0.467301
[epoch 37, batch   499] avg loss: 0.480051
[epoch 37, batch   599] avg loss: 0.453568
[epoch 37, batch   699] avg loss: 0.466720
[epoch 37, batch   799] avg loss: 0.457832
[epoch 37, batch   899] avg loss: 0.466458
[epoch 37, batch   999] avg loss: 0.467654
[epoch 37, batch  1099] avg loss: 0.472067
[epoch 37, batch  1199] avg loss: 0.464907
[epoch 37, batch  1299] avg loss: 0.452209
[epoch 37, batch  1399] avg loss: 0.445851
[epoch 37, batch  1499] avg loss: 0.464035
[epoch 37, batch  1599] avg loss: 0.459129
[epoch 37, batch  1699] avg loss: 0.459522
[epoch 37, batch  1799] avg loss: 0.467342
[epoch 37, batch  1899] avg loss: 0.463918
[epoch 37, batch  1999] avg loss: 0.464307
[epoch 37, batch  2099] avg loss: 0.439081
[epoch 37, batch  2199] avg loss: 0.464397
[epoch 37, batch  2299] avg loss: 0.460969
[epoch 37, batch  2399] avg loss: 0.460099
[epoch 38, batch    99] avg loss: 0.463480
[epoch 38, batch   199] avg loss: 0.450019
[epoch 38, batch   299] avg loss: 0.475392
[epoch 38, batch   399] avg loss: 0.456298
[epoch 38, batch   499] avg loss: 0.461100
[epoch 38, batch   599] avg loss: 0.469217
[epoch 38, batch   699] avg loss: 0.440933
[epoch 38, batch   799] avg loss: 0.451113
[epoch 38, batch   899] avg loss: 0.451351
[epoch 38, batch   999] avg loss: 0.467876
[epoch 38, batch  1099] avg loss: 0.452914
[epoch 38, batch  1199] avg loss: 0.468046
[epoch 38, batch  1299] avg loss: 0.462058
[epoch 38, batch  1399] avg loss: 0.457887
[epoch 38, batch  1499] avg loss: 0.450666
[epoch 38, batch  1599] avg loss: 0.467396
[epoch 38, batch  1699] avg loss: 0.450000
[epoch 38, batch  1799] avg loss: 0.455950
[epoch 38, batch  1899] avg loss: 0.455813
[epoch 38, batch  1999] avg loss: 0.463392
[epoch 38, batch  2099] avg loss: 0.452551
[epoch 38, batch  2199] avg loss: 0.458375
[epoch 38, batch  2299] avg loss: 0.461784
[epoch 38, batch  2399] avg loss: 0.462822
[epoch 39, batch    99] avg loss: 0.449555
[epoch 39, batch   199] avg loss: 0.453106
[epoch 39, batch   299] avg loss: 0.443972
[epoch 39, batch   399] avg loss: 0.464618
[epoch 39, batch   499] avg loss: 0.450908
[epoch 39, batch   599] avg loss: 0.453450
[epoch 39, batch   699] avg loss: 0.468715
[epoch 39, batch   799] avg loss: 0.465958
[epoch 39, batch   899] avg loss: 0.456891
[epoch 39, batch   999] avg loss: 0.456295
[epoch 39, batch  1099] avg loss: 0.443963
[epoch 39, batch  1199] avg loss: 0.446852
[epoch 39, batch  1299] avg loss: 0.444599
[epoch 39, batch  1399] avg loss: 0.465040
[epoch 39, batch  1499] avg loss: 0.462813
[epoch 39, batch  1599] avg loss: 0.452353
[epoch 39, batch  1699] avg loss: 0.467866
[epoch 39, batch  1799] avg loss: 0.453602
[epoch 39, batch  1899] avg loss: 0.448535
[epoch 39, batch  1999] avg loss: 0.455170
[epoch 39, batch  2099] avg loss: 0.451150
[epoch 39, batch  2199] avg loss: 0.458452
[epoch 39, batch  2299] avg loss: 0.453715
[epoch 39, batch  2399] avg loss: 0.457994
Model saved to model/20200502-233629.pth.
accuracy/TriangPrismIsosc : 0.77
n_examples/TriangPrismIsosc : 500.0
accuracy/parallelepiped : 0.158
n_examples/parallelepiped : 500.0
accuracy/sphere : 1.0
n_examples/sphere : 102.0
accuracy/wire : 0.97
n_examples/wire : 200.0
accuracy/avg_geom : 0.5837173579109063
loss/validation_geom : 0.9916831176218716
accuracy/Au : 0.0
n_examples/Au : 0.0
accuracy/SiN : 0.0
n_examples/SiN : 1302.0
accuracy/SiO2 : 0.0
n_examples/SiO2 : 0.0
accuracy/avg_mat : 0.0
loss/validation_mat : 1.8665982200985862
MSE/ShortestDim : 3.632357918172388
MAE/ShortestDim : 1.145571792180637
MSE/MiddleDim : 5.671058607907156
MAE/MiddleDim : 1.4960690511536487
MSE/LongDim : 176.72857769147225
MAE/LongDim : 7.752315843526485
MSE/log Area/Vol : 7.286531638806133
MAE/log Area/Vol : 2.428041979647635
loss/validation_dim : 193.31852585635795
loss/validation : 196.1768071940784
Metrics saved to model/20200502-233629_metrics.csv.
Parsed 2604 rows from data/sim_train_spectrum_SiN.
Parsed 2604 rows from data/sim_train_labels_SiN.
Parsed 9765 rows from data/gen_spectrum_SiN_00-of-16.
Parsed 9765 rows from data/gen_labels_SiN_00-of-16.
Parsed 9765 rows from data/gen_spectrum_SiN_01-of-16.
Parsed 9765 rows from data/gen_labels_SiN_01-of-16.
Parsed 9765 rows from data/gen_spectrum_SiN_02-of-16.
Parsed 9765 rows from data/gen_labels_SiN_02-of-16.
Parsed 9765 rows from data/gen_spectrum_SiN_03-of-16.
Parsed 9765 rows from data/gen_labels_SiN_03-of-16.
Parsed 9765 rows from data/gen_spectrum_SiN_04-of-16.
Parsed 9765 rows from data/gen_labels_SiN_04-of-16.
Parsed 9765 rows from data/gen_spectrum_SiN_05-of-16.
Parsed 9765 rows from data/gen_labels_SiN_05-of-16.
Parsed 9765 rows from data/gen_spectrum_SiN_06-of-16.
Parsed 9765 rows from data/gen_labels_SiN_06-of-16.
Parsed 9765 rows from data/gen_spectrum_SiN_07-of-16.
Parsed 9765 rows from data/gen_labels_SiN_07-of-16.
Parsed 9765 rows from data/gen_spectrum_SiN_08-of-16.
Parsed 9765 rows from data/gen_labels_SiN_08-of-16.
Parsed 9765 rows from data/gen_spectrum_SiN_09-of-16.
Parsed 9765 rows from data/gen_labels_SiN_09-of-16.
Parsed 9765 rows from data/gen_spectrum_SiN_10-of-16.
Parsed 9765 rows from data/gen_labels_SiN_10-of-16.
Parsed 9765 rows from data/gen_spectrum_SiN_11-of-16.
Parsed 9765 rows from data/gen_labels_SiN_11-of-16.
Parsed 9765 rows from data/gen_spectrum_SiN_12-of-16.
Parsed 9765 rows from data/gen_labels_SiN_12-of-16.
Parsed 9765 rows from data/gen_spectrum_SiN_13-of-16.
Parsed 9765 rows from data/gen_labels_SiN_13-of-16.
Parsed 9765 rows from data/gen_spectrum_SiN_14-of-16.
Parsed 9765 rows from data/gen_labels_SiN_14-of-16.
Parsed 9765 rows from data/gen_spectrum_SiN_15-of-16.
Parsed 9765 rows from data/gen_labels_SiN_15-of-16.
Parsed 1302 rows from data/sim_validation_spectrum_SiN.
Parsed 1302 rows from data/sim_validation_labels_SiN.
Logging training progress to tensorboard dir runs/alexnet-SiN-lr_0.000010-trainsize_158844-05_02_2020_23:37-joint.
[epoch 0, batch    99] avg loss: 1.388250
[epoch 0, batch   199] avg loss: 1.387163
[epoch 0, batch   299] avg loss: 1.387629
[epoch 0, batch   399] avg loss: 1.387702
[epoch 0, batch   499] avg loss: 1.387726
[epoch 0, batch   599] avg loss: 1.387330
[epoch 0, batch   699] avg loss: 1.387096
[epoch 0, batch   799] avg loss: 1.386792
[epoch 0, batch   899] avg loss: 1.386310
[epoch 0, batch   999] avg loss: 1.385592
[epoch 0, batch  1099] avg loss: 1.384526
[epoch 0, batch  1199] avg loss: 1.380880
[epoch 0, batch  1299] avg loss: 1.376000
[epoch 0, batch  1399] avg loss: 1.364247
[epoch 0, batch  1499] avg loss: 1.347272
[epoch 0, batch  1599] avg loss: 1.327843
[epoch 0, batch  1699] avg loss: 1.315582
[epoch 0, batch  1799] avg loss: 1.292686
[epoch 0, batch  1899] avg loss: 1.263292
[epoch 0, batch  1999] avg loss: 1.234660
[epoch 0, batch  2099] avg loss: 1.206692
[epoch 0, batch  2199] avg loss: 1.162049
[epoch 0, batch  2299] avg loss: 1.112756
[epoch 0, batch  2399] avg loss: 1.072709
[epoch 1, batch    99] avg loss: 1.014315
[epoch 1, batch   199] avg loss: 0.994735
[epoch 1, batch   299] avg loss: 0.965096
[epoch 1, batch   399] avg loss: 0.956566
[epoch 1, batch   499] avg loss: 0.925762
[epoch 1, batch   599] avg loss: 0.914190
[epoch 1, batch   699] avg loss: 0.913287
[epoch 1, batch   799] avg loss: 0.906891
[epoch 1, batch   899] avg loss: 0.894496
[epoch 1, batch   999] avg loss: 0.902709
[epoch 1, batch  1099] avg loss: 0.885431
[epoch 1, batch  1199] avg loss: 0.886998
[epoch 1, batch  1299] avg loss: 0.876148
[epoch 1, batch  1399] avg loss: 0.883556
[epoch 1, batch  1499] avg loss: 0.880444
[epoch 1, batch  1599] avg loss: 0.877696
[epoch 1, batch  1699] avg loss: 0.861383
[epoch 1, batch  1799] avg loss: 0.855766
[epoch 1, batch  1899] avg loss: 0.865369
[epoch 1, batch  1999] avg loss: 0.855245
[epoch 1, batch  2099] avg loss: 0.840566
[epoch 1, batch  2199] avg loss: 0.844777
[epoch 1, batch  2299] avg loss: 0.843815
[epoch 1, batch  2399] avg loss: 0.838081
[epoch 2, batch    99] avg loss: 0.835513
[epoch 2, batch   199] avg loss: 0.845885
[epoch 2, batch   299] avg loss: 0.839601
[epoch 2, batch   399] avg loss: 0.830885
[epoch 2, batch   499] avg loss: 0.826532
[epoch 2, batch   599] avg loss: 0.844846
[epoch 2, batch   699] avg loss: 0.831503
[epoch 2, batch   799] avg loss: 0.835929
[epoch 2, batch   899] avg loss: 0.824454
[epoch 2, batch   999] avg loss: 0.826431
[epoch 2, batch  1099] avg loss: 0.840155
[epoch 2, batch  1199] avg loss: 0.823689
[epoch 2, batch  1299] avg loss: 0.809489
[epoch 2, batch  1399] avg loss: 0.827944
[epoch 2, batch  1499] avg loss: 0.828379
[epoch 2, batch  1599] avg loss: 0.828543
[epoch 2, batch  1699] avg loss: 0.824608
[epoch 2, batch  1799] avg loss: 0.812183
[epoch 2, batch  1899] avg loss: 0.815682
[epoch 2, batch  1999] avg loss: 0.830812
[epoch 2, batch  2099] avg loss: 0.816692
[epoch 2, batch  2199] avg loss: 0.807220
[epoch 2, batch  2299] avg loss: 0.810139
[epoch 2, batch  2399] avg loss: 0.807090
[epoch 3, batch    99] avg loss: 0.820062
[epoch 3, batch   199] avg loss: 0.823031
[epoch 3, batch   299] avg loss: 0.811952
[epoch 3, batch   399] avg loss: 0.811761
[epoch 3, batch   499] avg loss: 0.803563
[epoch 3, batch   599] avg loss: 0.819592
[epoch 3, batch   699] avg loss: 0.800723
[epoch 3, batch   799] avg loss: 0.811420
[epoch 3, batch   899] avg loss: 0.808434
[epoch 3, batch   999] avg loss: 0.809212
[epoch 3, batch  1099] avg loss: 0.811494
[epoch 3, batch  1199] avg loss: 0.807761
[epoch 3, batch  1299] avg loss: 0.789787
[epoch 3, batch  1399] avg loss: 0.807667
[epoch 3, batch  1499] avg loss: 0.806348
[epoch 3, batch  1599] avg loss: 0.800280
[epoch 3, batch  1699] avg loss: 0.809095
[epoch 3, batch  1799] avg loss: 0.801795
[epoch 3, batch  1899] avg loss: 0.784601
[epoch 3, batch  1999] avg loss: 0.798175
[epoch 3, batch  2099] avg loss: 0.794470
[epoch 3, batch  2199] avg loss: 0.806411
[epoch 3, batch  2299] avg loss: 0.799949
[epoch 3, batch  2399] avg loss: 0.798519
[epoch 4, batch    99] avg loss: 0.786786
[epoch 4, batch   199] avg loss: 0.792819
[epoch 4, batch   299] avg loss: 0.787797
[epoch 4, batch   399] avg loss: 0.791334
[epoch 4, batch   499] avg loss: 0.809225
[epoch 4, batch   599] avg loss: 0.799328
[epoch 4, batch   699] avg loss: 0.788330
[epoch 4, batch   799] avg loss: 0.797408
[epoch 4, batch   899] avg loss: 0.790929
[epoch 4, batch   999] avg loss: 0.793106
[epoch 4, batch  1099] avg loss: 0.799682
[epoch 4, batch  1199] avg loss: 0.794190
[epoch 4, batch  1299] avg loss: 0.785119
[epoch 4, batch  1399] avg loss: 0.792177
[epoch 4, batch  1499] avg loss: 0.785021
[epoch 4, batch  1599] avg loss: 0.800147
[epoch 4, batch  1699] avg loss: 0.791868
[epoch 4, batch  1799] avg loss: 0.779912
[epoch 4, batch  1899] avg loss: 0.776966
[epoch 4, batch  1999] avg loss: 0.767701
[epoch 4, batch  2099] avg loss: 0.788164
[epoch 4, batch  2199] avg loss: 0.768457
[epoch 4, batch  2299] avg loss: 0.789249
[epoch 4, batch  2399] avg loss: 0.775228
[epoch 5, batch    99] avg loss: 0.774465
[epoch 5, batch   199] avg loss: 0.791498
[epoch 5, batch   299] avg loss: 0.780288
[epoch 5, batch   399] avg loss: 0.787313
[epoch 5, batch   499] avg loss: 0.785850
[epoch 5, batch   599] avg loss: 0.783331
[epoch 5, batch   699] avg loss: 0.788416
[epoch 5, batch   799] avg loss: 0.778779
[epoch 5, batch   899] avg loss: 0.783147
[epoch 5, batch   999] avg loss: 0.782418
[epoch 5, batch  1099] avg loss: 0.774204
[epoch 5, batch  1199] avg loss: 0.784973
[epoch 5, batch  1299] avg loss: 0.781059
[epoch 5, batch  1399] avg loss: 0.773259
[epoch 5, batch  1499] avg loss: 0.778071
[epoch 5, batch  1599] avg loss: 0.777047
[epoch 5, batch  1699] avg loss: 0.781605
[epoch 5, batch  1799] avg loss: 0.776337
[epoch 5, batch  1899] avg loss: 0.771928
[epoch 5, batch  1999] avg loss: 0.769467
[epoch 5, batch  2099] avg loss: 0.770619
[epoch 5, batch  2199] avg loss: 0.764828
[epoch 5, batch  2299] avg loss: 0.771265
[epoch 5, batch  2399] avg loss: 0.766368
[epoch 6, batch    99] avg loss: 0.779817
[epoch 6, batch   199] avg loss: 0.782265
[epoch 6, batch   299] avg loss: 0.766031
[epoch 6, batch   399] avg loss: 0.781725
[epoch 6, batch   499] avg loss: 0.771987
[epoch 6, batch   599] avg loss: 0.759517
[epoch 6, batch   699] avg loss: 0.775996
[epoch 6, batch   799] avg loss: 0.764611
[epoch 6, batch   899] avg loss: 0.758644
[epoch 6, batch   999] avg loss: 0.755217
[epoch 6, batch  1099] avg loss: 0.768598
[epoch 6, batch  1199] avg loss: 0.770667
[epoch 6, batch  1299] avg loss: 0.765305
[epoch 6, batch  1399] avg loss: 0.752289
[epoch 6, batch  1499] avg loss: 0.759758
[epoch 6, batch  1599] avg loss: 0.770040
[epoch 6, batch  1699] avg loss: 0.774282
[epoch 6, batch  1799] avg loss: 0.761995
[epoch 6, batch  1899] avg loss: 0.752445
[epoch 6, batch  1999] avg loss: 0.760882
[epoch 6, batch  2099] avg loss: 0.749556
[epoch 6, batch  2199] avg loss: 0.763264
[epoch 6, batch  2299] avg loss: 0.760463
[epoch 6, batch  2399] avg loss: 0.756115
[epoch 7, batch    99] avg loss: 0.764527
[epoch 7, batch   199] avg loss: 0.750500
[epoch 7, batch   299] avg loss: 0.756734
[epoch 7, batch   399] avg loss: 0.753428
[epoch 7, batch   499] avg loss: 0.761082
[epoch 7, batch   599] avg loss: 0.767073
[epoch 7, batch   699] avg loss: 0.761667
[epoch 7, batch   799] avg loss: 0.755465
[epoch 7, batch   899] avg loss: 0.755391
[epoch 7, batch   999] avg loss: 0.751773
[epoch 7, batch  1099] avg loss: 0.764453
[epoch 7, batch  1199] avg loss: 0.760947
[epoch 7, batch  1299] avg loss: 0.763963
[epoch 7, batch  1399] avg loss: 0.747992
[epoch 7, batch  1499] avg loss: 0.761507
[epoch 7, batch  1599] avg loss: 0.747442
[epoch 7, batch  1699] avg loss: 0.745646
[epoch 7, batch  1799] avg loss: 0.753413
[epoch 7, batch  1899] avg loss: 0.755414
[epoch 7, batch  1999] avg loss: 0.757188
[epoch 7, batch  2099] avg loss: 0.748126
[epoch 7, batch  2199] avg loss: 0.740618
[epoch 7, batch  2299] avg loss: 0.740573
[epoch 7, batch  2399] avg loss: 0.750419
[epoch 8, batch    99] avg loss: 0.744007
[epoch 8, batch   199] avg loss: 0.755893
[epoch 8, batch   299] avg loss: 0.732733
[epoch 8, batch   399] avg loss: 0.748485
[epoch 8, batch   499] avg loss: 0.745858
[epoch 8, batch   599] avg loss: 0.747256
[epoch 8, batch   699] avg loss: 0.751940
[epoch 8, batch   799] avg loss: 0.748027
[epoch 8, batch   899] avg loss: 0.742157
[epoch 8, batch   999] avg loss: 0.737950
[epoch 8, batch  1099] avg loss: 0.742443
[epoch 8, batch  1199] avg loss: 0.747499
[epoch 8, batch  1299] avg loss: 0.738105
[epoch 8, batch  1399] avg loss: 0.746466
[epoch 8, batch  1499] avg loss: 0.750100
[epoch 8, batch  1599] avg loss: 0.732114
[epoch 8, batch  1699] avg loss: 0.727805
[epoch 8, batch  1799] avg loss: 0.728832
[epoch 8, batch  1899] avg loss: 0.727476
[epoch 8, batch  1999] avg loss: 0.725384
[epoch 8, batch  2099] avg loss: 0.713442
[epoch 8, batch  2199] avg loss: 0.739855
[epoch 8, batch  2299] avg loss: 0.726872
[epoch 8, batch  2399] avg loss: 0.728725
[epoch 9, batch    99] avg loss: 0.723407
[epoch 9, batch   199] avg loss: 0.736831
[epoch 9, batch   299] avg loss: 0.732294
[epoch 9, batch   399] avg loss: 0.730407
[epoch 9, batch   499] avg loss: 0.728111
[epoch 9, batch   599] avg loss: 0.728291
[epoch 9, batch   699] avg loss: 0.740378
[epoch 9, batch   799] avg loss: 0.726328
[epoch 9, batch   899] avg loss: 0.734792
[epoch 9, batch   999] avg loss: 0.735485
[epoch 9, batch  1099] avg loss: 0.728646
[epoch 9, batch  1199] avg loss: 0.720767
[epoch 9, batch  1299] avg loss: 0.724426
[epoch 9, batch  1399] avg loss: 0.725611
[epoch 9, batch  1499] avg loss: 0.723773
[epoch 9, batch  1599] avg loss: 0.712827
[epoch 9, batch  1699] avg loss: 0.720065
[epoch 9, batch  1799] avg loss: 0.722324
[epoch 9, batch  1899] avg loss: 0.721954
[epoch 9, batch  1999] avg loss: 0.721472
[epoch 9, batch  2099] avg loss: 0.704737
[epoch 9, batch  2199] avg loss: 0.710334
[epoch 9, batch  2299] avg loss: 0.726097
[epoch 9, batch  2399] avg loss: 0.715602
[epoch 10, batch    99] avg loss: 0.716732
[epoch 10, batch   199] avg loss: 0.717559
[epoch 10, batch   299] avg loss: 0.714286
[epoch 10, batch   399] avg loss: 0.706099
[epoch 10, batch   499] avg loss: 0.703248
[epoch 10, batch   599] avg loss: 0.724596
[epoch 10, batch   699] avg loss: 0.711471
[epoch 10, batch   799] avg loss: 0.715225
[epoch 10, batch   899] avg loss: 0.718655
[epoch 10, batch   999] avg loss: 0.707963
[epoch 10, batch  1099] avg loss: 0.715941
[epoch 10, batch  1199] avg loss: 0.710226
[epoch 10, batch  1299] avg loss: 0.688592
[epoch 10, batch  1399] avg loss: 0.701844
[epoch 10, batch  1499] avg loss: 0.702236
[epoch 10, batch  1599] avg loss: 0.704582
[epoch 10, batch  1699] avg loss: 0.725495
[epoch 10, batch  1799] avg loss: 0.711164
[epoch 10, batch  1899] avg loss: 0.701332
[epoch 10, batch  1999] avg loss: 0.693459
[epoch 10, batch  2099] avg loss: 0.703109
[epoch 10, batch  2199] avg loss: 0.695665
[epoch 10, batch  2299] avg loss: 0.707373
[epoch 10, batch  2399] avg loss: 0.704460
[epoch 11, batch    99] avg loss: 0.698553
[epoch 11, batch   199] avg loss: 0.704056
[epoch 11, batch   299] avg loss: 0.702118
[epoch 11, batch   399] avg loss: 0.692433
[epoch 11, batch   499] avg loss: 0.699639
[epoch 11, batch   599] avg loss: 0.692096
[epoch 11, batch   699] avg loss: 0.689715
[epoch 11, batch   799] avg loss: 0.692230
[epoch 11, batch   899] avg loss: 0.706023
[epoch 11, batch   999] avg loss: 0.692841
[epoch 11, batch  1099] avg loss: 0.703635
[epoch 11, batch  1199] avg loss: 0.692129
[epoch 11, batch  1299] avg loss: 0.690232
[epoch 11, batch  1399] avg loss: 0.692448
[epoch 11, batch  1499] avg loss: 0.689892
[epoch 11, batch  1599] avg loss: 0.693463
[epoch 11, batch  1699] avg loss: 0.691035
[epoch 11, batch  1799] avg loss: 0.705855
[epoch 11, batch  1899] avg loss: 0.681521
[epoch 11, batch  1999] avg loss: 0.700206
[epoch 11, batch  2099] avg loss: 0.677497
[epoch 11, batch  2199] avg loss: 0.696940
[epoch 11, batch  2299] avg loss: 0.686226
[epoch 11, batch  2399] avg loss: 0.691327
[epoch 12, batch    99] avg loss: 0.687721
[epoch 12, batch   199] avg loss: 0.695187
[epoch 12, batch   299] avg loss: 0.686066
[epoch 12, batch   399] avg loss: 0.675637
[epoch 12, batch   499] avg loss: 0.683853
[epoch 12, batch   599] avg loss: 0.695977
[epoch 12, batch   699] avg loss: 0.676788
[epoch 12, batch   799] avg loss: 0.689971
[epoch 12, batch   899] avg loss: 0.679398
[epoch 12, batch   999] avg loss: 0.671269
[epoch 12, batch  1099] avg loss: 0.689389
[epoch 12, batch  1199] avg loss: 0.682896
[epoch 12, batch  1299] avg loss: 0.690095
[epoch 12, batch  1399] avg loss: 0.676952
[epoch 12, batch  1499] avg loss: 0.676558
[epoch 12, batch  1599] avg loss: 0.679802
[epoch 12, batch  1699] avg loss: 0.679063
[epoch 12, batch  1799] avg loss: 0.692565
[epoch 12, batch  1899] avg loss: 0.666139
[epoch 12, batch  1999] avg loss: 0.680273
[epoch 12, batch  2099] avg loss: 0.686634
[epoch 12, batch  2199] avg loss: 0.668582
[epoch 12, batch  2299] avg loss: 0.674872
[epoch 12, batch  2399] avg loss: 0.669331
[epoch 13, batch    99] avg loss: 0.668783
[epoch 13, batch   199] avg loss: 0.663550
[epoch 13, batch   299] avg loss: 0.682885
[epoch 13, batch   399] avg loss: 0.675695
[epoch 13, batch   499] avg loss: 0.683860
[epoch 13, batch   599] avg loss: 0.672115
[epoch 13, batch   699] avg loss: 0.685285
[epoch 13, batch   799] avg loss: 0.664016
[epoch 13, batch   899] avg loss: 0.672006
[epoch 13, batch   999] avg loss: 0.675064
[epoch 13, batch  1099] avg loss: 0.664894
[epoch 13, batch  1199] avg loss: 0.672999
[epoch 13, batch  1299] avg loss: 0.684205
[epoch 13, batch  1399] avg loss: 0.658250
[epoch 13, batch  1499] avg loss: 0.667214
[epoch 13, batch  1599] avg loss: 0.656103
[epoch 13, batch  1699] avg loss: 0.667951
[epoch 13, batch  1799] avg loss: 0.669308
[epoch 13, batch  1899] avg loss: 0.658180
[epoch 13, batch  1999] avg loss: 0.662343
[epoch 13, batch  2099] avg loss: 0.670341
[epoch 13, batch  2199] avg loss: 0.676262
[epoch 13, batch  2299] avg loss: 0.662383
[epoch 13, batch  2399] avg loss: 0.682383
[epoch 14, batch    99] avg loss: 0.655291
[epoch 14, batch   199] avg loss: 0.670745
[epoch 14, batch   299] avg loss: 0.657483
[epoch 14, batch   399] avg loss: 0.641617
[epoch 14, batch   499] avg loss: 0.673772
[epoch 14, batch   599] avg loss: 0.662510
[epoch 14, batch   699] avg loss: 0.657553
[epoch 14, batch   799] avg loss: 0.678702
[epoch 14, batch   899] avg loss: 0.657723
[epoch 14, batch   999] avg loss: 0.664578
[epoch 14, batch  1099] avg loss: 0.669975
[epoch 14, batch  1199] avg loss: 0.654595
[epoch 14, batch  1299] avg loss: 0.642893
[epoch 14, batch  1399] avg loss: 0.667575
[epoch 14, batch  1499] avg loss: 0.653595
[epoch 14, batch  1599] avg loss: 0.669412
[epoch 14, batch  1699] avg loss: 0.657836
[epoch 14, batch  1799] avg loss: 0.665376
[epoch 14, batch  1899] avg loss: 0.653984
[epoch 14, batch  1999] avg loss: 0.653384
[epoch 14, batch  2099] avg loss: 0.653491
[epoch 14, batch  2199] avg loss: 0.660972
[epoch 14, batch  2299] avg loss: 0.652804
[epoch 14, batch  2399] avg loss: 0.660119
[epoch 15, batch    99] avg loss: 0.664290
[epoch 15, batch   199] avg loss: 0.659985
[epoch 15, batch   299] avg loss: 0.649803
[epoch 15, batch   399] avg loss: 0.662249
[epoch 15, batch   499] avg loss: 0.657062
[epoch 15, batch   599] avg loss: 0.655648
[epoch 15, batch   699] avg loss: 0.655502
[epoch 15, batch   799] avg loss: 0.659468
[epoch 15, batch   899] avg loss: 0.652982
[epoch 15, batch   999] avg loss: 0.642260
[epoch 15, batch  1099] avg loss: 0.642870
[epoch 15, batch  1199] avg loss: 0.647987
[epoch 15, batch  1299] avg loss: 0.637946
[epoch 15, batch  1399] avg loss: 0.652119
[epoch 15, batch  1499] avg loss: 0.667319
[epoch 15, batch  1599] avg loss: 0.648505
[epoch 15, batch  1699] avg loss: 0.648218
[epoch 15, batch  1799] avg loss: 0.649052
[epoch 15, batch  1899] avg loss: 0.643579
[epoch 15, batch  1999] avg loss: 0.642318
[epoch 15, batch  2099] avg loss: 0.650698
[epoch 15, batch  2199] avg loss: 0.646461
[epoch 15, batch  2299] avg loss: 0.644655
[epoch 15, batch  2399] avg loss: 0.642921
[epoch 16, batch    99] avg loss: 0.635212
[epoch 16, batch   199] avg loss: 0.642648
[epoch 16, batch   299] avg loss: 0.648033
[epoch 16, batch   399] avg loss: 0.643415
[epoch 16, batch   499] avg loss: 0.649450
[epoch 16, batch   599] avg loss: 0.638357
[epoch 16, batch   699] avg loss: 0.652586
[epoch 16, batch   799] avg loss: 0.641468
[epoch 16, batch   899] avg loss: 0.634622
[epoch 16, batch   999] avg loss: 0.643566
[epoch 16, batch  1099] avg loss: 0.642100
[epoch 16, batch  1199] avg loss: 0.642180
[epoch 16, batch  1299] avg loss: 0.653157
[epoch 16, batch  1399] avg loss: 0.636499
[epoch 16, batch  1499] avg loss: 0.632427
[epoch 16, batch  1599] avg loss: 0.641650
[epoch 16, batch  1699] avg loss: 0.647749
[epoch 16, batch  1799] avg loss: 0.639139
[epoch 16, batch  1899] avg loss: 0.641464
[epoch 16, batch  1999] avg loss: 0.644970
[epoch 16, batch  2099] avg loss: 0.627089
[epoch 16, batch  2199] avg loss: 0.637358
[epoch 16, batch  2299] avg loss: 0.648245
[epoch 16, batch  2399] avg loss: 0.644296
[epoch 17, batch    99] avg loss: 0.632140
[epoch 17, batch   199] avg loss: 0.635612
[epoch 17, batch   299] avg loss: 0.642660
[epoch 17, batch   399] avg loss: 0.642140
[epoch 17, batch   499] avg loss: 0.645203
[epoch 17, batch   599] avg loss: 0.627316
[epoch 17, batch   699] avg loss: 0.625046
[epoch 17, batch   799] avg loss: 0.620862
[epoch 17, batch   899] avg loss: 0.629569
[epoch 17, batch   999] avg loss: 0.627310
[epoch 17, batch  1099] avg loss: 0.634442
[epoch 17, batch  1199] avg loss: 0.631983
[epoch 17, batch  1299] avg loss: 0.639654
[epoch 17, batch  1399] avg loss: 0.645762
[epoch 17, batch  1499] avg loss: 0.625335
[epoch 17, batch  1599] avg loss: 0.635292
[epoch 17, batch  1699] avg loss: 0.643758
[epoch 17, batch  1799] avg loss: 0.640931
[epoch 17, batch  1899] avg loss: 0.631770
[epoch 17, batch  1999] avg loss: 0.634862
[epoch 17, batch  2099] avg loss: 0.628304
[epoch 17, batch  2199] avg loss: 0.635752
[epoch 17, batch  2299] avg loss: 0.640641
[epoch 17, batch  2399] avg loss: 0.632841
[epoch 18, batch    99] avg loss: 0.620316
[epoch 18, batch   199] avg loss: 0.621459
[epoch 18, batch   299] avg loss: 0.622662
[epoch 18, batch   399] avg loss: 0.640456
[epoch 18, batch   499] avg loss: 0.633435
[epoch 18, batch   599] avg loss: 0.623884
[epoch 18, batch   699] avg loss: 0.621474
[epoch 18, batch   799] avg loss: 0.634083
[epoch 18, batch   899] avg loss: 0.653535
[epoch 18, batch   999] avg loss: 0.625164
[epoch 18, batch  1099] avg loss: 0.628663
[epoch 18, batch  1199] avg loss: 0.631764
[epoch 18, batch  1299] avg loss: 0.632204
[epoch 18, batch  1399] avg loss: 0.630571
[epoch 18, batch  1499] avg loss: 0.624479
[epoch 18, batch  1599] avg loss: 0.625860
[epoch 18, batch  1699] avg loss: 0.624868
[epoch 18, batch  1799] avg loss: 0.617859
[epoch 18, batch  1899] avg loss: 0.627178
[epoch 18, batch  1999] avg loss: 0.623225
[epoch 18, batch  2099] avg loss: 0.630073
[epoch 18, batch  2199] avg loss: 0.634433
[epoch 18, batch  2299] avg loss: 0.627746
[epoch 18, batch  2399] avg loss: 0.618854
[epoch 19, batch    99] avg loss: 0.620942
[epoch 19, batch   199] avg loss: 0.630946
[epoch 19, batch   299] avg loss: 0.619218
[epoch 19, batch   399] avg loss: 0.621626
[epoch 19, batch   499] avg loss: 0.628330
[epoch 19, batch   599] avg loss: 0.630051
[epoch 19, batch   699] avg loss: 0.615899
[epoch 19, batch   799] avg loss: 0.627217
[epoch 19, batch   899] avg loss: 0.614018
[epoch 19, batch   999] avg loss: 0.622301
[epoch 19, batch  1099] avg loss: 0.625753
[epoch 19, batch  1199] avg loss: 0.618872
[epoch 19, batch  1299] avg loss: 0.622630
[epoch 19, batch  1399] avg loss: 0.635148
[epoch 19, batch  1499] avg loss: 0.620066
[epoch 19, batch  1599] avg loss: 0.611734
[epoch 19, batch  1699] avg loss: 0.621770
[epoch 19, batch  1799] avg loss: 0.608326
[epoch 19, batch  1899] avg loss: 0.630763
[epoch 19, batch  1999] avg loss: 0.618148
[epoch 19, batch  2099] avg loss: 0.620782
[epoch 19, batch  2199] avg loss: 0.628092
[epoch 19, batch  2299] avg loss: 0.617999
[epoch 19, batch  2399] avg loss: 0.615021
[epoch 20, batch    99] avg loss: 0.614775
[epoch 20, batch   199] avg loss: 0.624629
[epoch 20, batch   299] avg loss: 0.619304
[epoch 20, batch   399] avg loss: 0.611359
[epoch 20, batch   499] avg loss: 0.623418
[epoch 20, batch   599] avg loss: 0.621244
[epoch 20, batch   699] avg loss: 0.619635
[epoch 20, batch   799] avg loss: 0.600070
[epoch 20, batch   899] avg loss: 0.612368
[epoch 20, batch   999] avg loss: 0.618250
[epoch 20, batch  1099] avg loss: 0.622720
[epoch 20, batch  1199] avg loss: 0.609561
[epoch 20, batch  1299] avg loss: 0.619938
[epoch 20, batch  1399] avg loss: 0.612153
[epoch 20, batch  1499] avg loss: 0.613864
[epoch 20, batch  1599] avg loss: 0.622921
[epoch 20, batch  1699] avg loss: 0.615354
[epoch 20, batch  1799] avg loss: 0.613085
[epoch 20, batch  1899] avg loss: 0.611852
[epoch 20, batch  1999] avg loss: 0.622257
[epoch 20, batch  2099] avg loss: 0.606923
[epoch 20, batch  2199] avg loss: 0.612212
[epoch 20, batch  2299] avg loss: 0.626871
[epoch 20, batch  2399] avg loss: 0.606760
[epoch 21, batch    99] avg loss: 0.612953
[epoch 21, batch   199] avg loss: 0.618359
[epoch 21, batch   299] avg loss: 0.614857
[epoch 21, batch   399] avg loss: 0.611699
[epoch 21, batch   499] avg loss: 0.616524
[epoch 21, batch   599] avg loss: 0.611718
[epoch 21, batch   699] avg loss: 0.612792
[epoch 21, batch   799] avg loss: 0.611474
[epoch 21, batch   899] avg loss: 0.610812
[epoch 21, batch   999] avg loss: 0.612538
[epoch 21, batch  1099] avg loss: 0.608914
[epoch 21, batch  1199] avg loss: 0.611789
[epoch 21, batch  1299] avg loss: 0.630038
[epoch 21, batch  1399] avg loss: 0.608577
[epoch 21, batch  1499] avg loss: 0.597493
[epoch 21, batch  1599] avg loss: 0.614352
[epoch 21, batch  1699] avg loss: 0.622355
[epoch 21, batch  1799] avg loss: 0.604773
[epoch 21, batch  1899] avg loss: 0.595295
[epoch 21, batch  1999] avg loss: 0.604690
[epoch 21, batch  2099] avg loss: 0.622741
[epoch 21, batch  2199] avg loss: 0.595822
[epoch 21, batch  2299] avg loss: 0.604131
[epoch 21, batch  2399] avg loss: 0.593596
[epoch 22, batch    99] avg loss: 0.601832
[epoch 22, batch   199] avg loss: 0.611750
[epoch 22, batch   299] avg loss: 0.609553
[epoch 22, batch   399] avg loss: 0.601505
[epoch 22, batch   499] avg loss: 0.607906
[epoch 22, batch   599] avg loss: 0.612294
[epoch 22, batch   699] avg loss: 0.600568
[epoch 22, batch   799] avg loss: 0.604966
[epoch 22, batch   899] avg loss: 0.615007
[epoch 22, batch   999] avg loss: 0.611443
[epoch 22, batch  1099] avg loss: 0.611445
[epoch 22, batch  1199] avg loss: 0.611278
[epoch 22, batch  1299] avg loss: 0.598738
[epoch 22, batch  1399] avg loss: 0.610119
[epoch 22, batch  1499] avg loss: 0.599452
[epoch 22, batch  1599] avg loss: 0.606530
[epoch 22, batch  1699] avg loss: 0.604780
[epoch 22, batch  1799] avg loss: 0.603299
[epoch 22, batch  1899] avg loss: 0.609164
[epoch 22, batch  1999] avg loss: 0.622426
[epoch 22, batch  2099] avg loss: 0.599425
[epoch 22, batch  2199] avg loss: 0.616753
[epoch 22, batch  2299] avg loss: 0.589727
[epoch 22, batch  2399] avg loss: 0.595965
[epoch 23, batch    99] avg loss: 0.603499
[epoch 23, batch   199] avg loss: 0.613511
[epoch 23, batch   299] avg loss: 0.613857
[epoch 23, batch   399] avg loss: 0.593219
[epoch 23, batch   499] avg loss: 0.595807
[epoch 23, batch   599] avg loss: 0.592682
[epoch 23, batch   699] avg loss: 0.607585
[epoch 23, batch   799] avg loss: 0.599895
[epoch 23, batch   899] avg loss: 0.602984
[epoch 23, batch   999] avg loss: 0.616339
[epoch 23, batch  1099] avg loss: 0.607669
[epoch 23, batch  1199] avg loss: 0.611344
[epoch 23, batch  1299] avg loss: 0.603771
[epoch 23, batch  1399] avg loss: 0.601422
[epoch 23, batch  1499] avg loss: 0.592130
[epoch 23, batch  1599] avg loss: 0.597431
[epoch 23, batch  1699] avg loss: 0.606229
[epoch 23, batch  1799] avg loss: 0.591666
[epoch 23, batch  1899] avg loss: 0.592087
[epoch 23, batch  1999] avg loss: 0.605724
[epoch 23, batch  2099] avg loss: 0.593636
[epoch 23, batch  2199] avg loss: 0.601361
[epoch 23, batch  2299] avg loss: 0.610351
[epoch 23, batch  2399] avg loss: 0.609049
[epoch 24, batch    99] avg loss: 0.598824
[epoch 24, batch   199] avg loss: 0.595082
[epoch 24, batch   299] avg loss: 0.592442
[epoch 24, batch   399] avg loss: 0.604416
[epoch 24, batch   499] avg loss: 0.605689
[epoch 24, batch   599] avg loss: 0.596450
[epoch 24, batch   699] avg loss: 0.602413
[epoch 24, batch   799] avg loss: 0.599809
[epoch 24, batch   899] avg loss: 0.596842
[epoch 24, batch   999] avg loss: 0.603039
[epoch 24, batch  1099] avg loss: 0.620998
[epoch 24, batch  1199] avg loss: 0.608670
[epoch 24, batch  1299] avg loss: 0.609142
[epoch 24, batch  1399] avg loss: 0.586689
[epoch 24, batch  1499] avg loss: 0.581956
[epoch 24, batch  1599] avg loss: 0.598489
[epoch 24, batch  1699] avg loss: 0.586454
[epoch 24, batch  1799] avg loss: 0.597597
[epoch 24, batch  1899] avg loss: 0.601217
[epoch 24, batch  1999] avg loss: 0.605325
[epoch 24, batch  2099] avg loss: 0.580214
[epoch 24, batch  2199] avg loss: 0.600279
[epoch 24, batch  2299] avg loss: 0.589261
[epoch 24, batch  2399] avg loss: 0.593181
[epoch 25, batch    99] avg loss: 0.576259
[epoch 25, batch   199] avg loss: 0.593544
[epoch 25, batch   299] avg loss: 0.607957
[epoch 25, batch   399] avg loss: 0.587993
[epoch 25, batch   499] avg loss: 0.608727
[epoch 25, batch   599] avg loss: 0.598237
[epoch 25, batch   699] avg loss: 0.605908
[epoch 25, batch   799] avg loss: 0.580320
[epoch 25, batch   899] avg loss: 0.578481
[epoch 25, batch   999] avg loss: 0.598532
[epoch 25, batch  1099] avg loss: 0.600328
[epoch 25, batch  1199] avg loss: 0.576279
[epoch 25, batch  1299] avg loss: 0.595619
[epoch 25, batch  1399] avg loss: 0.599305
[epoch 25, batch  1499] avg loss: 0.600568
[epoch 25, batch  1599] avg loss: 0.592532
[epoch 25, batch  1699] avg loss: 0.613262
[epoch 25, batch  1799] avg loss: 0.592697
[epoch 25, batch  1899] avg loss: 0.580539
[epoch 25, batch  1999] avg loss: 0.596237
[epoch 25, batch  2099] avg loss: 0.593305
[epoch 25, batch  2199] avg loss: 0.583993
[epoch 25, batch  2299] avg loss: 0.591936
[epoch 25, batch  2399] avg loss: 0.601310
[epoch 26, batch    99] avg loss: 0.598018
[epoch 26, batch   199] avg loss: 0.600495
[epoch 26, batch   299] avg loss: 0.580422
[epoch 26, batch   399] avg loss: 0.600054
[epoch 26, batch   499] avg loss: 0.574493
[epoch 26, batch   599] avg loss: 0.587210
[epoch 26, batch   699] avg loss: 0.589771
[epoch 26, batch   799] avg loss: 0.598775
[epoch 26, batch   899] avg loss: 0.584882
[epoch 26, batch   999] avg loss: 0.595334
[epoch 26, batch  1099] avg loss: 0.596160
[epoch 26, batch  1199] avg loss: 0.580930
[epoch 26, batch  1299] avg loss: 0.598305
[epoch 26, batch  1399] avg loss: 0.592193
[epoch 26, batch  1499] avg loss: 0.591562
[epoch 26, batch  1599] avg loss: 0.595090
[epoch 26, batch  1699] avg loss: 0.582848
[epoch 26, batch  1799] avg loss: 0.600655
[epoch 26, batch  1899] avg loss: 0.599015
[epoch 26, batch  1999] avg loss: 0.588214
[epoch 26, batch  2099] avg loss: 0.582176
[epoch 26, batch  2199] avg loss: 0.611284
[epoch 26, batch  2299] avg loss: 0.587895
[epoch 26, batch  2399] avg loss: 0.589811
[epoch 27, batch    99] avg loss: 0.583265
[epoch 27, batch   199] avg loss: 0.586457
[epoch 27, batch   299] avg loss: 0.600301
[epoch 27, batch   399] avg loss: 0.592492
[epoch 27, batch   499] avg loss: 0.589065
[epoch 27, batch   599] avg loss: 0.605326
[epoch 27, batch   699] avg loss: 0.587083
[epoch 27, batch   799] avg loss: 0.585095
[epoch 27, batch   899] avg loss: 0.585646
[epoch 27, batch   999] avg loss: 0.580247
[epoch 27, batch  1099] avg loss: 0.600934
[epoch 27, batch  1199] avg loss: 0.581899
[epoch 27, batch  1299] avg loss: 0.578114
[epoch 27, batch  1399] avg loss: 0.583748
[epoch 27, batch  1499] avg loss: 0.609229
[epoch 27, batch  1599] avg loss: 0.580179
[epoch 27, batch  1699] avg loss: 0.592592
[epoch 27, batch  1799] avg loss: 0.590200
[epoch 27, batch  1899] avg loss: 0.587406
[epoch 27, batch  1999] avg loss: 0.576626
[epoch 27, batch  2099] avg loss: 0.599198
[epoch 27, batch  2199] avg loss: 0.588286
[epoch 27, batch  2299] avg loss: 0.578035
[epoch 27, batch  2399] avg loss: 0.575689
[epoch 28, batch    99] avg loss: 0.595644
[epoch 28, batch   199] avg loss: 0.602608
[epoch 28, batch   299] avg loss: 0.579906
[epoch 28, batch   399] avg loss: 0.587754
[epoch 28, batch   499] avg loss: 0.583738
[epoch 28, batch   599] avg loss: 0.587005
[epoch 28, batch   699] avg loss: 0.588282
[epoch 28, batch   799] avg loss: 0.594177
[epoch 28, batch   899] avg loss: 0.585793
[epoch 28, batch   999] avg loss: 0.574853
[epoch 28, batch  1099] avg loss: 0.574476
[epoch 28, batch  1199] avg loss: 0.586536
[epoch 28, batch  1299] avg loss: 0.594241
[epoch 28, batch  1399] avg loss: 0.579823
[epoch 28, batch  1499] avg loss: 0.586551
[epoch 28, batch  1599] avg loss: 0.592562
[epoch 28, batch  1699] avg loss: 0.587160
[epoch 28, batch  1799] avg loss: 0.578833
[epoch 28, batch  1899] avg loss: 0.573507
[epoch 28, batch  1999] avg loss: 0.592294
[epoch 28, batch  2099] avg loss: 0.590104
[epoch 28, batch  2199] avg loss: 0.580369
[epoch 28, batch  2299] avg loss: 0.570601
[epoch 28, batch  2399] avg loss: 0.597417
[epoch 29, batch    99] avg loss: 0.572597
[epoch 29, batch   199] avg loss: 0.575765
[epoch 29, batch   299] avg loss: 0.588636
[epoch 29, batch   399] avg loss: 0.579854
[epoch 29, batch   499] avg loss: 0.578964
[epoch 29, batch   599] avg loss: 0.575360
[epoch 29, batch   699] avg loss: 0.582416
[epoch 29, batch   799] avg loss: 0.587764
[epoch 29, batch   899] avg loss: 0.588717
[epoch 29, batch   999] avg loss: 0.573195
[epoch 29, batch  1099] avg loss: 0.589680
[epoch 29, batch  1199] avg loss: 0.586080
[epoch 29, batch  1299] avg loss: 0.575252
[epoch 29, batch  1399] avg loss: 0.586096
[epoch 29, batch  1499] avg loss: 0.593937
[epoch 29, batch  1599] avg loss: 0.583396
[epoch 29, batch  1699] avg loss: 0.597898
[epoch 29, batch  1799] avg loss: 0.588870
[epoch 29, batch  1899] avg loss: 0.577360
[epoch 29, batch  1999] avg loss: 0.584288
[epoch 29, batch  2099] avg loss: 0.581222
[epoch 29, batch  2199] avg loss: 0.592656
[epoch 29, batch  2299] avg loss: 0.598538
[epoch 29, batch  2399] avg loss: 0.562699
[epoch 30, batch    99] avg loss: 0.584326
[epoch 30, batch   199] avg loss: 0.573486
[epoch 30, batch   299] avg loss: 0.576229
[epoch 30, batch   399] avg loss: 0.566751
[epoch 30, batch   499] avg loss: 0.578752
[epoch 30, batch   599] avg loss: 0.582126
[epoch 30, batch   699] avg loss: 0.574060
[epoch 30, batch   799] avg loss: 0.592108
[epoch 30, batch   899] avg loss: 0.573237
[epoch 30, batch   999] avg loss: 0.572999
[epoch 30, batch  1099] avg loss: 0.575216
[epoch 30, batch  1199] avg loss: 0.586344
[epoch 30, batch  1299] avg loss: 0.584748
[epoch 30, batch  1399] avg loss: 0.590831
[epoch 30, batch  1499] avg loss: 0.580398
[epoch 30, batch  1599] avg loss: 0.589659
[epoch 30, batch  1699] avg loss: 0.580686
[epoch 30, batch  1799] avg loss: 0.578695
[epoch 30, batch  1899] avg loss: 0.590958
[epoch 30, batch  1999] avg loss: 0.581260
[epoch 30, batch  2099] avg loss: 0.574171
[epoch 30, batch  2199] avg loss: 0.570500
[epoch 30, batch  2299] avg loss: 0.580863
[epoch 30, batch  2399] avg loss: 0.574756
[epoch 31, batch    99] avg loss: 0.570153
[epoch 31, batch   199] avg loss: 0.580814
[epoch 31, batch   299] avg loss: 0.572023
[epoch 31, batch   399] avg loss: 0.572590
[epoch 31, batch   499] avg loss: 0.585390
[epoch 31, batch   599] avg loss: 0.586422
[epoch 31, batch   699] avg loss: 0.583964
[epoch 31, batch   799] avg loss: 0.577316
[epoch 31, batch   899] avg loss: 0.562702
[epoch 31, batch   999] avg loss: 0.576822
[epoch 31, batch  1099] avg loss: 0.575231
[epoch 31, batch  1199] avg loss: 0.578035
[epoch 31, batch  1299] avg loss: 0.568214
[epoch 31, batch  1399] avg loss: 0.572575
[epoch 31, batch  1499] avg loss: 0.584174
[epoch 31, batch  1599] avg loss: 0.586392
[epoch 31, batch  1699] avg loss: 0.579330
[epoch 31, batch  1799] avg loss: 0.580089
[epoch 31, batch  1899] avg loss: 0.581647
[epoch 31, batch  1999] avg loss: 0.578591
[epoch 31, batch  2099] avg loss: 0.583248
[epoch 31, batch  2199] avg loss: 0.576527
[epoch 31, batch  2299] avg loss: 0.574884
[epoch 31, batch  2399] avg loss: 0.580195
[epoch 32, batch    99] avg loss: 0.583937
[epoch 32, batch   199] avg loss: 0.576213
[epoch 32, batch   299] avg loss: 0.580261
[epoch 32, batch   399] avg loss: 0.581400
[epoch 32, batch   499] avg loss: 0.572131
[epoch 32, batch   599] avg loss: 0.586354
[epoch 32, batch   699] avg loss: 0.576371
[epoch 32, batch   799] avg loss: 0.574505
[epoch 32, batch   899] avg loss: 0.583784
[epoch 32, batch   999] avg loss: 0.567793
[epoch 32, batch  1099] avg loss: 0.564281
[epoch 32, batch  1199] avg loss: 0.564063
[epoch 32, batch  1299] avg loss: 0.570857
[epoch 32, batch  1399] avg loss: 0.569242
[epoch 32, batch  1499] avg loss: 0.571616
[epoch 32, batch  1599] avg loss: 0.568065
[epoch 32, batch  1699] avg loss: 0.573518
[epoch 32, batch  1799] avg loss: 0.587375
[epoch 32, batch  1899] avg loss: 0.576945
[epoch 32, batch  1999] avg loss: 0.573950
[epoch 32, batch  2099] avg loss: 0.576725
[epoch 32, batch  2199] avg loss: 0.570533
[epoch 32, batch  2299] avg loss: 0.572470
[epoch 32, batch  2399] avg loss: 0.576367
[epoch 33, batch    99] avg loss: 0.579682
[epoch 33, batch   199] avg loss: 0.574216
[epoch 33, batch   299] avg loss: 0.560650
[epoch 33, batch   399] avg loss: 0.582424
[epoch 33, batch   499] avg loss: 0.580206
[epoch 33, batch   599] avg loss: 0.575900
[epoch 33, batch   699] avg loss: 0.558817
[epoch 33, batch   799] avg loss: 0.569630
[epoch 33, batch   899] avg loss: 0.570282
[epoch 33, batch   999] avg loss: 0.568802
[epoch 33, batch  1099] avg loss: 0.573563
[epoch 33, batch  1199] avg loss: 0.575056
[epoch 33, batch  1299] avg loss: 0.581690
[epoch 33, batch  1399] avg loss: 0.575041
[epoch 33, batch  1499] avg loss: 0.559207
[epoch 33, batch  1599] avg loss: 0.568170
[epoch 33, batch  1699] avg loss: 0.577782
[epoch 33, batch  1799] avg loss: 0.572101
[epoch 33, batch  1899] avg loss: 0.573348
[epoch 33, batch  1999] avg loss: 0.564491
[epoch 33, batch  2099] avg loss: 0.566959
[epoch 33, batch  2199] avg loss: 0.568620
[epoch 33, batch  2299] avg loss: 0.570756
[epoch 33, batch  2399] avg loss: 0.580154
[epoch 34, batch    99] avg loss: 0.572567
[epoch 34, batch   199] avg loss: 0.573027
[epoch 34, batch   299] avg loss: 0.579892
[epoch 34, batch   399] avg loss: 0.559377
[epoch 34, batch   499] avg loss: 0.571768
[epoch 34, batch   599] avg loss: 0.585966
[epoch 34, batch   699] avg loss: 0.577504
[epoch 34, batch   799] avg loss: 0.568200
[epoch 34, batch   899] avg loss: 0.583535
[epoch 34, batch   999] avg loss: 0.566047
[epoch 34, batch  1099] avg loss: 0.563903
[epoch 34, batch  1199] avg loss: 0.566213
[epoch 34, batch  1299] avg loss: 0.561785
[epoch 34, batch  1399] avg loss: 0.557947
[epoch 34, batch  1499] avg loss: 0.562650
[epoch 34, batch  1599] avg loss: 0.569872
[epoch 34, batch  1699] avg loss: 0.574502
[epoch 34, batch  1799] avg loss: 0.568554
[epoch 34, batch  1899] avg loss: 0.574935
[epoch 34, batch  1999] avg loss: 0.570139
[epoch 34, batch  2099] avg loss: 0.573019
[epoch 34, batch  2199] avg loss: 0.578181
[epoch 34, batch  2299] avg loss: 0.569328
[epoch 34, batch  2399] avg loss: 0.562057
[epoch 35, batch    99] avg loss: 0.579375
[epoch 35, batch   199] avg loss: 0.565598
[epoch 35, batch   299] avg loss: 0.566778
[epoch 35, batch   399] avg loss: 0.568421
[epoch 35, batch   499] avg loss: 0.576700
[epoch 35, batch   599] avg loss: 0.560290
[epoch 35, batch   699] avg loss: 0.580444
[epoch 35, batch   799] avg loss: 0.552873
[epoch 35, batch   899] avg loss: 0.562087
[epoch 35, batch   999] avg loss: 0.576631
[epoch 35, batch  1099] avg loss: 0.561470
[epoch 35, batch  1199] avg loss: 0.553032
[epoch 35, batch  1299] avg loss: 0.571912
[epoch 35, batch  1399] avg loss: 0.569443
[epoch 35, batch  1499] avg loss: 0.566899
[epoch 35, batch  1599] avg loss: 0.558725
[epoch 35, batch  1699] avg loss: 0.576820
[epoch 35, batch  1799] avg loss: 0.583425
[epoch 35, batch  1899] avg loss: 0.564193
[epoch 35, batch  1999] avg loss: 0.563170
[epoch 35, batch  2099] avg loss: 0.570599
[epoch 35, batch  2199] avg loss: 0.562953
[epoch 35, batch  2299] avg loss: 0.567571
[epoch 35, batch  2399] avg loss: 0.570641
[epoch 36, batch    99] avg loss: 0.550558
[epoch 36, batch   199] avg loss: 0.567640
[epoch 36, batch   299] avg loss: 0.564124
[epoch 36, batch   399] avg loss: 0.577006
[epoch 36, batch   499] avg loss: 0.573039
[epoch 36, batch   599] avg loss: 0.565014
[epoch 36, batch   699] avg loss: 0.569964
[epoch 36, batch   799] avg loss: 0.570322
[epoch 36, batch   899] avg loss: 0.565106
[epoch 36, batch   999] avg loss: 0.554946
[epoch 36, batch  1099] avg loss: 0.569963
[epoch 36, batch  1199] avg loss: 0.575606
[epoch 36, batch  1299] avg loss: 0.566905
[epoch 36, batch  1399] avg loss: 0.568770
[epoch 36, batch  1499] avg loss: 0.560660
[epoch 36, batch  1599] avg loss: 0.572048
[epoch 36, batch  1699] avg loss: 0.565005
[epoch 36, batch  1799] avg loss: 0.564428
[epoch 36, batch  1899] avg loss: 0.576777
[epoch 36, batch  1999] avg loss: 0.571015
[epoch 36, batch  2099] avg loss: 0.567266
[epoch 36, batch  2199] avg loss: 0.565290
[epoch 36, batch  2299] avg loss: 0.567847
[epoch 36, batch  2399] avg loss: 0.566998
[epoch 37, batch    99] avg loss: 0.563274
[epoch 37, batch   199] avg loss: 0.562687
[epoch 37, batch   299] avg loss: 0.562641
[epoch 37, batch   399] avg loss: 0.567261
[epoch 37, batch   499] avg loss: 0.567600
[epoch 37, batch   599] avg loss: 0.573680
[epoch 37, batch   699] avg loss: 0.575771
[epoch 37, batch   799] avg loss: 0.565117
[epoch 37, batch   899] avg loss: 0.564395
[epoch 37, batch   999] avg loss: 0.560669
[epoch 37, batch  1099] avg loss: 0.567298
[epoch 37, batch  1199] avg loss: 0.561723
[epoch 37, batch  1299] avg loss: 0.550847
[epoch 37, batch  1399] avg loss: 0.566328
[epoch 37, batch  1499] avg loss: 0.567028
[epoch 37, batch  1599] avg loss: 0.574811
[epoch 37, batch  1699] avg loss: 0.550801
[epoch 37, batch  1799] avg loss: 0.563771
[epoch 37, batch  1899] avg loss: 0.571718
[epoch 37, batch  1999] avg loss: 0.563749
[epoch 37, batch  2099] avg loss: 0.557966
[epoch 37, batch  2199] avg loss: 0.556857
[epoch 37, batch  2299] avg loss: 0.557531
[epoch 37, batch  2399] avg loss: 0.576249
[epoch 38, batch    99] avg loss: 0.559224
[epoch 38, batch   199] avg loss: 0.556442
[epoch 38, batch   299] avg loss: 0.562864
[epoch 38, batch   399] avg loss: 0.576179
[epoch 38, batch   499] avg loss: 0.554960
[epoch 38, batch   599] avg loss: 0.561535
[epoch 38, batch   699] avg loss: 0.556791
[epoch 38, batch   799] avg loss: 0.563908
[epoch 38, batch   899] avg loss: 0.569672
[epoch 38, batch   999] avg loss: 0.566015
[epoch 38, batch  1099] avg loss: 0.567152
[epoch 38, batch  1199] avg loss: 0.570602
[epoch 38, batch  1299] avg loss: 0.567871
[epoch 38, batch  1399] avg loss: 0.564235
[epoch 38, batch  1499] avg loss: 0.560120
[epoch 38, batch  1599] avg loss: 0.561120
[epoch 38, batch  1699] avg loss: 0.556524
[epoch 38, batch  1799] avg loss: 0.566703
[epoch 38, batch  1899] avg loss: 0.557305
[epoch 38, batch  1999] avg loss: 0.563353
[epoch 38, batch  2099] avg loss: 0.556224
[epoch 38, batch  2199] avg loss: 0.561747
[epoch 38, batch  2299] avg loss: 0.556009
[epoch 38, batch  2399] avg loss: 0.557175
[epoch 39, batch    99] avg loss: 0.562809
[epoch 39, batch   199] avg loss: 0.567940
[epoch 39, batch   299] avg loss: 0.569643
[epoch 39, batch   399] avg loss: 0.550145
[epoch 39, batch   499] avg loss: 0.571015
[epoch 39, batch   599] avg loss: 0.554402
[epoch 39, batch   699] avg loss: 0.548899
[epoch 39, batch   799] avg loss: 0.556407
[epoch 39, batch   899] avg loss: 0.555085
[epoch 39, batch   999] avg loss: 0.564747
[epoch 39, batch  1099] avg loss: 0.567192
[epoch 39, batch  1199] avg loss: 0.574575
[epoch 39, batch  1299] avg loss: 0.570129
[epoch 39, batch  1399] avg loss: 0.560640
[epoch 39, batch  1499] avg loss: 0.549379
[epoch 39, batch  1599] avg loss: 0.562479
[epoch 39, batch  1699] avg loss: 0.569679
[epoch 39, batch  1799] avg loss: 0.567244
[epoch 39, batch  1899] avg loss: 0.554237
[epoch 39, batch  1999] avg loss: 0.551740
[epoch 39, batch  2099] avg loss: 0.563531
[epoch 39, batch  2199] avg loss: 0.552885
[epoch 39, batch  2299] avg loss: 0.551636
[epoch 39, batch  2399] avg loss: 0.568338
Model saved to model/20200503-000833.pth.
accuracy/TriangPrismIsosc : 0.776
n_examples/TriangPrismIsosc : 500.0
accuracy/parallelepiped : 0.122
n_examples/parallelepiped : 500.0
accuracy/sphere : 1.0
n_examples/sphere : 102.0
accuracy/wire : 0.93
n_examples/wire : 200.0
accuracy/avg_geom : 0.5660522273425499
loss/validation_geom : 0.9231095628987442
accuracy/Au : 0.0
n_examples/Au : 0.0
accuracy/SiN : 0.9523809523809523
n_examples/SiN : 1302.0
accuracy/SiO2 : 0.0
n_examples/SiO2 : 0.0
accuracy/avg_mat : 0.9523809523809523
loss/validation_mat : 0.7266700220181279
MSE/ShortestDim : 3.2890846234862154
MAE/ShortestDim : 1.0206301933792512
MSE/MiddleDim : 6.82079355031847
MAE/MiddleDim : 1.795063223890079
MSE/LongDim : 195.96163241947485
MAE/LongDim : 9.031864371351016
MSE/log Area/Vol : 8.44469475343297
MAE/log Area/Vol : 2.6664530348301665
loss/validation_dim : 214.5162053467125
loss/validation : 216.16598493162937
Metrics saved to model/20200503-000833_metrics.csv.
Parsed 2604 rows from data/sim_train_spectrum_SiN.
Parsed 2604 rows from data/sim_train_labels_SiN.
Parsed 9765 rows from data/gen_spectrum_SiN_00-of-16.
Parsed 9765 rows from data/gen_labels_SiN_00-of-16.
Parsed 9765 rows from data/gen_spectrum_SiN_01-of-16.
Parsed 9765 rows from data/gen_labels_SiN_01-of-16.
Parsed 9765 rows from data/gen_spectrum_SiN_02-of-16.
Parsed 9765 rows from data/gen_labels_SiN_02-of-16.
Parsed 9765 rows from data/gen_spectrum_SiN_03-of-16.
Parsed 9765 rows from data/gen_labels_SiN_03-of-16.
Parsed 9765 rows from data/gen_spectrum_SiN_04-of-16.
Parsed 9765 rows from data/gen_labels_SiN_04-of-16.
Parsed 9765 rows from data/gen_spectrum_SiN_05-of-16.
Parsed 9765 rows from data/gen_labels_SiN_05-of-16.
Parsed 9765 rows from data/gen_spectrum_SiN_06-of-16.
Parsed 9765 rows from data/gen_labels_SiN_06-of-16.
Parsed 9765 rows from data/gen_spectrum_SiN_07-of-16.
Parsed 9765 rows from data/gen_labels_SiN_07-of-16.
Parsed 9765 rows from data/gen_spectrum_SiN_08-of-16.
Parsed 9765 rows from data/gen_labels_SiN_08-of-16.
Parsed 9765 rows from data/gen_spectrum_SiN_09-of-16.
Parsed 9765 rows from data/gen_labels_SiN_09-of-16.
Parsed 9765 rows from data/gen_spectrum_SiN_10-of-16.
Parsed 9765 rows from data/gen_labels_SiN_10-of-16.
Parsed 9765 rows from data/gen_spectrum_SiN_11-of-16.
Parsed 9765 rows from data/gen_labels_SiN_11-of-16.
Parsed 9765 rows from data/gen_spectrum_SiN_12-of-16.
Parsed 9765 rows from data/gen_labels_SiN_12-of-16.
Parsed 9765 rows from data/gen_spectrum_SiN_13-of-16.
Parsed 9765 rows from data/gen_labels_SiN_13-of-16.
Parsed 9765 rows from data/gen_spectrum_SiN_14-of-16.
Parsed 9765 rows from data/gen_labels_SiN_14-of-16.
Parsed 9765 rows from data/gen_spectrum_SiN_15-of-16.
Parsed 9765 rows from data/gen_labels_SiN_15-of-16.
Parsed 1302 rows from data/sim_validation_spectrum_SiN.
Parsed 1302 rows from data/sim_validation_labels_SiN.
Logging training progress to tensorboard dir runs/alexnet-SiN-lr_0.000050-trainsize_158844-05_03_2020_00:09-joint.
[epoch 0, batch    99] avg loss: 1.386432
[epoch 0, batch   199] avg loss: 1.385546
[epoch 0, batch   299] avg loss: 1.378574
[epoch 0, batch   399] avg loss: 1.330200
[epoch 0, batch   499] avg loss: 1.294766
[epoch 0, batch   599] avg loss: 1.225871
[epoch 0, batch   699] avg loss: 1.111058
[epoch 0, batch   799] avg loss: 1.011613
[epoch 0, batch   899] avg loss: 0.934374
[epoch 0, batch   999] avg loss: 0.896039
[epoch 0, batch  1099] avg loss: 0.866468
[epoch 0, batch  1199] avg loss: 0.845711
[epoch 0, batch  1299] avg loss: 0.847060
[epoch 0, batch  1399] avg loss: 0.830640
[epoch 0, batch  1499] avg loss: 0.841603
[epoch 0, batch  1599] avg loss: 0.836384
[epoch 0, batch  1699] avg loss: 0.825513
[epoch 0, batch  1799] avg loss: 0.821176
[epoch 0, batch  1899] avg loss: 0.809634
[epoch 0, batch  1999] avg loss: 0.812089
[epoch 0, batch  2099] avg loss: 0.807602
[epoch 0, batch  2199] avg loss: 0.802832
[epoch 0, batch  2299] avg loss: 0.799478
[epoch 0, batch  2399] avg loss: 0.786982
[epoch 1, batch    99] avg loss: 0.793122
[epoch 1, batch   199] avg loss: 0.781916
[epoch 1, batch   299] avg loss: 0.778445
[epoch 1, batch   399] avg loss: 0.775499
[epoch 1, batch   499] avg loss: 0.779081
[epoch 1, batch   599] avg loss: 0.781889
[epoch 1, batch   699] avg loss: 0.774661
[epoch 1, batch   799] avg loss: 0.765434
[epoch 1, batch   899] avg loss: 0.757872
[epoch 1, batch   999] avg loss: 0.754005
[epoch 1, batch  1099] avg loss: 0.758531
[epoch 1, batch  1199] avg loss: 0.741644
[epoch 1, batch  1299] avg loss: 0.747371
[epoch 1, batch  1399] avg loss: 0.746057
[epoch 1, batch  1499] avg loss: 0.731748
[epoch 1, batch  1599] avg loss: 0.746880
[epoch 1, batch  1699] avg loss: 0.738523
[epoch 1, batch  1799] avg loss: 0.738622
[epoch 1, batch  1899] avg loss: 0.718674
[epoch 1, batch  1999] avg loss: 0.717948
[epoch 1, batch  2099] avg loss: 0.730528
[epoch 1, batch  2199] avg loss: 0.704178
[epoch 1, batch  2299] avg loss: 0.719095
[epoch 1, batch  2399] avg loss: 0.714925
[epoch 2, batch    99] avg loss: 0.712854
[epoch 2, batch   199] avg loss: 0.707659
[epoch 2, batch   299] avg loss: 0.708132
[epoch 2, batch   399] avg loss: 0.712632
[epoch 2, batch   499] avg loss: 0.696346
[epoch 2, batch   599] avg loss: 0.704625
[epoch 2, batch   699] avg loss: 0.701750
[epoch 2, batch   799] avg loss: 0.696382
[epoch 2, batch   899] avg loss: 0.688934
[epoch 2, batch   999] avg loss: 0.693594
[epoch 2, batch  1099] avg loss: 0.696263
[epoch 2, batch  1199] avg loss: 0.690561
[epoch 2, batch  1299] avg loss: 0.681321
[epoch 2, batch  1399] avg loss: 0.686666
[epoch 2, batch  1499] avg loss: 0.692899
[epoch 2, batch  1599] avg loss: 0.668528
[epoch 2, batch  1699] avg loss: 0.666261
[epoch 2, batch  1799] avg loss: 0.664656
[epoch 2, batch  1899] avg loss: 0.678120
[epoch 2, batch  1999] avg loss: 0.676295
[epoch 2, batch  2099] avg loss: 0.671368
[epoch 2, batch  2199] avg loss: 0.666171
[epoch 2, batch  2299] avg loss: 0.670654
[epoch 2, batch  2399] avg loss: 0.665971
[epoch 3, batch    99] avg loss: 0.658698
[epoch 3, batch   199] avg loss: 0.645909
[epoch 3, batch   299] avg loss: 0.670381
[epoch 3, batch   399] avg loss: 0.652463
[epoch 3, batch   499] avg loss: 0.659862
[epoch 3, batch   599] avg loss: 0.656115
[epoch 3, batch   699] avg loss: 0.643926
[epoch 3, batch   799] avg loss: 0.652268
[epoch 3, batch   899] avg loss: 0.645720
[epoch 3, batch   999] avg loss: 0.645634
[epoch 3, batch  1099] avg loss: 0.663590
[epoch 3, batch  1199] avg loss: 0.638661
[epoch 3, batch  1299] avg loss: 0.655445
[epoch 3, batch  1399] avg loss: 0.642307
[epoch 3, batch  1499] avg loss: 0.643171
[epoch 3, batch  1599] avg loss: 0.640838
[epoch 3, batch  1699] avg loss: 0.639269
[epoch 3, batch  1799] avg loss: 0.650623
[epoch 3, batch  1899] avg loss: 0.634488
[epoch 3, batch  1999] avg loss: 0.656470
[epoch 3, batch  2099] avg loss: 0.656453
[epoch 3, batch  2199] avg loss: 0.632104
[epoch 3, batch  2299] avg loss: 0.635026
[epoch 3, batch  2399] avg loss: 0.626088
[epoch 4, batch    99] avg loss: 0.652127
[epoch 4, batch   199] avg loss: 0.630510
[epoch 4, batch   299] avg loss: 0.633066
[epoch 4, batch   399] avg loss: 0.654536
[epoch 4, batch   499] avg loss: 0.631545
[epoch 4, batch   599] avg loss: 0.634208
[epoch 4, batch   699] avg loss: 0.615844
[epoch 4, batch   799] avg loss: 0.624108
[epoch 4, batch   899] avg loss: 0.635645
[epoch 4, batch   999] avg loss: 0.619385
[epoch 4, batch  1099] avg loss: 0.618371
[epoch 4, batch  1199] avg loss: 0.633252
[epoch 4, batch  1299] avg loss: 0.634052
[epoch 4, batch  1399] avg loss: 0.629243
[epoch 4, batch  1499] avg loss: 0.611085
[epoch 4, batch  1599] avg loss: 0.618111
[epoch 4, batch  1699] avg loss: 0.644431
[epoch 4, batch  1799] avg loss: 0.628669
[epoch 4, batch  1899] avg loss: 0.606045
[epoch 4, batch  1999] avg loss: 0.608982
[epoch 4, batch  2099] avg loss: 0.591091
[epoch 4, batch  2199] avg loss: 0.622459
[epoch 4, batch  2299] avg loss: 0.625781
[epoch 4, batch  2399] avg loss: 0.607298
[epoch 5, batch    99] avg loss: 0.612897
[epoch 5, batch   199] avg loss: 0.621110
[epoch 5, batch   299] avg loss: 0.618389
[epoch 5, batch   399] avg loss: 0.615607
[epoch 5, batch   499] avg loss: 0.603604
[epoch 5, batch   599] avg loss: 0.617141
[epoch 5, batch   699] avg loss: 0.635876
[epoch 5, batch   799] avg loss: 0.605078
[epoch 5, batch   899] avg loss: 0.597094
[epoch 5, batch   999] avg loss: 0.600952
[epoch 5, batch  1099] avg loss: 0.607045
[epoch 5, batch  1199] avg loss: 0.611274
[epoch 5, batch  1299] avg loss: 0.614041
[epoch 5, batch  1399] avg loss: 0.635301
[epoch 5, batch  1499] avg loss: 0.598714
[epoch 5, batch  1599] avg loss: 0.603302
[epoch 5, batch  1699] avg loss: 0.615190
[epoch 5, batch  1799] avg loss: 0.619365
[epoch 5, batch  1899] avg loss: 0.609341
[epoch 5, batch  1999] avg loss: 0.606726
[epoch 5, batch  2099] avg loss: 0.606601
[epoch 5, batch  2199] avg loss: 0.604074
[epoch 5, batch  2299] avg loss: 0.605541
[epoch 5, batch  2399] avg loss: 0.607819
[epoch 6, batch    99] avg loss: 0.605721
[epoch 6, batch   199] avg loss: 0.591862
[epoch 6, batch   299] avg loss: 0.602238
[epoch 6, batch   399] avg loss: 0.591385
[epoch 6, batch   499] avg loss: 0.615975
[epoch 6, batch   599] avg loss: 0.592869
[epoch 6, batch   699] avg loss: 0.599393
[epoch 6, batch   799] avg loss: 0.592869
[epoch 6, batch   899] avg loss: 0.608003
[epoch 6, batch   999] avg loss: 0.609122
[epoch 6, batch  1099] avg loss: 0.599508
[epoch 6, batch  1199] avg loss: 0.607757
[epoch 6, batch  1299] avg loss: 0.598561
[epoch 6, batch  1399] avg loss: 0.607787
[epoch 6, batch  1499] avg loss: 0.593905
[epoch 6, batch  1599] avg loss: 0.598874
[epoch 6, batch  1699] avg loss: 0.606141
[epoch 6, batch  1799] avg loss: 0.587698
[epoch 6, batch  1899] avg loss: 0.585802
[epoch 6, batch  1999] avg loss: 0.593408
[epoch 6, batch  2099] avg loss: 0.600142
[epoch 6, batch  2199] avg loss: 0.587773
[epoch 6, batch  2299] avg loss: 0.595721
[epoch 6, batch  2399] avg loss: 0.590821
[epoch 7, batch    99] avg loss: 0.591505
[epoch 7, batch   199] avg loss: 0.595455
[epoch 7, batch   299] avg loss: 0.600721
[epoch 7, batch   399] avg loss: 0.592713
[epoch 7, batch   499] avg loss: 0.583525
[epoch 7, batch   599] avg loss: 0.595418
[epoch 7, batch   699] avg loss: 0.597299
[epoch 7, batch   799] avg loss: 0.588533
[epoch 7, batch   899] avg loss: 0.603969
[epoch 7, batch   999] avg loss: 0.577918
[epoch 7, batch  1099] avg loss: 0.583617
[epoch 7, batch  1199] avg loss: 0.589492
[epoch 7, batch  1299] avg loss: 0.590666
[epoch 7, batch  1399] avg loss: 0.600261
[epoch 7, batch  1499] avg loss: 0.584888
[epoch 7, batch  1599] avg loss: 0.591861
[epoch 7, batch  1699] avg loss: 0.584726
[epoch 7, batch  1799] avg loss: 0.574966
[epoch 7, batch  1899] avg loss: 0.583983
[epoch 7, batch  1999] avg loss: 0.593321
[epoch 7, batch  2099] avg loss: 0.607926
[epoch 7, batch  2199] avg loss: 0.595343
[epoch 7, batch  2299] avg loss: 0.584274
[epoch 7, batch  2399] avg loss: 0.585608
[epoch 8, batch    99] avg loss: 0.589032
[epoch 8, batch   199] avg loss: 0.589995
[epoch 8, batch   299] avg loss: 0.574905
[epoch 8, batch   399] avg loss: 0.576456
[epoch 8, batch   499] avg loss: 0.576195
[epoch 8, batch   599] avg loss: 0.570236
[epoch 8, batch   699] avg loss: 0.597854
[epoch 8, batch   799] avg loss: 0.576365
[epoch 8, batch   899] avg loss: 0.578104
[epoch 8, batch   999] avg loss: 0.576364
[epoch 8, batch  1099] avg loss: 0.577904
[epoch 8, batch  1199] avg loss: 0.579236
[epoch 8, batch  1299] avg loss: 0.592488
[epoch 8, batch  1399] avg loss: 0.594223
[epoch 8, batch  1499] avg loss: 0.587559
[epoch 8, batch  1599] avg loss: 0.589166
[epoch 8, batch  1699] avg loss: 0.572243
[epoch 8, batch  1799] avg loss: 0.584480
[epoch 8, batch  1899] avg loss: 0.571237
[epoch 8, batch  1999] avg loss: 0.589376
[epoch 8, batch  2099] avg loss: 0.593334
[epoch 8, batch  2199] avg loss: 0.593190
[epoch 8, batch  2299] avg loss: 0.588384
[epoch 8, batch  2399] avg loss: 0.585089
[epoch 9, batch    99] avg loss: 0.574812
[epoch 9, batch   199] avg loss: 0.579823
[epoch 9, batch   299] avg loss: 0.576131
[epoch 9, batch   399] avg loss: 0.578948
[epoch 9, batch   499] avg loss: 0.590489
[epoch 9, batch   599] avg loss: 0.586158
[epoch 9, batch   699] avg loss: 0.569300
[epoch 9, batch   799] avg loss: 0.585585
[epoch 9, batch   899] avg loss: 0.575458
[epoch 9, batch   999] avg loss: 0.583917
[epoch 9, batch  1099] avg loss: 0.571509
[epoch 9, batch  1199] avg loss: 0.572004
[epoch 9, batch  1299] avg loss: 0.589248
[epoch 9, batch  1399] avg loss: 0.587844
[epoch 9, batch  1499] avg loss: 0.594684
[epoch 9, batch  1599] avg loss: 0.579381
[epoch 9, batch  1699] avg loss: 0.577973
[epoch 9, batch  1799] avg loss: 0.570313
[epoch 9, batch  1899] avg loss: 0.585495
[epoch 9, batch  1999] avg loss: 0.582066
[epoch 9, batch  2099] avg loss: 0.580636
[epoch 9, batch  2199] avg loss: 0.595434
[epoch 9, batch  2299] avg loss: 0.582696
[epoch 9, batch  2399] avg loss: 0.586058
[epoch 10, batch    99] avg loss: 0.569475
[epoch 10, batch   199] avg loss: 0.575047
[epoch 10, batch   299] avg loss: 0.569572
[epoch 10, batch   399] avg loss: 0.582358
[epoch 10, batch   499] avg loss: 0.581716
[epoch 10, batch   599] avg loss: 0.585471
[epoch 10, batch   699] avg loss: 0.575137
[epoch 10, batch   799] avg loss: 0.583701
[epoch 10, batch   899] avg loss: 0.561375
[epoch 10, batch   999] avg loss: 0.573930
[epoch 10, batch  1099] avg loss: 0.586339
[epoch 10, batch  1199] avg loss: 0.575537
[epoch 10, batch  1299] avg loss: 0.571498
[epoch 10, batch  1399] avg loss: 0.583088
[epoch 10, batch  1499] avg loss: 0.575385
[epoch 10, batch  1599] avg loss: 0.598221
[epoch 10, batch  1699] avg loss: 0.572953
[epoch 10, batch  1799] avg loss: 0.584491
[epoch 10, batch  1899] avg loss: 0.581921
[epoch 10, batch  1999] avg loss: 0.580548
[epoch 10, batch  2099] avg loss: 0.586319
[epoch 10, batch  2199] avg loss: 0.576262
[epoch 10, batch  2299] avg loss: 0.565220
[epoch 10, batch  2399] avg loss: 0.574755
[epoch 11, batch    99] avg loss: 0.569682
[epoch 11, batch   199] avg loss: 0.579055
[epoch 11, batch   299] avg loss: 0.575774
[epoch 11, batch   399] avg loss: 0.566633
[epoch 11, batch   499] avg loss: 0.556843
[epoch 11, batch   599] avg loss: 0.580943
[epoch 11, batch   699] avg loss: 0.575734
[epoch 11, batch   799] avg loss: 0.564432
[epoch 11, batch   899] avg loss: 0.564000
[epoch 11, batch   999] avg loss: 0.567037
[epoch 11, batch  1099] avg loss: 0.562332
[epoch 11, batch  1199] avg loss: 0.588146
[epoch 11, batch  1299] avg loss: 0.570297
[epoch 11, batch  1399] avg loss: 0.570879
[epoch 11, batch  1499] avg loss: 0.581218
[epoch 11, batch  1599] avg loss: 0.583824
[epoch 11, batch  1699] avg loss: 0.575726
[epoch 11, batch  1799] avg loss: 0.561955
[epoch 11, batch  1899] avg loss: 0.583428
[epoch 11, batch  1999] avg loss: 0.550566
[epoch 11, batch  2099] avg loss: 0.557701
[epoch 11, batch  2199] avg loss: 0.573771
[epoch 11, batch  2299] avg loss: 0.564933
[epoch 11, batch  2399] avg loss: 0.571542
[epoch 12, batch    99] avg loss: 0.564984
[epoch 12, batch   199] avg loss: 0.579624
[epoch 12, batch   299] avg loss: 0.562237
[epoch 12, batch   399] avg loss: 0.568892
[epoch 12, batch   499] avg loss: 0.558376
[epoch 12, batch   599] avg loss: 0.576080
[epoch 12, batch   699] avg loss: 0.547657
[epoch 12, batch   799] avg loss: 0.563131
[epoch 12, batch   899] avg loss: 0.571729
[epoch 12, batch   999] avg loss: 0.563304
[epoch 12, batch  1099] avg loss: 0.569944
[epoch 12, batch  1199] avg loss: 0.576963
[epoch 12, batch  1299] avg loss: 0.561481
[epoch 12, batch  1399] avg loss: 0.566237
[epoch 12, batch  1499] avg loss: 0.580684
[epoch 12, batch  1599] avg loss: 0.586601
[epoch 12, batch  1699] avg loss: 0.567195
[epoch 12, batch  1799] avg loss: 0.572831
[epoch 12, batch  1899] avg loss: 0.551188
[epoch 12, batch  1999] avg loss: 0.561014
[epoch 12, batch  2099] avg loss: 0.581080
[epoch 12, batch  2199] avg loss: 0.565046
[epoch 12, batch  2299] avg loss: 0.555869
[epoch 12, batch  2399] avg loss: 0.547465
[epoch 13, batch    99] avg loss: 0.575427
[epoch 13, batch   199] avg loss: 0.549546
[epoch 13, batch   299] avg loss: 0.566412
[epoch 13, batch   399] avg loss: 0.553288
[epoch 13, batch   499] avg loss: 0.563433
[epoch 13, batch   599] avg loss: 0.560344
[epoch 13, batch   699] avg loss: 0.560850
[epoch 13, batch   799] avg loss: 0.566472
[epoch 13, batch   899] avg loss: 0.564661
[epoch 13, batch   999] avg loss: 0.551431
[epoch 13, batch  1099] avg loss: 0.556652
[epoch 13, batch  1199] avg loss: 0.564596
[epoch 13, batch  1299] avg loss: 0.573792
[epoch 13, batch  1399] avg loss: 0.565063
[epoch 13, batch  1499] avg loss: 0.575771
[epoch 13, batch  1599] avg loss: 0.568755
[epoch 13, batch  1699] avg loss: 0.556065
[epoch 13, batch  1799] avg loss: 0.570411
[epoch 13, batch  1899] avg loss: 0.566019
[epoch 13, batch  1999] avg loss: 0.560442
[epoch 13, batch  2099] avg loss: 0.561961
[epoch 13, batch  2199] avg loss: 0.567247
[epoch 13, batch  2299] avg loss: 0.571289
[epoch 13, batch  2399] avg loss: 0.551189
[epoch 14, batch    99] avg loss: 0.579182
[epoch 14, batch   199] avg loss: 0.554635
[epoch 14, batch   299] avg loss: 0.555788
[epoch 14, batch   399] avg loss: 0.571311
[epoch 14, batch   499] avg loss: 0.553603
[epoch 14, batch   599] avg loss: 0.575841
[epoch 14, batch   699] avg loss: 0.572899
[epoch 14, batch   799] avg loss: 0.558701
[epoch 14, batch   899] avg loss: 0.555517
[epoch 14, batch   999] avg loss: 0.561534
[epoch 14, batch  1099] avg loss: 0.554080
[epoch 14, batch  1199] avg loss: 0.579035
[epoch 14, batch  1299] avg loss: 0.553688
[epoch 14, batch  1399] avg loss: 0.569069
[epoch 14, batch  1499] avg loss: 0.549335
[epoch 14, batch  1599] avg loss: 0.550140
[epoch 14, batch  1699] avg loss: 0.558046
[epoch 14, batch  1799] avg loss: 0.571200
[epoch 14, batch  1899] avg loss: 0.548438
[epoch 14, batch  1999] avg loss: 0.569273
[epoch 14, batch  2099] avg loss: 0.559640
[epoch 14, batch  2199] avg loss: 0.554404
[epoch 14, batch  2299] avg loss: 0.569394
[epoch 14, batch  2399] avg loss: 0.562121
[epoch 15, batch    99] avg loss: 0.556778
[epoch 15, batch   199] avg loss: 0.560157
[epoch 15, batch   299] avg loss: 0.569560
[epoch 15, batch   399] avg loss: 0.547702
[epoch 15, batch   499] avg loss: 0.561133
[epoch 15, batch   599] avg loss: 0.561822
[epoch 15, batch   699] avg loss: 0.560012
[epoch 15, batch   799] avg loss: 0.554147
[epoch 15, batch   899] avg loss: 0.562645
[epoch 15, batch   999] avg loss: 0.562357
[epoch 15, batch  1099] avg loss: 0.558187
[epoch 15, batch  1199] avg loss: 0.563737
[epoch 15, batch  1299] avg loss: 0.547643
[epoch 15, batch  1399] avg loss: 0.552849
[epoch 15, batch  1499] avg loss: 0.543021
[epoch 15, batch  1599] avg loss: 0.554369
[epoch 15, batch  1699] avg loss: 0.565972
[epoch 15, batch  1799] avg loss: 0.570904
[epoch 15, batch  1899] avg loss: 0.546917
[epoch 15, batch  1999] avg loss: 0.554104
[epoch 15, batch  2099] avg loss: 0.559465
[epoch 15, batch  2199] avg loss: 0.543232
[epoch 15, batch  2299] avg loss: 0.555779
[epoch 15, batch  2399] avg loss: 0.557972
[epoch 16, batch    99] avg loss: 0.555620
[epoch 16, batch   199] avg loss: 0.553577
[epoch 16, batch   299] avg loss: 0.568959
[epoch 16, batch   399] avg loss: 0.561356
[epoch 16, batch   499] avg loss: 0.554984
[epoch 16, batch   599] avg loss: 0.566195
[epoch 16, batch   699] avg loss: 0.552138
[epoch 16, batch   799] avg loss: 0.551900
[epoch 16, batch   899] avg loss: 0.546619
[epoch 16, batch   999] avg loss: 0.554117
[epoch 16, batch  1099] avg loss: 0.535756
[epoch 16, batch  1199] avg loss: 0.559049
[epoch 16, batch  1299] avg loss: 0.560581
[epoch 16, batch  1399] avg loss: 0.536205
[epoch 16, batch  1499] avg loss: 0.558424
[epoch 16, batch  1599] avg loss: 0.549108
[epoch 16, batch  1699] avg loss: 0.551835
[epoch 16, batch  1799] avg loss: 0.569516
[epoch 16, batch  1899] avg loss: 0.545666
[epoch 16, batch  1999] avg loss: 0.553975
[epoch 16, batch  2099] avg loss: 0.559571
[epoch 16, batch  2199] avg loss: 0.553201
[epoch 16, batch  2299] avg loss: 0.571454
[epoch 16, batch  2399] avg loss: 0.555174
[epoch 17, batch    99] avg loss: 0.556123
[epoch 17, batch   199] avg loss: 0.549938
[epoch 17, batch   299] avg loss: 0.556545
[epoch 17, batch   399] avg loss: 0.545814
[epoch 17, batch   499] avg loss: 0.562980
[epoch 17, batch   599] avg loss: 0.547090
[epoch 17, batch   699] avg loss: 0.548705
[epoch 17, batch   799] avg loss: 0.560678
[epoch 17, batch   899] avg loss: 0.539947
[epoch 17, batch   999] avg loss: 0.550513
[epoch 17, batch  1099] avg loss: 0.542046
[epoch 17, batch  1199] avg loss: 0.553351
[epoch 17, batch  1299] avg loss: 0.540171
[epoch 17, batch  1399] avg loss: 0.552809
[epoch 17, batch  1499] avg loss: 0.537683
[epoch 17, batch  1599] avg loss: 0.553014
[epoch 17, batch  1699] avg loss: 0.558378
[epoch 17, batch  1799] avg loss: 0.560043
[epoch 17, batch  1899] avg loss: 0.530404
[epoch 17, batch  1999] avg loss: 0.540295
[epoch 17, batch  2099] avg loss: 0.571246
[epoch 17, batch  2199] avg loss: 0.539148
[epoch 17, batch  2299] avg loss: 0.539054
[epoch 17, batch  2399] avg loss: 0.550625
[epoch 18, batch    99] avg loss: 0.536425
[epoch 18, batch   199] avg loss: 0.548141
[epoch 18, batch   299] avg loss: 0.566821
[epoch 18, batch   399] avg loss: 0.555129
[epoch 18, batch   499] avg loss: 0.548796
[epoch 18, batch   599] avg loss: 0.542827
[epoch 18, batch   699] avg loss: 0.538212
[epoch 18, batch   799] avg loss: 0.548759
[epoch 18, batch   899] avg loss: 0.552452
[epoch 18, batch   999] avg loss: 0.546822
[epoch 18, batch  1099] avg loss: 0.532773
[epoch 18, batch  1199] avg loss: 0.538420
[epoch 18, batch  1299] avg loss: 0.541824
[epoch 18, batch  1399] avg loss: 0.546649
[epoch 18, batch  1499] avg loss: 0.538777
[epoch 18, batch  1599] avg loss: 0.550606
[epoch 18, batch  1699] avg loss: 0.542457
[epoch 18, batch  1799] avg loss: 0.546556
[epoch 18, batch  1899] avg loss: 0.547108
[epoch 18, batch  1999] avg loss: 0.542894
[epoch 18, batch  2099] avg loss: 0.532895
[epoch 18, batch  2199] avg loss: 0.548694
[epoch 18, batch  2299] avg loss: 0.545322
[epoch 18, batch  2399] avg loss: 0.551463
[epoch 19, batch    99] avg loss: 0.529313
[epoch 19, batch   199] avg loss: 0.519518
[epoch 19, batch   299] avg loss: 0.540890
[epoch 19, batch   399] avg loss: 0.545742
[epoch 19, batch   499] avg loss: 0.543807
[epoch 19, batch   599] avg loss: 0.545173
[epoch 19, batch   699] avg loss: 0.531613
[epoch 19, batch   799] avg loss: 0.543984
[epoch 19, batch   899] avg loss: 0.532713
[epoch 19, batch   999] avg loss: 0.531910
[epoch 19, batch  1099] avg loss: 0.537249
[epoch 19, batch  1199] avg loss: 0.530220
[epoch 19, batch  1299] avg loss: 0.547813
[epoch 19, batch  1399] avg loss: 0.533533
[epoch 19, batch  1499] avg loss: 0.545177
[epoch 19, batch  1599] avg loss: 0.533861
[epoch 19, batch  1699] avg loss: 0.534549
[epoch 19, batch  1799] avg loss: 0.533629
[epoch 19, batch  1899] avg loss: 0.541431
[epoch 19, batch  1999] avg loss: 0.550799
[epoch 19, batch  2099] avg loss: 0.537301
[epoch 19, batch  2199] avg loss: 0.557460
[epoch 19, batch  2299] avg loss: 0.559421
[epoch 19, batch  2399] avg loss: 0.532205
[epoch 20, batch    99] avg loss: 0.539461
[epoch 20, batch   199] avg loss: 0.538191
[epoch 20, batch   299] avg loss: 0.529746
[epoch 20, batch   399] avg loss: 0.539232
[epoch 20, batch   499] avg loss: 0.547770
[epoch 20, batch   599] avg loss: 0.542035
[epoch 20, batch   699] avg loss: 0.539225
[epoch 20, batch   799] avg loss: 0.536178
[epoch 20, batch   899] avg loss: 0.521533
[epoch 20, batch   999] avg loss: 0.536782
[epoch 20, batch  1099] avg loss: 0.536101
[epoch 20, batch  1199] avg loss: 0.525806
[epoch 20, batch  1299] avg loss: 0.535834
[epoch 20, batch  1399] avg loss: 0.544811
[epoch 20, batch  1499] avg loss: 0.538419
[epoch 20, batch  1599] avg loss: 0.545266
[epoch 20, batch  1699] avg loss: 0.534162
[epoch 20, batch  1799] avg loss: 0.522735
[epoch 20, batch  1899] avg loss: 0.526934
[epoch 20, batch  1999] avg loss: 0.532932
[epoch 20, batch  2099] avg loss: 0.535265
[epoch 20, batch  2199] avg loss: 0.537286
[epoch 20, batch  2299] avg loss: 0.530797
[epoch 20, batch  2399] avg loss: 0.543436
[epoch 21, batch    99] avg loss: 0.526636
[epoch 21, batch   199] avg loss: 0.536472
[epoch 21, batch   299] avg loss: 0.547723
[epoch 21, batch   399] avg loss: 0.516146
[epoch 21, batch   499] avg loss: 0.546573
[epoch 21, batch   599] avg loss: 0.517461
[epoch 21, batch   699] avg loss: 0.525650
[epoch 21, batch   799] avg loss: 0.526179
[epoch 21, batch   899] avg loss: 0.534048
[epoch 21, batch   999] avg loss: 0.537576
[epoch 21, batch  1099] avg loss: 0.536411
[epoch 21, batch  1199] avg loss: 0.530888
[epoch 21, batch  1299] avg loss: 0.545397
[epoch 21, batch  1399] avg loss: 0.530620
[epoch 21, batch  1499] avg loss: 0.518676
[epoch 21, batch  1599] avg loss: 0.539046
[epoch 21, batch  1699] avg loss: 0.531766
[epoch 21, batch  1799] avg loss: 0.513307
[epoch 21, batch  1899] avg loss: 0.539464
[epoch 21, batch  1999] avg loss: 0.525135
[epoch 21, batch  2099] avg loss: 0.531227
[epoch 21, batch  2199] avg loss: 0.537850
[epoch 21, batch  2299] avg loss: 0.520628
[epoch 21, batch  2399] avg loss: 0.521550
[epoch 22, batch    99] avg loss: 0.526143
[epoch 22, batch   199] avg loss: 0.532298
[epoch 22, batch   299] avg loss: 0.541816
[epoch 22, batch   399] avg loss: 0.519199
[epoch 22, batch   499] avg loss: 0.526654
[epoch 22, batch   599] avg loss: 0.524185
[epoch 22, batch   699] avg loss: 0.538746
[epoch 22, batch   799] avg loss: 0.528398
[epoch 22, batch   899] avg loss: 0.539360
[epoch 22, batch   999] avg loss: 0.536865
[epoch 22, batch  1099] avg loss: 0.545043
[epoch 22, batch  1199] avg loss: 0.526290
[epoch 22, batch  1299] avg loss: 0.521059
[epoch 22, batch  1399] avg loss: 0.538737
[epoch 22, batch  1499] avg loss: 0.526161
[epoch 22, batch  1599] avg loss: 0.533340
[epoch 22, batch  1699] avg loss: 0.522580
[epoch 22, batch  1799] avg loss: 0.522877
[epoch 22, batch  1899] avg loss: 0.523486
[epoch 22, batch  1999] avg loss: 0.516782
[epoch 22, batch  2099] avg loss: 0.535255
[epoch 22, batch  2199] avg loss: 0.514355
[epoch 22, batch  2299] avg loss: 0.517083
[epoch 22, batch  2399] avg loss: 0.527307
[epoch 23, batch    99] avg loss: 0.522777
[epoch 23, batch   199] avg loss: 0.528174
[epoch 23, batch   299] avg loss: 0.527630
[epoch 23, batch   399] avg loss: 0.529611
[epoch 23, batch   499] avg loss: 0.527831
[epoch 23, batch   599] avg loss: 0.527442
[epoch 23, batch   699] avg loss: 0.524982
[epoch 23, batch   799] avg loss: 0.523862
[epoch 23, batch   899] avg loss: 0.523943
[epoch 23, batch   999] avg loss: 0.515937
[epoch 23, batch  1099] avg loss: 0.539809
[epoch 23, batch  1199] avg loss: 0.522329
[epoch 23, batch  1299] avg loss: 0.514689
[epoch 23, batch  1399] avg loss: 0.502234
[epoch 23, batch  1499] avg loss: 0.526418
[epoch 23, batch  1599] avg loss: 0.535904
[epoch 23, batch  1699] avg loss: 0.522184
[epoch 23, batch  1799] avg loss: 0.550574
[epoch 23, batch  1899] avg loss: 0.531540
[epoch 23, batch  1999] avg loss: 0.521028
[epoch 23, batch  2099] avg loss: 0.511869
[epoch 23, batch  2199] avg loss: 0.506961
[epoch 23, batch  2299] avg loss: 0.523587
[epoch 23, batch  2399] avg loss: 0.521675
[epoch 24, batch    99] avg loss: 0.517842
[epoch 24, batch   199] avg loss: 0.525909
[epoch 24, batch   299] avg loss: 0.518801
[epoch 24, batch   399] avg loss: 0.528382
[epoch 24, batch   499] avg loss: 0.528030
[epoch 24, batch   599] avg loss: 0.522808
[epoch 24, batch   699] avg loss: 0.518333
[epoch 24, batch   799] avg loss: 0.520464
[epoch 24, batch   899] avg loss: 0.526322
[epoch 24, batch   999] avg loss: 0.523018
[epoch 24, batch  1099] avg loss: 0.539388
[epoch 24, batch  1199] avg loss: 0.517398
[epoch 24, batch  1299] avg loss: 0.513110
[epoch 24, batch  1399] avg loss: 0.519783
[epoch 24, batch  1499] avg loss: 0.523155
[epoch 24, batch  1599] avg loss: 0.523020
[epoch 24, batch  1699] avg loss: 0.524141
[epoch 24, batch  1799] avg loss: 0.518677
[epoch 24, batch  1899] avg loss: 0.514031
[epoch 24, batch  1999] avg loss: 0.504685
[epoch 24, batch  2099] avg loss: 0.530523
[epoch 24, batch  2199] avg loss: 0.515581
[epoch 24, batch  2299] avg loss: 0.515276
[epoch 24, batch  2399] avg loss: 0.529708
[epoch 25, batch    99] avg loss: 0.513097
[epoch 25, batch   199] avg loss: 0.515135
[epoch 25, batch   299] avg loss: 0.513579
[epoch 25, batch   399] avg loss: 0.513804
[epoch 25, batch   499] avg loss: 0.529774
[epoch 25, batch   599] avg loss: 0.522413
[epoch 25, batch   699] avg loss: 0.536008
[epoch 25, batch   799] avg loss: 0.504367
[epoch 25, batch   899] avg loss: 0.503974
[epoch 25, batch   999] avg loss: 0.512777
[epoch 25, batch  1099] avg loss: 0.504639
[epoch 25, batch  1199] avg loss: 0.528303
[epoch 25, batch  1299] avg loss: 0.524533
[epoch 25, batch  1399] avg loss: 0.504104
[epoch 25, batch  1499] avg loss: 0.525296
[epoch 25, batch  1599] avg loss: 0.516768
[epoch 25, batch  1699] avg loss: 0.516866
[epoch 25, batch  1799] avg loss: 0.512374
[epoch 25, batch  1899] avg loss: 0.514404
[epoch 25, batch  1999] avg loss: 0.528001
[epoch 25, batch  2099] avg loss: 0.513947
[epoch 25, batch  2199] avg loss: 0.523149
[epoch 25, batch  2299] avg loss: 0.507853
[epoch 25, batch  2399] avg loss: 0.516343
[epoch 26, batch    99] avg loss: 0.519781
[epoch 26, batch   199] avg loss: 0.495297
[epoch 26, batch   299] avg loss: 0.513508
[epoch 26, batch   399] avg loss: 0.506968
[epoch 26, batch   499] avg loss: 0.500253
[epoch 26, batch   599] avg loss: 0.526662
[epoch 26, batch   699] avg loss: 0.501823
[epoch 26, batch   799] avg loss: 0.521205
[epoch 26, batch   899] avg loss: 0.507927
[epoch 26, batch   999] avg loss: 0.519790
[epoch 26, batch  1099] avg loss: 0.523028
[epoch 26, batch  1199] avg loss: 0.532654
[epoch 26, batch  1299] avg loss: 0.526874
[epoch 26, batch  1399] avg loss: 0.512099
[epoch 26, batch  1499] avg loss: 0.512806
[epoch 26, batch  1599] avg loss: 0.507865
[epoch 26, batch  1699] avg loss: 0.509296
[epoch 26, batch  1799] avg loss: 0.514523
[epoch 26, batch  1899] avg loss: 0.520234
[epoch 26, batch  1999] avg loss: 0.507629
[epoch 26, batch  2099] avg loss: 0.520237
[epoch 26, batch  2199] avg loss: 0.514422
[epoch 26, batch  2299] avg loss: 0.505309
[epoch 26, batch  2399] avg loss: 0.512342
[epoch 27, batch    99] avg loss: 0.522067
[epoch 27, batch   199] avg loss: 0.520695
[epoch 27, batch   299] avg loss: 0.513943
[epoch 27, batch   399] avg loss: 0.510981
[epoch 27, batch   499] avg loss: 0.520015
[epoch 27, batch   599] avg loss: 0.503875
[epoch 27, batch   699] avg loss: 0.519497
[epoch 27, batch   799] avg loss: 0.495559
[epoch 27, batch   899] avg loss: 0.508205
[epoch 27, batch   999] avg loss: 0.513043
[epoch 27, batch  1099] avg loss: 0.512654
[epoch 27, batch  1199] avg loss: 0.531017
[epoch 27, batch  1299] avg loss: 0.503663
[epoch 27, batch  1399] avg loss: 0.506371
[epoch 27, batch  1499] avg loss: 0.507585
[epoch 27, batch  1599] avg loss: 0.511045
[epoch 27, batch  1699] avg loss: 0.501878
[epoch 27, batch  1799] avg loss: 0.509078
[epoch 27, batch  1899] avg loss: 0.524242
[epoch 27, batch  1999] avg loss: 0.502118
[epoch 27, batch  2099] avg loss: 0.510595
[epoch 27, batch  2199] avg loss: 0.495853
[epoch 27, batch  2299] avg loss: 0.513717
[epoch 27, batch  2399] avg loss: 0.512957
[epoch 28, batch    99] avg loss: 0.510364
[epoch 28, batch   199] avg loss: 0.502589
[epoch 28, batch   299] avg loss: 0.520223
[epoch 28, batch   399] avg loss: 0.496443
[epoch 28, batch   499] avg loss: 0.499728
[epoch 28, batch   599] avg loss: 0.514365
[epoch 28, batch   699] avg loss: 0.492281
[epoch 28, batch   799] avg loss: 0.506661
[epoch 28, batch   899] avg loss: 0.524421
[epoch 28, batch   999] avg loss: 0.504841
[epoch 28, batch  1099] avg loss: 0.513537
[epoch 28, batch  1199] avg loss: 0.511665
[epoch 28, batch  1299] avg loss: 0.505045
[epoch 28, batch  1399] avg loss: 0.502398
[epoch 28, batch  1499] avg loss: 0.520068
[epoch 28, batch  1599] avg loss: 0.489984
[epoch 28, batch  1699] avg loss: 0.525313
[epoch 28, batch  1799] avg loss: 0.509229
[epoch 28, batch  1899] avg loss: 0.500984
[epoch 28, batch  1999] avg loss: 0.502835
[epoch 28, batch  2099] avg loss: 0.507069
[epoch 28, batch  2199] avg loss: 0.506329
[epoch 28, batch  2299] avg loss: 0.501937
[epoch 28, batch  2399] avg loss: 0.525619
[epoch 29, batch    99] avg loss: 0.523364
[epoch 29, batch   199] avg loss: 0.511106
[epoch 29, batch   299] avg loss: 0.500519
[epoch 29, batch   399] avg loss: 0.494866
[epoch 29, batch   499] avg loss: 0.520283
[epoch 29, batch   599] avg loss: 0.517479
[epoch 29, batch   699] avg loss: 0.518078
[epoch 29, batch   799] avg loss: 0.510540
[epoch 29, batch   899] avg loss: 0.501763
[epoch 29, batch   999] avg loss: 0.507854
[epoch 29, batch  1099] avg loss: 0.488186
[epoch 29, batch  1199] avg loss: 0.508120
[epoch 29, batch  1299] avg loss: 0.505020
[epoch 29, batch  1399] avg loss: 0.500317
[epoch 29, batch  1499] avg loss: 0.496910
[epoch 29, batch  1599] avg loss: 0.517831
[epoch 29, batch  1699] avg loss: 0.500395
[epoch 29, batch  1799] avg loss: 0.500812
[epoch 29, batch  1899] avg loss: 0.486053
[epoch 29, batch  1999] avg loss: 0.511732
[epoch 29, batch  2099] avg loss: 0.518596
[epoch 29, batch  2199] avg loss: 0.496797
[epoch 29, batch  2299] avg loss: 0.516578
[epoch 29, batch  2399] avg loss: 0.508060
[epoch 30, batch    99] avg loss: 0.496463
[epoch 30, batch   199] avg loss: 0.492466
[epoch 30, batch   299] avg loss: 0.495980
[epoch 30, batch   399] avg loss: 0.516864
[epoch 30, batch   499] avg loss: 0.510068
[epoch 30, batch   599] avg loss: 0.504803
[epoch 30, batch   699] avg loss: 0.512058
[epoch 30, batch   799] avg loss: 0.503537
[epoch 30, batch   899] avg loss: 0.518591
[epoch 30, batch   999] avg loss: 0.503304
[epoch 30, batch  1099] avg loss: 0.492171
[epoch 30, batch  1199] avg loss: 0.503740
[epoch 30, batch  1299] avg loss: 0.509666
[epoch 30, batch  1399] avg loss: 0.487042
[epoch 30, batch  1499] avg loss: 0.504702
[epoch 30, batch  1599] avg loss: 0.518282
[epoch 30, batch  1699] avg loss: 0.511561
[epoch 30, batch  1799] avg loss: 0.503561
[epoch 30, batch  1899] avg loss: 0.489725
[epoch 30, batch  1999] avg loss: 0.501895
[epoch 30, batch  2099] avg loss: 0.514443
[epoch 30, batch  2199] avg loss: 0.491823
[epoch 30, batch  2299] avg loss: 0.497946
[epoch 30, batch  2399] avg loss: 0.522712
[epoch 31, batch    99] avg loss: 0.507127
[epoch 31, batch   199] avg loss: 0.503017
[epoch 31, batch   299] avg loss: 0.499670
[epoch 31, batch   399] avg loss: 0.496102
[epoch 31, batch   499] avg loss: 0.501567
[epoch 31, batch   599] avg loss: 0.504822
[epoch 31, batch   699] avg loss: 0.506468
[epoch 31, batch   799] avg loss: 0.507682
[epoch 31, batch   899] avg loss: 0.493317
[epoch 31, batch   999] avg loss: 0.493018
[epoch 31, batch  1099] avg loss: 0.494413
[epoch 31, batch  1199] avg loss: 0.498426
[epoch 31, batch  1299] avg loss: 0.503654
[epoch 31, batch  1399] avg loss: 0.507840
[epoch 31, batch  1499] avg loss: 0.501506
[epoch 31, batch  1599] avg loss: 0.501267
[epoch 31, batch  1699] avg loss: 0.501887
[epoch 31, batch  1799] avg loss: 0.507197
[epoch 31, batch  1899] avg loss: 0.494763
[epoch 31, batch  1999] avg loss: 0.498178
[epoch 31, batch  2099] avg loss: 0.495165
[epoch 31, batch  2199] avg loss: 0.516297
[epoch 31, batch  2299] avg loss: 0.507399
[epoch 31, batch  2399] avg loss: 0.474322
[epoch 32, batch    99] avg loss: 0.502286
[epoch 32, batch   199] avg loss: 0.497306
[epoch 32, batch   299] avg loss: 0.503415
[epoch 32, batch   399] avg loss: 0.501581
[epoch 32, batch   499] avg loss: 0.488740
[epoch 32, batch   599] avg loss: 0.511154
[epoch 32, batch   699] avg loss: 0.482200
[epoch 32, batch   799] avg loss: 0.498319
[epoch 32, batch   899] avg loss: 0.489676
[epoch 32, batch   999] avg loss: 0.520704
[epoch 32, batch  1099] avg loss: 0.513786
[epoch 32, batch  1199] avg loss: 0.498569
[epoch 32, batch  1299] avg loss: 0.500583
[epoch 32, batch  1399] avg loss: 0.488977
[epoch 32, batch  1499] avg loss: 0.492156
[epoch 32, batch  1599] avg loss: 0.496545
[epoch 32, batch  1699] avg loss: 0.498854
[epoch 32, batch  1799] avg loss: 0.499648
[epoch 32, batch  1899] avg loss: 0.478493
[epoch 32, batch  1999] avg loss: 0.502127
[epoch 32, batch  2099] avg loss: 0.498415
[epoch 32, batch  2199] avg loss: 0.505903
[epoch 32, batch  2299] avg loss: 0.494948
[epoch 32, batch  2399] avg loss: 0.501146
[epoch 33, batch    99] avg loss: 0.498802
[epoch 33, batch   199] avg loss: 0.511263
[epoch 33, batch   299] avg loss: 0.502019
[epoch 33, batch   399] avg loss: 0.492191
[epoch 33, batch   499] avg loss: 0.509311
[epoch 33, batch   599] avg loss: 0.478473
[epoch 33, batch   699] avg loss: 0.492798
[epoch 33, batch   799] avg loss: 0.489663
[epoch 33, batch   899] avg loss: 0.504140
[epoch 33, batch   999] avg loss: 0.510168
[epoch 33, batch  1099] avg loss: 0.489552
[epoch 33, batch  1199] avg loss: 0.486386
[epoch 33, batch  1299] avg loss: 0.508612
[epoch 33, batch  1399] avg loss: 0.510181
[epoch 33, batch  1499] avg loss: 0.479149
[epoch 33, batch  1599] avg loss: 0.504977
[epoch 33, batch  1699] avg loss: 0.505123
[epoch 33, batch  1799] avg loss: 0.493680
[epoch 33, batch  1899] avg loss: 0.491738
[epoch 33, batch  1999] avg loss: 0.496881
[epoch 33, batch  2099] avg loss: 0.481669
[epoch 33, batch  2199] avg loss: 0.494512
[epoch 33, batch  2299] avg loss: 0.483110
[epoch 33, batch  2399] avg loss: 0.503928
[epoch 34, batch    99] avg loss: 0.509037
[epoch 34, batch   199] avg loss: 0.487114
[epoch 34, batch   299] avg loss: 0.505930
[epoch 34, batch   399] avg loss: 0.485758
[epoch 34, batch   499] avg loss: 0.492105
[epoch 34, batch   599] avg loss: 0.497659
[epoch 34, batch   699] avg loss: 0.493967
[epoch 34, batch   799] avg loss: 0.503754
[epoch 34, batch   899] avg loss: 0.496270
[epoch 34, batch   999] avg loss: 0.491787
[epoch 34, batch  1099] avg loss: 0.488716
[epoch 34, batch  1199] avg loss: 0.492579
[epoch 34, batch  1299] avg loss: 0.506320
[epoch 34, batch  1399] avg loss: 0.494684
[epoch 34, batch  1499] avg loss: 0.494337
[epoch 34, batch  1599] avg loss: 0.484377
[epoch 34, batch  1699] avg loss: 0.498775
[epoch 34, batch  1799] avg loss: 0.489043
[epoch 34, batch  1899] avg loss: 0.495091
[epoch 34, batch  1999] avg loss: 0.517052
[epoch 34, batch  2099] avg loss: 0.479218
[epoch 34, batch  2199] avg loss: 0.493522
[epoch 34, batch  2299] avg loss: 0.491603
[epoch 34, batch  2399] avg loss: 0.494438
[epoch 35, batch    99] avg loss: 0.502694
[epoch 35, batch   199] avg loss: 0.487353
[epoch 35, batch   299] avg loss: 0.499953
[epoch 35, batch   399] avg loss: 0.511333
[epoch 35, batch   499] avg loss: 0.498814
[epoch 35, batch   599] avg loss: 0.486481
[epoch 35, batch   699] avg loss: 0.477294
[epoch 35, batch   799] avg loss: 0.512041
[epoch 35, batch   899] avg loss: 0.493569
[epoch 35, batch   999] avg loss: 0.499052
[epoch 35, batch  1099] avg loss: 0.480234
[epoch 35, batch  1199] avg loss: 0.490316
[epoch 35, batch  1299] avg loss: 0.497636
[epoch 35, batch  1399] avg loss: 0.491272
[epoch 35, batch  1499] avg loss: 0.493570
[epoch 35, batch  1599] avg loss: 0.484976
[epoch 35, batch  1699] avg loss: 0.492313
[epoch 35, batch  1799] avg loss: 0.498467
[epoch 35, batch  1899] avg loss: 0.493734
[epoch 35, batch  1999] avg loss: 0.478878
[epoch 35, batch  2099] avg loss: 0.488535
[epoch 35, batch  2199] avg loss: 0.483760
[epoch 35, batch  2299] avg loss: 0.495692
[epoch 35, batch  2399] avg loss: 0.490730
[epoch 36, batch    99] avg loss: 0.489906
[epoch 36, batch   199] avg loss: 0.486202
[epoch 36, batch   299] avg loss: 0.506517
[epoch 36, batch   399] avg loss: 0.489855
[epoch 36, batch   499] avg loss: 0.495170
[epoch 36, batch   599] avg loss: 0.492847
[epoch 36, batch   699] avg loss: 0.489318
[epoch 36, batch   799] avg loss: 0.493995
[epoch 36, batch   899] avg loss: 0.500952
[epoch 36, batch   999] avg loss: 0.486498
[epoch 36, batch  1099] avg loss: 0.498310
[epoch 36, batch  1199] avg loss: 0.495160
[epoch 36, batch  1299] avg loss: 0.497801
[epoch 36, batch  1399] avg loss: 0.475603
[epoch 36, batch  1499] avg loss: 0.478966
[epoch 36, batch  1599] avg loss: 0.476943
[epoch 36, batch  1699] avg loss: 0.497868
[epoch 36, batch  1799] avg loss: 0.493649
[epoch 36, batch  1899] avg loss: 0.503731
[epoch 36, batch  1999] avg loss: 0.487123
[epoch 36, batch  2099] avg loss: 0.495651
[epoch 36, batch  2199] avg loss: 0.492113
[epoch 36, batch  2299] avg loss: 0.500413
[epoch 36, batch  2399] avg loss: 0.476232
[epoch 37, batch    99] avg loss: 0.490947
[epoch 37, batch   199] avg loss: 0.498409
[epoch 37, batch   299] avg loss: 0.490291
[epoch 37, batch   399] avg loss: 0.481536
[epoch 37, batch   499] avg loss: 0.495791
[epoch 37, batch   599] avg loss: 0.497911
[epoch 37, batch   699] avg loss: 0.497987
[epoch 37, batch   799] avg loss: 0.480372
[epoch 37, batch   899] avg loss: 0.500011
[epoch 37, batch   999] avg loss: 0.475447
[epoch 37, batch  1099] avg loss: 0.483960
[epoch 37, batch  1199] avg loss: 0.488798
[epoch 37, batch  1299] avg loss: 0.497909
[epoch 37, batch  1399] avg loss: 0.495195
[epoch 37, batch  1499] avg loss: 0.476135
[epoch 37, batch  1599] avg loss: 0.496796
[epoch 37, batch  1699] avg loss: 0.480211
[epoch 37, batch  1799] avg loss: 0.491828
[epoch 37, batch  1899] avg loss: 0.489126
[epoch 37, batch  1999] avg loss: 0.496398
[epoch 37, batch  2099] avg loss: 0.481567
[epoch 37, batch  2199] avg loss: 0.489962
[epoch 37, batch  2299] avg loss: 0.488911
[epoch 37, batch  2399] avg loss: 0.500545
[epoch 38, batch    99] avg loss: 0.517310
[epoch 38, batch   199] avg loss: 0.502444
[epoch 38, batch   299] avg loss: 0.489359
[epoch 38, batch   399] avg loss: 0.482407
[epoch 38, batch   499] avg loss: 0.482770
[epoch 38, batch   599] avg loss: 0.482933
[epoch 38, batch   699] avg loss: 0.469109
[epoch 38, batch   799] avg loss: 0.478360
[epoch 38, batch   899] avg loss: 0.483651
[epoch 38, batch   999] avg loss: 0.473070
[epoch 38, batch  1099] avg loss: 0.493303
[epoch 38, batch  1199] avg loss: 0.480921
[epoch 38, batch  1299] avg loss: 0.486237
[epoch 38, batch  1399] avg loss: 0.493803
[epoch 38, batch  1499] avg loss: 0.492589
[epoch 38, batch  1599] avg loss: 0.496749
[epoch 38, batch  1699] avg loss: 0.485314
[epoch 38, batch  1799] avg loss: 0.479359
[epoch 38, batch  1899] avg loss: 0.493231
[epoch 38, batch  1999] avg loss: 0.487399
[epoch 38, batch  2099] avg loss: 0.478690
[epoch 38, batch  2199] avg loss: 0.497126
[epoch 38, batch  2299] avg loss: 0.477090
[epoch 38, batch  2399] avg loss: 0.482205
[epoch 39, batch    99] avg loss: 0.485048
[epoch 39, batch   199] avg loss: 0.482848
[epoch 39, batch   299] avg loss: 0.483826
[epoch 39, batch   399] avg loss: 0.492939
[epoch 39, batch   499] avg loss: 0.499223
[epoch 39, batch   599] avg loss: 0.481101
[epoch 39, batch   699] avg loss: 0.477800
[epoch 39, batch   799] avg loss: 0.506224
[epoch 39, batch   899] avg loss: 0.500690
[epoch 39, batch   999] avg loss: 0.479615
[epoch 39, batch  1099] avg loss: 0.480900
[epoch 39, batch  1199] avg loss: 0.465941
[epoch 39, batch  1299] avg loss: 0.483661
[epoch 39, batch  1399] avg loss: 0.475806
[epoch 39, batch  1499] avg loss: 0.491950
[epoch 39, batch  1599] avg loss: 0.490973
[epoch 39, batch  1699] avg loss: 0.470575
[epoch 39, batch  1799] avg loss: 0.479647
[epoch 39, batch  1899] avg loss: 0.466434
[epoch 39, batch  1999] avg loss: 0.501480
[epoch 39, batch  2099] avg loss: 0.489681
[epoch 39, batch  2199] avg loss: 0.486671
[epoch 39, batch  2299] avg loss: 0.501756
[epoch 39, batch  2399] avg loss: 0.482845
Model saved to model/20200503-004123.pth.
accuracy/TriangPrismIsosc : 0.878
n_examples/TriangPrismIsosc : 500.0
accuracy/parallelepiped : 0.136
n_examples/parallelepiped : 500.0
accuracy/sphere : 1.0
n_examples/sphere : 102.0
accuracy/wire : 0.925
n_examples/wire : 200.0
accuracy/avg_geom : 0.6098310291858678
loss/validation_geom : 0.805294740767706
accuracy/Au : 0.0
n_examples/Au : 0.0
accuracy/SiN : 0.11674347158218126
n_examples/SiN : 1302.0
accuracy/SiO2 : 0.0
n_examples/SiO2 : 0.0
accuracy/avg_mat : 0.11674347158218126
loss/validation_mat : 1.231031102885116
MSE/ShortestDim : 1.9292623458370086
MAE/ShortestDim : 1.0553689391199161
MSE/MiddleDim : 7.17737550046953
MAE/MiddleDim : 1.8356813117289505
MSE/LongDim : 191.1561307423675
MAE/LongDim : 8.677332653977354
MSE/log Area/Vol : 14.051754929502987
MAE/log Area/Vol : 3.5436937065534693
loss/validation_dim : 214.31452351817703
loss/validation : 216.35084936182986
Metrics saved to model/20200503-004123_metrics.csv.
Parsed 2604 rows from data/sim_train_spectrum_SiO2.
Parsed 2604 rows from data/sim_train_labels_SiO2.
Parsed 9765 rows from data/gen_spectrum_SiO2_00-of-16.
Parsed 9765 rows from data/gen_labels_SiO2_00-of-16.
Parsed 9765 rows from data/gen_spectrum_SiO2_01-of-16.
Parsed 9765 rows from data/gen_labels_SiO2_01-of-16.
Parsed 9765 rows from data/gen_spectrum_SiO2_02-of-16.
Parsed 9765 rows from data/gen_labels_SiO2_02-of-16.
Parsed 9765 rows from data/gen_spectrum_SiO2_03-of-16.
Parsed 9765 rows from data/gen_labels_SiO2_03-of-16.
Parsed 9765 rows from data/gen_spectrum_SiO2_04-of-16.
Parsed 9765 rows from data/gen_labels_SiO2_04-of-16.
Parsed 9765 rows from data/gen_spectrum_SiO2_05-of-16.
Parsed 9765 rows from data/gen_labels_SiO2_05-of-16.
Parsed 9765 rows from data/gen_spectrum_SiO2_06-of-16.
Parsed 9765 rows from data/gen_labels_SiO2_06-of-16.
Parsed 9765 rows from data/gen_spectrum_SiO2_07-of-16.
Parsed 9765 rows from data/gen_labels_SiO2_07-of-16.
Parsed 9765 rows from data/gen_spectrum_SiO2_08-of-16.
Parsed 9765 rows from data/gen_labels_SiO2_08-of-16.
Parsed 9765 rows from data/gen_spectrum_SiO2_09-of-16.
Parsed 9765 rows from data/gen_labels_SiO2_09-of-16.
Parsed 9765 rows from data/gen_spectrum_SiO2_10-of-16.
Parsed 9765 rows from data/gen_labels_SiO2_10-of-16.
Parsed 9765 rows from data/gen_spectrum_SiO2_11-of-16.
Parsed 9765 rows from data/gen_labels_SiO2_11-of-16.
Parsed 9765 rows from data/gen_spectrum_SiO2_12-of-16.
Parsed 9765 rows from data/gen_labels_SiO2_12-of-16.
Parsed 9765 rows from data/gen_spectrum_SiO2_13-of-16.
Parsed 9765 rows from data/gen_labels_SiO2_13-of-16.
Parsed 9765 rows from data/gen_spectrum_SiO2_14-of-16.
Parsed 9765 rows from data/gen_labels_SiO2_14-of-16.
Parsed 9765 rows from data/gen_spectrum_SiO2_15-of-16.
Parsed 9765 rows from data/gen_labels_SiO2_15-of-16.
Parsed 1302 rows from data/sim_validation_spectrum_SiO2.
Parsed 1302 rows from data/sim_validation_labels_SiO2.
Logging training progress to tensorboard dir runs/alexnet-SiO2-lr_0.000100-trainsize_158844-05_03_2020_00:42-joint.
[epoch 0, batch    99] avg loss: 1.328658
[epoch 0, batch   199] avg loss: 0.934936
[epoch 0, batch   299] avg loss: 0.854926
[epoch 0, batch   399] avg loss: 0.854164
[epoch 0, batch   499] avg loss: 0.849606
[epoch 0, batch   599] avg loss: 0.838362
[epoch 0, batch   699] avg loss: 0.838761
[epoch 0, batch   799] avg loss: 0.831247
[epoch 0, batch   899] avg loss: 0.803948
[epoch 0, batch   999] avg loss: 0.793573
[epoch 0, batch  1099] avg loss: 0.786163
[epoch 0, batch  1199] avg loss: 0.778839
[epoch 0, batch  1299] avg loss: 0.790351
[epoch 0, batch  1399] avg loss: 0.782319
[epoch 0, batch  1499] avg loss: 0.761655
[epoch 0, batch  1599] avg loss: 0.748072
[epoch 0, batch  1699] avg loss: 0.737259
[epoch 0, batch  1799] avg loss: 0.712887
[epoch 0, batch  1899] avg loss: 0.703267
[epoch 0, batch  1999] avg loss: 0.681884
[epoch 0, batch  2099] avg loss: 0.676705
[epoch 0, batch  2199] avg loss: 0.657769
[epoch 0, batch  2299] avg loss: 0.663639
[epoch 0, batch  2399] avg loss: 0.638483
[epoch 1, batch    99] avg loss: 0.638385
[epoch 1, batch   199] avg loss: 0.621033
[epoch 1, batch   299] avg loss: 0.616902
[epoch 1, batch   399] avg loss: 0.625374
[epoch 1, batch   499] avg loss: 0.622292
[epoch 1, batch   599] avg loss: 0.613022
[epoch 1, batch   699] avg loss: 0.603097
[epoch 1, batch   799] avg loss: 0.592853
[epoch 1, batch   899] avg loss: 0.595003
[epoch 1, batch   999] avg loss: 0.584567
[epoch 1, batch  1099] avg loss: 0.585709
[epoch 1, batch  1199] avg loss: 0.594261
[epoch 1, batch  1299] avg loss: 0.591095
[epoch 1, batch  1399] avg loss: 0.589941
[epoch 1, batch  1499] avg loss: 0.588104
[epoch 1, batch  1599] avg loss: 0.590247
[epoch 1, batch  1699] avg loss: 0.606439
[epoch 1, batch  1799] avg loss: 0.589357
[epoch 1, batch  1899] avg loss: 0.577648
[epoch 1, batch  1999] avg loss: 0.577380
[epoch 1, batch  2099] avg loss: 0.578906
[epoch 1, batch  2199] avg loss: 0.570543
[epoch 1, batch  2299] avg loss: 0.561635
[epoch 1, batch  2399] avg loss: 0.557371
[epoch 2, batch    99] avg loss: 0.562653
[epoch 2, batch   199] avg loss: 0.566453
[epoch 2, batch   299] avg loss: 0.559909
[epoch 2, batch   399] avg loss: 0.563404
[epoch 2, batch   499] avg loss: 0.576980
[epoch 2, batch   599] avg loss: 0.568800
[epoch 2, batch   699] avg loss: 0.561658
[epoch 2, batch   799] avg loss: 0.567815
[epoch 2, batch   899] avg loss: 0.553146
[epoch 2, batch   999] avg loss: 0.566006
[epoch 2, batch  1099] avg loss: 0.552719
[epoch 2, batch  1199] avg loss: 0.559440
[epoch 2, batch  1299] avg loss: 0.574301
[epoch 2, batch  1399] avg loss: 0.554182
[epoch 2, batch  1499] avg loss: 0.559412
[epoch 2, batch  1599] avg loss: 0.562460
[epoch 2, batch  1699] avg loss: 0.551553
[epoch 2, batch  1799] avg loss: 0.560871
[epoch 2, batch  1899] avg loss: 0.559554
[epoch 2, batch  1999] avg loss: 0.545488
[epoch 2, batch  2099] avg loss: 0.556078
[epoch 2, batch  2199] avg loss: 0.539464
[epoch 2, batch  2299] avg loss: 0.555532
[epoch 2, batch  2399] avg loss: 0.546244
[epoch 3, batch    99] avg loss: 0.543360
[epoch 3, batch   199] avg loss: 0.544293
[epoch 3, batch   299] avg loss: 0.552626
[epoch 3, batch   399] avg loss: 0.559513
[epoch 3, batch   499] avg loss: 0.546295
[epoch 3, batch   599] avg loss: 0.543855
[epoch 3, batch   699] avg loss: 0.553122
[epoch 3, batch   799] avg loss: 0.530439
[epoch 3, batch   899] avg loss: 0.536522
[epoch 3, batch   999] avg loss: 0.538233
[epoch 3, batch  1099] avg loss: 0.536806
[epoch 3, batch  1199] avg loss: 0.530589
[epoch 3, batch  1299] avg loss: 0.537184
[epoch 3, batch  1399] avg loss: 0.531877
[epoch 3, batch  1499] avg loss: 0.532498
[epoch 3, batch  1599] avg loss: 0.545162
[epoch 3, batch  1699] avg loss: 0.534440
[epoch 3, batch  1799] avg loss: 0.532736
[epoch 3, batch  1899] avg loss: 0.532700
[epoch 3, batch  1999] avg loss: 0.540542
[epoch 3, batch  2099] avg loss: 0.539720
[epoch 3, batch  2199] avg loss: 0.530004
[epoch 3, batch  2299] avg loss: 0.535360
[epoch 3, batch  2399] avg loss: 0.531632
[epoch 4, batch    99] avg loss: 0.531002
[epoch 4, batch   199] avg loss: 0.528134
[epoch 4, batch   299] avg loss: 0.530438
[epoch 4, batch   399] avg loss: 0.529809
[epoch 4, batch   499] avg loss: 0.526200
[epoch 4, batch   599] avg loss: 0.528050
[epoch 4, batch   699] avg loss: 0.534913
[epoch 4, batch   799] avg loss: 0.531088
[epoch 4, batch   899] avg loss: 0.536028
[epoch 4, batch   999] avg loss: 0.516853
[epoch 4, batch  1099] avg loss: 0.541925
[epoch 4, batch  1199] avg loss: 0.516793
[epoch 4, batch  1299] avg loss: 0.510901
[epoch 4, batch  1399] avg loss: 0.522573
[epoch 4, batch  1499] avg loss: 0.529789
[epoch 4, batch  1599] avg loss: 0.518557
[epoch 4, batch  1699] avg loss: 0.519386
[epoch 4, batch  1799] avg loss: 0.530068
[epoch 4, batch  1899] avg loss: 0.537523
[epoch 4, batch  1999] avg loss: 0.523482
[epoch 4, batch  2099] avg loss: 0.528701
[epoch 4, batch  2199] avg loss: 0.515024
[epoch 4, batch  2299] avg loss: 0.515315
[epoch 4, batch  2399] avg loss: 0.509586
[epoch 5, batch    99] avg loss: 0.519334
[epoch 5, batch   199] avg loss: 0.522057
[epoch 5, batch   299] avg loss: 0.511298
[epoch 5, batch   399] avg loss: 0.512782
[epoch 5, batch   499] avg loss: 0.521926
[epoch 5, batch   599] avg loss: 0.527173
[epoch 5, batch   699] avg loss: 0.519622
[epoch 5, batch   799] avg loss: 0.510368
[epoch 5, batch   899] avg loss: 0.513668
[epoch 5, batch   999] avg loss: 0.521647
[epoch 5, batch  1099] avg loss: 0.521555
[epoch 5, batch  1199] avg loss: 0.518859
[epoch 5, batch  1299] avg loss: 0.512421
[epoch 5, batch  1399] avg loss: 0.517884
[epoch 5, batch  1499] avg loss: 0.509166
[epoch 5, batch  1599] avg loss: 0.513767
[epoch 5, batch  1699] avg loss: 0.507085
[epoch 5, batch  1799] avg loss: 0.518709
[epoch 5, batch  1899] avg loss: 0.505902
[epoch 5, batch  1999] avg loss: 0.502916
[epoch 5, batch  2099] avg loss: 0.510278
[epoch 5, batch  2199] avg loss: 0.507245
[epoch 5, batch  2299] avg loss: 0.511463
[epoch 5, batch  2399] avg loss: 0.514224
[epoch 6, batch    99] avg loss: 0.506588
[epoch 6, batch   199] avg loss: 0.521571
[epoch 6, batch   299] avg loss: 0.511465
[epoch 6, batch   399] avg loss: 0.507501
[epoch 6, batch   499] avg loss: 0.519255
[epoch 6, batch   599] avg loss: 0.502871
[epoch 6, batch   699] avg loss: 0.500938
[epoch 6, batch   799] avg loss: 0.507361
[epoch 6, batch   899] avg loss: 0.506016
[epoch 6, batch   999] avg loss: 0.515710
[epoch 6, batch  1099] avg loss: 0.502842
[epoch 6, batch  1199] avg loss: 0.505415
[epoch 6, batch  1299] avg loss: 0.521915
[epoch 6, batch  1399] avg loss: 0.503938
[epoch 6, batch  1499] avg loss: 0.503574
[epoch 6, batch  1599] avg loss: 0.504692
[epoch 6, batch  1699] avg loss: 0.508960
[epoch 6, batch  1799] avg loss: 0.514914
[epoch 6, batch  1899] avg loss: 0.507621
[epoch 6, batch  1999] avg loss: 0.501012
[epoch 6, batch  2099] avg loss: 0.504618
[epoch 6, batch  2199] avg loss: 0.508209
[epoch 6, batch  2299] avg loss: 0.505012
[epoch 6, batch  2399] avg loss: 0.498841
[epoch 7, batch    99] avg loss: 0.489863
[epoch 7, batch   199] avg loss: 0.499653
[epoch 7, batch   299] avg loss: 0.500289
[epoch 7, batch   399] avg loss: 0.498863
[epoch 7, batch   499] avg loss: 0.559474
[epoch 7, batch   599] avg loss: 0.508117
[epoch 7, batch   699] avg loss: 0.506362
[epoch 7, batch   799] avg loss: 0.496801
[epoch 7, batch   899] avg loss: 0.500058
[epoch 7, batch   999] avg loss: 0.502951
[epoch 7, batch  1099] avg loss: 0.502341
[epoch 7, batch  1199] avg loss: 0.505736
[epoch 7, batch  1299] avg loss: 0.496028
[epoch 7, batch  1399] avg loss: 0.496245
[epoch 7, batch  1499] avg loss: 0.498536
[epoch 7, batch  1599] avg loss: 0.506388
[epoch 7, batch  1699] avg loss: 0.502568
[epoch 7, batch  1799] avg loss: 0.491697
[epoch 7, batch  1899] avg loss: 0.501074
[epoch 7, batch  1999] avg loss: 0.498403
[epoch 7, batch  2099] avg loss: 0.497093
[epoch 7, batch  2199] avg loss: 0.505041
[epoch 7, batch  2299] avg loss: 0.494297
[epoch 7, batch  2399] avg loss: 0.511983
[epoch 8, batch    99] avg loss: 0.488465
[epoch 8, batch   199] avg loss: 0.505050
[epoch 8, batch   299] avg loss: 0.487163
[epoch 8, batch   399] avg loss: 0.496560
[epoch 8, batch   499] avg loss: 0.499685
[epoch 8, batch   599] avg loss: 0.500178
[epoch 8, batch   699] avg loss: 0.496861
[epoch 8, batch   799] avg loss: 0.501304
[epoch 8, batch   899] avg loss: 0.495577
[epoch 8, batch   999] avg loss: 0.498372
[epoch 8, batch  1099] avg loss: 0.497504
[epoch 8, batch  1199] avg loss: 0.499930
[epoch 8, batch  1299] avg loss: 0.484577
[epoch 8, batch  1399] avg loss: 0.494077
[epoch 8, batch  1499] avg loss: 0.490939
[epoch 8, batch  1599] avg loss: 0.502939
[epoch 8, batch  1699] avg loss: 0.519219
[epoch 8, batch  1799] avg loss: 0.501592
[epoch 8, batch  1899] avg loss: 0.495929
[epoch 8, batch  1999] avg loss: 0.508285
[epoch 8, batch  2099] avg loss: 0.488128
[epoch 8, batch  2199] avg loss: 0.482210
[epoch 8, batch  2299] avg loss: 0.495226
[epoch 8, batch  2399] avg loss: 0.496639
[epoch 9, batch    99] avg loss: 0.486810
[epoch 9, batch   199] avg loss: 0.482941
[epoch 9, batch   299] avg loss: 0.495591
[epoch 9, batch   399] avg loss: 0.473749
[epoch 9, batch   499] avg loss: 0.494882
[epoch 9, batch   599] avg loss: 0.481295
[epoch 9, batch   699] avg loss: 0.489083
[epoch 9, batch   799] avg loss: 0.489243
[epoch 9, batch   899] avg loss: 0.513865
[epoch 9, batch   999] avg loss: 0.499470
[epoch 9, batch  1099] avg loss: 0.482337
[epoch 9, batch  1199] avg loss: 0.485167
[epoch 9, batch  1299] avg loss: 0.498197
[epoch 9, batch  1399] avg loss: 0.485718
[epoch 9, batch  1499] avg loss: 0.487659
[epoch 9, batch  1599] avg loss: 0.488467
[epoch 9, batch  1699] avg loss: 0.489950
[epoch 9, batch  1799] avg loss: 0.485581
[epoch 9, batch  1899] avg loss: 0.475327
[epoch 9, batch  1999] avg loss: 0.482890
[epoch 9, batch  2099] avg loss: 0.485807
[epoch 9, batch  2199] avg loss: 0.496703
[epoch 9, batch  2299] avg loss: 0.486324
[epoch 9, batch  2399] avg loss: 0.496903
[epoch 10, batch    99] avg loss: 0.482563
[epoch 10, batch   199] avg loss: 0.475844
[epoch 10, batch   299] avg loss: 0.500202
[epoch 10, batch   399] avg loss: 0.482836
[epoch 10, batch   499] avg loss: 0.492124
[epoch 10, batch   599] avg loss: 0.478442
[epoch 10, batch   699] avg loss: 0.489632
[epoch 10, batch   799] avg loss: 0.482300
[epoch 10, batch   899] avg loss: 0.484566
[epoch 10, batch   999] avg loss: 0.478037
[epoch 10, batch  1099] avg loss: 0.483643
[epoch 10, batch  1199] avg loss: 0.485196
[epoch 10, batch  1299] avg loss: 0.477954
[epoch 10, batch  1399] avg loss: 0.473376
[epoch 10, batch  1499] avg loss: 0.483680
[epoch 10, batch  1599] avg loss: 0.476884
[epoch 10, batch  1699] avg loss: 0.478276
[epoch 10, batch  1799] avg loss: 0.477286
[epoch 10, batch  1899] avg loss: 0.480959
[epoch 10, batch  1999] avg loss: 0.483156
[epoch 10, batch  2099] avg loss: 0.505850
[epoch 10, batch  2199] avg loss: 0.470297
[epoch 10, batch  2299] avg loss: 0.479281
[epoch 10, batch  2399] avg loss: 0.485011
[epoch 11, batch    99] avg loss: 0.490150
[epoch 11, batch   199] avg loss: 0.483098
[epoch 11, batch   299] avg loss: 0.490378
[epoch 11, batch   399] avg loss: 0.473353
[epoch 11, batch   499] avg loss: 0.478513
[epoch 11, batch   599] avg loss: 0.481352
[epoch 11, batch   699] avg loss: 0.475248
[epoch 11, batch   799] avg loss: 0.476637
[epoch 11, batch   899] avg loss: 0.466566
[epoch 11, batch   999] avg loss: 0.476340
[epoch 11, batch  1099] avg loss: 0.475821
[epoch 11, batch  1199] avg loss: 0.480323
[epoch 11, batch  1299] avg loss: 0.482262
[epoch 11, batch  1399] avg loss: 0.496033
[epoch 11, batch  1499] avg loss: 0.479691
[epoch 11, batch  1599] avg loss: 0.482886
[epoch 11, batch  1699] avg loss: 0.467363
[epoch 11, batch  1799] avg loss: 0.473072
[epoch 11, batch  1899] avg loss: 0.469629
[epoch 11, batch  1999] avg loss: 0.476130
[epoch 11, batch  2099] avg loss: 0.483254
[epoch 11, batch  2199] avg loss: 0.476219
[epoch 11, batch  2299] avg loss: 0.461061
[epoch 11, batch  2399] avg loss: 0.476434
[epoch 12, batch    99] avg loss: 0.461095
[epoch 12, batch   199] avg loss: 0.473438
[epoch 12, batch   299] avg loss: 0.490575
[epoch 12, batch   399] avg loss: 0.469914
[epoch 12, batch   499] avg loss: 0.463880
[epoch 12, batch   599] avg loss: 0.474244
[epoch 12, batch   699] avg loss: 0.482016
[epoch 12, batch   799] avg loss: 0.466049
[epoch 12, batch   899] avg loss: 0.467130
[epoch 12, batch   999] avg loss: 0.489829
[epoch 12, batch  1099] avg loss: 0.467713
[epoch 12, batch  1199] avg loss: 0.476562
[epoch 12, batch  1299] avg loss: 0.472742
[epoch 12, batch  1399] avg loss: 0.478027
[epoch 12, batch  1499] avg loss: 0.465908
[epoch 12, batch  1599] avg loss: 0.473754
[epoch 12, batch  1699] avg loss: 0.469179
[epoch 12, batch  1799] avg loss: 0.473199
[epoch 12, batch  1899] avg loss: 0.460695
[epoch 12, batch  1999] avg loss: 0.469585
[epoch 12, batch  2099] avg loss: 0.476985
[epoch 12, batch  2199] avg loss: 0.476536
[epoch 12, batch  2299] avg loss: 0.468744
[epoch 12, batch  2399] avg loss: 0.477275
[epoch 13, batch    99] avg loss: 0.459460
[epoch 13, batch   199] avg loss: 0.471837
[epoch 13, batch   299] avg loss: 0.475056
[epoch 13, batch   399] avg loss: 0.455825
[epoch 13, batch   499] avg loss: 0.472058
[epoch 13, batch   599] avg loss: 0.461483
[epoch 13, batch   699] avg loss: 0.468146
[epoch 13, batch   799] avg loss: 0.460646
[epoch 13, batch   899] avg loss: 0.466990
[epoch 13, batch   999] avg loss: 0.463707
[epoch 13, batch  1099] avg loss: 0.472156
[epoch 13, batch  1199] avg loss: 0.469792
[epoch 13, batch  1299] avg loss: 0.472171
[epoch 13, batch  1399] avg loss: 0.480213
[epoch 13, batch  1499] avg loss: 0.468424
[epoch 13, batch  1599] avg loss: 0.465498
[epoch 13, batch  1699] avg loss: 0.471495
[epoch 13, batch  1799] avg loss: 0.468844
[epoch 13, batch  1899] avg loss: 0.469162
[epoch 13, batch  1999] avg loss: 0.475241
[epoch 13, batch  2099] avg loss: 0.478750
[epoch 13, batch  2199] avg loss: 0.473172
[epoch 13, batch  2299] avg loss: 0.463219
[epoch 13, batch  2399] avg loss: 0.465676
[epoch 14, batch    99] avg loss: 0.468858
[epoch 14, batch   199] avg loss: 0.458974
[epoch 14, batch   299] avg loss: 0.472954
[epoch 14, batch   399] avg loss: 0.460886
[epoch 14, batch   499] avg loss: 0.466906
[epoch 14, batch   599] avg loss: 0.469778
[epoch 14, batch   699] avg loss: 0.468169
[epoch 14, batch   799] avg loss: 0.471776
[epoch 14, batch   899] avg loss: 0.468153
[epoch 14, batch   999] avg loss: 0.451383
[epoch 14, batch  1099] avg loss: 0.464983
[epoch 14, batch  1199] avg loss: 0.457190
[epoch 14, batch  1299] avg loss: 0.452716
[epoch 14, batch  1399] avg loss: 0.468231
[epoch 14, batch  1499] avg loss: 0.458513
[epoch 14, batch  1599] avg loss: 0.469203
[epoch 14, batch  1699] avg loss: 0.475708
[epoch 14, batch  1799] avg loss: 0.458362
[epoch 14, batch  1899] avg loss: 0.465998
[epoch 14, batch  1999] avg loss: 0.465057
[epoch 14, batch  2099] avg loss: 0.465688
[epoch 14, batch  2199] avg loss: 0.462347
[epoch 14, batch  2299] avg loss: 0.468228
[epoch 14, batch  2399] avg loss: 0.465710
[epoch 15, batch    99] avg loss: 0.460393
[epoch 15, batch   199] avg loss: 0.464910
[epoch 15, batch   299] avg loss: 0.455689
[epoch 15, batch   399] avg loss: 0.460475
[epoch 15, batch   499] avg loss: 0.459933
[epoch 15, batch   599] avg loss: 0.473346
[epoch 15, batch   699] avg loss: 0.461186
[epoch 15, batch   799] avg loss: 0.465241
[epoch 15, batch   899] avg loss: 0.459609
[epoch 15, batch   999] avg loss: 0.454316
[epoch 15, batch  1099] avg loss: 0.469433
[epoch 15, batch  1199] avg loss: 0.468735
[epoch 15, batch  1299] avg loss: 0.449288
[epoch 15, batch  1399] avg loss: 0.463953
[epoch 15, batch  1499] avg loss: 0.462994
[epoch 15, batch  1599] avg loss: 0.463832
[epoch 15, batch  1699] avg loss: 0.449580
[epoch 15, batch  1799] avg loss: 0.455742
[epoch 15, batch  1899] avg loss: 0.453550
[epoch 15, batch  1999] avg loss: 0.461188
[epoch 15, batch  2099] avg loss: 0.482653
[epoch 15, batch  2199] avg loss: 0.457545
[epoch 15, batch  2299] avg loss: 0.468143
[epoch 15, batch  2399] avg loss: 0.460338
[epoch 16, batch    99] avg loss: 0.460167
[epoch 16, batch   199] avg loss: 0.461461
[epoch 16, batch   299] avg loss: 0.471888
[epoch 16, batch   399] avg loss: 0.454651
[epoch 16, batch   499] avg loss: 0.464279
[epoch 16, batch   599] avg loss: 0.444496
[epoch 16, batch   699] avg loss: 0.450147
[epoch 16, batch   799] avg loss: 0.464636
[epoch 16, batch   899] avg loss: 0.469337
[epoch 16, batch   999] avg loss: 0.458188
[epoch 16, batch  1099] avg loss: 0.457982
[epoch 16, batch  1199] avg loss: 0.446736
[epoch 16, batch  1299] avg loss: 0.457104
[epoch 16, batch  1399] avg loss: 0.446595
[epoch 16, batch  1499] avg loss: 0.456027
[epoch 16, batch  1599] avg loss: 0.451919
[epoch 16, batch  1699] avg loss: 0.457504
[epoch 16, batch  1799] avg loss: 0.450896
[epoch 16, batch  1899] avg loss: 0.459061
[epoch 16, batch  1999] avg loss: 0.469205
[epoch 16, batch  2099] avg loss: 0.456337
[epoch 16, batch  2199] avg loss: 0.456099
[epoch 16, batch  2299] avg loss: 0.450532
[epoch 16, batch  2399] avg loss: 0.464490
[epoch 17, batch    99] avg loss: 0.460106
[epoch 17, batch   199] avg loss: 0.444347
[epoch 17, batch   299] avg loss: 0.453029
[epoch 17, batch   399] avg loss: 0.462641
[epoch 17, batch   499] avg loss: 0.442436
[epoch 17, batch   599] avg loss: 0.454903
[epoch 17, batch   699] avg loss: 0.457872
[epoch 17, batch   799] avg loss: 0.451360
[epoch 17, batch   899] avg loss: 0.441020
[epoch 17, batch   999] avg loss: 0.463819
[epoch 17, batch  1099] avg loss: 0.458181
[epoch 17, batch  1199] avg loss: 0.451732
[epoch 17, batch  1299] avg loss: 0.452889
[epoch 17, batch  1399] avg loss: 0.455673
[epoch 17, batch  1499] avg loss: 0.449988
[epoch 17, batch  1599] avg loss: 0.451342
[epoch 17, batch  1699] avg loss: 0.451463
[epoch 17, batch  1799] avg loss: 0.466474
[epoch 17, batch  1899] avg loss: 0.445756
[epoch 17, batch  1999] avg loss: 0.462182
[epoch 17, batch  2099] avg loss: 0.461721
[epoch 17, batch  2199] avg loss: 0.471663
[epoch 17, batch  2299] avg loss: 0.451619
[epoch 17, batch  2399] avg loss: 0.454506
[epoch 18, batch    99] avg loss: 0.442475
[epoch 18, batch   199] avg loss: 0.455268
[epoch 18, batch   299] avg loss: 0.458972
[epoch 18, batch   399] avg loss: 0.446066
[epoch 18, batch   499] avg loss: 0.463655
[epoch 18, batch   599] avg loss: 0.461110
[epoch 18, batch   699] avg loss: 0.452792
[epoch 18, batch   799] avg loss: 0.460404
[epoch 18, batch   899] avg loss: 0.445423
[epoch 18, batch   999] avg loss: 0.458365
[epoch 18, batch  1099] avg loss: 0.442033
[epoch 18, batch  1199] avg loss: 0.441863
[epoch 18, batch  1299] avg loss: 0.448004
[epoch 18, batch  1399] avg loss: 0.464648
[epoch 18, batch  1499] avg loss: 0.441389
[epoch 18, batch  1599] avg loss: 0.443613
[epoch 18, batch  1699] avg loss: 0.446282
[epoch 18, batch  1799] avg loss: 0.444137
[epoch 18, batch  1899] avg loss: 0.448695
[epoch 18, batch  1999] avg loss: 0.452493
[epoch 18, batch  2099] avg loss: 0.441659
[epoch 18, batch  2199] avg loss: 0.444317
[epoch 18, batch  2299] avg loss: 0.443248
[epoch 18, batch  2399] avg loss: 0.439831
[epoch 19, batch    99] avg loss: 0.465632
[epoch 19, batch   199] avg loss: 0.464564
[epoch 19, batch   299] avg loss: 0.446475
[epoch 19, batch   399] avg loss: 0.445914
[epoch 19, batch   499] avg loss: 0.450840
[epoch 19, batch   599] avg loss: 0.440692
[epoch 19, batch   699] avg loss: 0.451451
[epoch 19, batch   799] avg loss: 0.444449
[epoch 19, batch   899] avg loss: 0.450162
[epoch 19, batch   999] avg loss: 0.441673
[epoch 19, batch  1099] avg loss: 0.445446
[epoch 19, batch  1199] avg loss: 0.452688
[epoch 19, batch  1299] avg loss: 0.436357
[epoch 19, batch  1399] avg loss: 0.449649
[epoch 19, batch  1499] avg loss: 0.432752
[epoch 19, batch  1599] avg loss: 0.437525
[epoch 19, batch  1699] avg loss: 0.458160
[epoch 19, batch  1799] avg loss: 0.444446
[epoch 19, batch  1899] avg loss: 0.454587
[epoch 19, batch  1999] avg loss: 0.443221
[epoch 19, batch  2099] avg loss: 0.434074
[epoch 19, batch  2199] avg loss: 0.440712
[epoch 19, batch  2299] avg loss: 0.443287
[epoch 19, batch  2399] avg loss: 0.454401
[epoch 20, batch    99] avg loss: 0.431621
[epoch 20, batch   199] avg loss: 0.451173
[epoch 20, batch   299] avg loss: 0.442629
[epoch 20, batch   399] avg loss: 0.441621
[epoch 20, batch   499] avg loss: 0.449773
[epoch 20, batch   599] avg loss: 0.450583
[epoch 20, batch   699] avg loss: 0.455135
[epoch 20, batch   799] avg loss: 0.438734
[epoch 20, batch   899] avg loss: 0.446462
[epoch 20, batch   999] avg loss: 0.442544
[epoch 20, batch  1099] avg loss: 0.449182
[epoch 20, batch  1199] avg loss: 0.437876
[epoch 20, batch  1299] avg loss: 0.438843
[epoch 20, batch  1399] avg loss: 0.443131
[epoch 20, batch  1499] avg loss: 0.438608
[epoch 20, batch  1599] avg loss: 0.434665
[epoch 20, batch  1699] avg loss: 0.460591
[epoch 20, batch  1799] avg loss: 0.442333
[epoch 20, batch  1899] avg loss: 0.449105
[epoch 20, batch  1999] avg loss: 0.432371
[epoch 20, batch  2099] avg loss: 0.445759
[epoch 20, batch  2199] avg loss: 0.451861
[epoch 20, batch  2299] avg loss: 0.448892
[epoch 20, batch  2399] avg loss: 0.442919
[epoch 21, batch    99] avg loss: 0.438782
[epoch 21, batch   199] avg loss: 0.441580
[epoch 21, batch   299] avg loss: 0.442180
[epoch 21, batch   399] avg loss: 0.446380
[epoch 21, batch   499] avg loss: 0.442654
[epoch 21, batch   599] avg loss: 0.443233
[epoch 21, batch   699] avg loss: 0.431809
[epoch 21, batch   799] avg loss: 0.440349
[epoch 21, batch   899] avg loss: 0.433439
[epoch 21, batch   999] avg loss: 0.442587
[epoch 21, batch  1099] avg loss: 0.434347
[epoch 21, batch  1199] avg loss: 0.436785
[epoch 21, batch  1299] avg loss: 0.447547
[epoch 21, batch  1399] avg loss: 0.438379
[epoch 21, batch  1499] avg loss: 0.449489
[epoch 21, batch  1599] avg loss: 0.424993
[epoch 21, batch  1699] avg loss: 0.437813
[epoch 21, batch  1799] avg loss: 0.446757
[epoch 21, batch  1899] avg loss: 0.444831
[epoch 21, batch  1999] avg loss: 0.433618
[epoch 21, batch  2099] avg loss: 0.447916
[epoch 21, batch  2199] avg loss: 0.435506
[epoch 21, batch  2299] avg loss: 0.432105
[epoch 21, batch  2399] avg loss: 0.447323
[epoch 22, batch    99] avg loss: 0.430219
[epoch 22, batch   199] avg loss: 0.449175
[epoch 22, batch   299] avg loss: 0.434625
[epoch 22, batch   399] avg loss: 0.437924
[epoch 22, batch   499] avg loss: 0.429866
[epoch 22, batch   599] avg loss: 0.435487
[epoch 22, batch   699] avg loss: 0.439318
[epoch 22, batch   799] avg loss: 0.441601
[epoch 22, batch   899] avg loss: 0.443388
[epoch 22, batch   999] avg loss: 0.429932
[epoch 22, batch  1099] avg loss: 0.446629
[epoch 22, batch  1199] avg loss: 0.432388
[epoch 22, batch  1299] avg loss: 0.450512
[epoch 22, batch  1399] avg loss: 0.433384
[epoch 22, batch  1499] avg loss: 0.433485
[epoch 22, batch  1599] avg loss: 0.433958
[epoch 22, batch  1699] avg loss: 0.441294
[epoch 22, batch  1799] avg loss: 0.427826
[epoch 22, batch  1899] avg loss: 0.444276
[epoch 22, batch  1999] avg loss: 0.434252
[epoch 22, batch  2099] avg loss: 0.434042
[epoch 22, batch  2199] avg loss: 0.414350
[epoch 22, batch  2299] avg loss: 0.436383
[epoch 22, batch  2399] avg loss: 0.426059
[epoch 23, batch    99] avg loss: 0.427537
[epoch 23, batch   199] avg loss: 0.434802
[epoch 23, batch   299] avg loss: 0.446982
[epoch 23, batch   399] avg loss: 0.421399
[epoch 23, batch   499] avg loss: 0.435038
[epoch 23, batch   599] avg loss: 0.443358
[epoch 23, batch   699] avg loss: 0.435134
[epoch 23, batch   799] avg loss: 0.434733
[epoch 23, batch   899] avg loss: 0.423193
[epoch 23, batch   999] avg loss: 0.429167
[epoch 23, batch  1099] avg loss: 0.432145
[epoch 23, batch  1199] avg loss: 0.429618
[epoch 23, batch  1299] avg loss: 0.422558
[epoch 23, batch  1399] avg loss: 0.423980
[epoch 23, batch  1499] avg loss: 0.421404
[epoch 23, batch  1599] avg loss: 0.436893
[epoch 23, batch  1699] avg loss: 0.437395
[epoch 23, batch  1799] avg loss: 0.437899
[epoch 23, batch  1899] avg loss: 0.424732
[epoch 23, batch  1999] avg loss: 0.438097
[epoch 23, batch  2099] avg loss: 0.435020
[epoch 23, batch  2199] avg loss: 0.435004
[epoch 23, batch  2299] avg loss: 0.437383
[epoch 23, batch  2399] avg loss: 0.427584
[epoch 24, batch    99] avg loss: 0.429315
[epoch 24, batch   199] avg loss: 0.426879
[epoch 24, batch   299] avg loss: 0.419214
[epoch 24, batch   399] avg loss: 0.415959
[epoch 24, batch   499] avg loss: 0.425243
[epoch 24, batch   599] avg loss: 0.426894
[epoch 24, batch   699] avg loss: 0.447063
[epoch 24, batch   799] avg loss: 0.444958
[epoch 24, batch   899] avg loss: 0.433978
[epoch 24, batch   999] avg loss: 0.419174
[epoch 24, batch  1099] avg loss: 0.435538
[epoch 24, batch  1199] avg loss: 0.434224
[epoch 24, batch  1299] avg loss: 0.427656
[epoch 24, batch  1399] avg loss: 0.436328
[epoch 24, batch  1499] avg loss: 0.445087
[epoch 24, batch  1599] avg loss: 0.422285
[epoch 24, batch  1699] avg loss: 0.425057
[epoch 24, batch  1799] avg loss: 0.429755
[epoch 24, batch  1899] avg loss: 0.437934
[epoch 24, batch  1999] avg loss: 0.426943
[epoch 24, batch  2099] avg loss: 0.435098
[epoch 24, batch  2199] avg loss: 0.427062
[epoch 24, batch  2299] avg loss: 0.419381
[epoch 24, batch  2399] avg loss: 0.424874
[epoch 25, batch    99] avg loss: 0.427042
[epoch 25, batch   199] avg loss: 0.427630
[epoch 25, batch   299] avg loss: 0.431850
[epoch 25, batch   399] avg loss: 0.419919
[epoch 25, batch   499] avg loss: 0.424670
[epoch 25, batch   599] avg loss: 0.422973
[epoch 25, batch   699] avg loss: 0.419315
[epoch 25, batch   799] avg loss: 0.425964
[epoch 25, batch   899] avg loss: 0.412775
[epoch 25, batch   999] avg loss: 0.433024
[epoch 25, batch  1099] avg loss: 0.417042
[epoch 25, batch  1199] avg loss: 0.437197
[epoch 25, batch  1299] avg loss: 0.427226
[epoch 25, batch  1399] avg loss: 0.424553
[epoch 25, batch  1499] avg loss: 0.430683
[epoch 25, batch  1599] avg loss: 0.421935
[epoch 25, batch  1699] avg loss: 0.427435
[epoch 25, batch  1799] avg loss: 0.426922
[epoch 25, batch  1899] avg loss: 0.426697
[epoch 25, batch  1999] avg loss: 0.438861
[epoch 25, batch  2099] avg loss: 0.431561
[epoch 25, batch  2199] avg loss: 0.438757
[epoch 25, batch  2299] avg loss: 0.421509
[epoch 25, batch  2399] avg loss: 0.424789
[epoch 26, batch    99] avg loss: 0.418379
[epoch 26, batch   199] avg loss: 0.419237
[epoch 26, batch   299] avg loss: 0.417010
[epoch 26, batch   399] avg loss: 0.421332
[epoch 26, batch   499] avg loss: 0.431927
[epoch 26, batch   599] avg loss: 0.423387
[epoch 26, batch   699] avg loss: 0.422627
[epoch 26, batch   799] avg loss: 0.420710
[epoch 26, batch   899] avg loss: 0.426565
[epoch 26, batch   999] avg loss: 0.435669
[epoch 26, batch  1099] avg loss: 0.418070
[epoch 26, batch  1199] avg loss: 0.420943
[epoch 26, batch  1299] avg loss: 0.428069
[epoch 26, batch  1399] avg loss: 0.428055
[epoch 26, batch  1499] avg loss: 0.420213
[epoch 26, batch  1599] avg loss: 0.429269
[epoch 26, batch  1699] avg loss: 0.422597
[epoch 26, batch  1799] avg loss: 0.421677
[epoch 26, batch  1899] avg loss: 0.420810
[epoch 26, batch  1999] avg loss: 0.433163
[epoch 26, batch  2099] avg loss: 0.431993
[epoch 26, batch  2199] avg loss: 0.413262
[epoch 26, batch  2299] avg loss: 0.427239
[epoch 26, batch  2399] avg loss: 0.429604
[epoch 27, batch    99] avg loss: 0.423335
[epoch 27, batch   199] avg loss: 0.411933
[epoch 27, batch   299] avg loss: 0.419905
[epoch 27, batch   399] avg loss: 0.421266
[epoch 27, batch   499] avg loss: 0.427445
[epoch 27, batch   599] avg loss: 0.425063
[epoch 27, batch   699] avg loss: 0.431045
[epoch 27, batch   799] avg loss: 0.420632
[epoch 27, batch   899] avg loss: 0.430207
[epoch 27, batch   999] avg loss: 0.410800
[epoch 27, batch  1099] avg loss: 0.421231
[epoch 27, batch  1199] avg loss: 0.409626
[epoch 27, batch  1299] avg loss: 0.420352
[epoch 27, batch  1399] avg loss: 0.423244
[epoch 27, batch  1499] avg loss: 0.408239
[epoch 27, batch  1599] avg loss: 0.417819
[epoch 27, batch  1699] avg loss: 0.408409
[epoch 27, batch  1799] avg loss: 0.424310
[epoch 27, batch  1899] avg loss: 0.425896
[epoch 27, batch  1999] avg loss: 0.426106
[epoch 27, batch  2099] avg loss: 0.431368
[epoch 27, batch  2199] avg loss: 0.414484
[epoch 27, batch  2299] avg loss: 0.421608
[epoch 27, batch  2399] avg loss: 0.418720
[epoch 28, batch    99] avg loss: 0.442671
[epoch 28, batch   199] avg loss: 0.421404
[epoch 28, batch   299] avg loss: 0.428063
[epoch 28, batch   399] avg loss: 0.411695
[epoch 28, batch   499] avg loss: 0.409581
[epoch 28, batch   599] avg loss: 0.415574
[epoch 28, batch   699] avg loss: 0.423749
[epoch 28, batch   799] avg loss: 0.422769
[epoch 28, batch   899] avg loss: 0.409525
[epoch 28, batch   999] avg loss: 0.416454
[epoch 28, batch  1099] avg loss: 0.413120
[epoch 28, batch  1199] avg loss: 0.420218
[epoch 28, batch  1299] avg loss: 0.415820
[epoch 28, batch  1399] avg loss: 0.412803
[epoch 28, batch  1499] avg loss: 0.429526
[epoch 28, batch  1599] avg loss: 0.418928
[epoch 28, batch  1699] avg loss: 0.413719
[epoch 28, batch  1799] avg loss: 0.411551
[epoch 28, batch  1899] avg loss: 0.426108
[epoch 28, batch  1999] avg loss: 0.416558
[epoch 28, batch  2099] avg loss: 0.423636
[epoch 28, batch  2199] avg loss: 0.416841
[epoch 28, batch  2299] avg loss: 0.417508
[epoch 28, batch  2399] avg loss: 0.422237
[epoch 29, batch    99] avg loss: 0.422340
[epoch 29, batch   199] avg loss: 0.411887
[epoch 29, batch   299] avg loss: 0.415575
[epoch 29, batch   399] avg loss: 0.421737
[epoch 29, batch   499] avg loss: 0.407639
[epoch 29, batch   599] avg loss: 0.407431
[epoch 29, batch   699] avg loss: 0.427834
[epoch 29, batch   799] avg loss: 0.417489
[epoch 29, batch   899] avg loss: 0.410808
[epoch 29, batch   999] avg loss: 0.403441
[epoch 29, batch  1099] avg loss: 0.425195
[epoch 29, batch  1199] avg loss: 0.415386
[epoch 29, batch  1299] avg loss: 0.412715
[epoch 29, batch  1399] avg loss: 0.422488
[epoch 29, batch  1499] avg loss: 0.409085
[epoch 29, batch  1599] avg loss: 0.399992
[epoch 29, batch  1699] avg loss: 0.423567
[epoch 29, batch  1799] avg loss: 0.417715
[epoch 29, batch  1899] avg loss: 0.421531
[epoch 29, batch  1999] avg loss: 0.419741
[epoch 29, batch  2099] avg loss: 0.417410
[epoch 29, batch  2199] avg loss: 0.401768
[epoch 29, batch  2299] avg loss: 0.405887
[epoch 29, batch  2399] avg loss: 0.412233
[epoch 30, batch    99] avg loss: 0.426704
[epoch 30, batch   199] avg loss: 0.413964
[epoch 30, batch   299] avg loss: 0.420870
[epoch 30, batch   399] avg loss: 0.416146
[epoch 30, batch   499] avg loss: 0.414076
[epoch 30, batch   599] avg loss: 0.401390
[epoch 30, batch   699] avg loss: 0.411541
[epoch 30, batch   799] avg loss: 0.419492
[epoch 30, batch   899] avg loss: 0.418324
[epoch 30, batch   999] avg loss: 0.414877
[epoch 30, batch  1099] avg loss: 0.397693
[epoch 30, batch  1199] avg loss: 0.418077
[epoch 30, batch  1299] avg loss: 0.416516
[epoch 30, batch  1399] avg loss: 0.408662
[epoch 30, batch  1499] avg loss: 0.404022
[epoch 30, batch  1599] avg loss: 0.421517
[epoch 30, batch  1699] avg loss: 0.415076
[epoch 30, batch  1799] avg loss: 0.419526
[epoch 30, batch  1899] avg loss: 0.412811
[epoch 30, batch  1999] avg loss: 0.408848
[epoch 30, batch  2099] avg loss: 0.402148
[epoch 30, batch  2199] avg loss: 0.409026
[epoch 30, batch  2299] avg loss: 0.405630
[epoch 30, batch  2399] avg loss: 0.407794
[epoch 31, batch    99] avg loss: 0.408401
[epoch 31, batch   199] avg loss: 0.413823
[epoch 31, batch   299] avg loss: 0.413796
[epoch 31, batch   399] avg loss: 0.406913
[epoch 31, batch   499] avg loss: 0.413771
[epoch 31, batch   599] avg loss: 0.410585
[epoch 31, batch   699] avg loss: 0.420614
[epoch 31, batch   799] avg loss: 0.423464
[epoch 31, batch   899] avg loss: 0.406993
[epoch 31, batch   999] avg loss: 0.413145
[epoch 31, batch  1099] avg loss: 0.404446
[epoch 31, batch  1199] avg loss: 0.413140
[epoch 31, batch  1299] avg loss: 0.412235
[epoch 31, batch  1399] avg loss: 0.393344
[epoch 31, batch  1499] avg loss: 0.404558
[epoch 31, batch  1599] avg loss: 0.406258
[epoch 31, batch  1699] avg loss: 0.412100
[epoch 31, batch  1799] avg loss: 0.408001
[epoch 31, batch  1899] avg loss: 0.401964
[epoch 31, batch  1999] avg loss: 0.404494
[epoch 31, batch  2099] avg loss: 0.417655
[epoch 31, batch  2199] avg loss: 0.398355
[epoch 31, batch  2299] avg loss: 0.406638
[epoch 31, batch  2399] avg loss: 0.405426
[epoch 32, batch    99] avg loss: 0.412823
[epoch 32, batch   199] avg loss: 0.406460
[epoch 32, batch   299] avg loss: 0.404204
[epoch 32, batch   399] avg loss: 0.410914
[epoch 32, batch   499] avg loss: 0.402772
[epoch 32, batch   599] avg loss: 0.414293
[epoch 32, batch   699] avg loss: 0.421685
[epoch 32, batch   799] avg loss: 0.407474
[epoch 32, batch   899] avg loss: 0.411011
[epoch 32, batch   999] avg loss: 0.404927
[epoch 32, batch  1099] avg loss: 0.413804
[epoch 32, batch  1199] avg loss: 0.409715
[epoch 32, batch  1299] avg loss: 0.412663
[epoch 32, batch  1399] avg loss: 0.411219
[epoch 32, batch  1499] avg loss: 0.406459
[epoch 32, batch  1599] avg loss: 0.400192
[epoch 32, batch  1699] avg loss: 0.422176
[epoch 32, batch  1799] avg loss: 0.398650
[epoch 32, batch  1899] avg loss: 0.395570
[epoch 32, batch  1999] avg loss: 0.413836
[epoch 32, batch  2099] avg loss: 0.400265
[epoch 32, batch  2199] avg loss: 0.404783
[epoch 32, batch  2299] avg loss: 0.396753
[epoch 32, batch  2399] avg loss: 0.408484
[epoch 33, batch    99] avg loss: 0.417016
[epoch 33, batch   199] avg loss: 0.386782
[epoch 33, batch   299] avg loss: 0.403891
[epoch 33, batch   399] avg loss: 0.405511
[epoch 33, batch   499] avg loss: 0.394490
[epoch 33, batch   599] avg loss: 0.412860
[epoch 33, batch   699] avg loss: 0.411746
[epoch 33, batch   799] avg loss: 0.403969
[epoch 33, batch   899] avg loss: 0.400208
[epoch 33, batch   999] avg loss: 0.414960
[epoch 33, batch  1099] avg loss: 0.407603
[epoch 33, batch  1199] avg loss: 0.396291
[epoch 33, batch  1299] avg loss: 0.389648
[epoch 33, batch  1399] avg loss: 0.405050
[epoch 33, batch  1499] avg loss: 0.409060
[epoch 33, batch  1599] avg loss: 0.401563
[epoch 33, batch  1699] avg loss: 0.423586
[epoch 33, batch  1799] avg loss: 0.398354
[epoch 33, batch  1899] avg loss: 0.427067
[epoch 33, batch  1999] avg loss: 0.406657
[epoch 33, batch  2099] avg loss: 0.395863
[epoch 33, batch  2199] avg loss: 0.404649
[epoch 33, batch  2299] avg loss: 0.415350
[epoch 33, batch  2399] avg loss: 0.407226
[epoch 34, batch    99] avg loss: 0.398780
[epoch 34, batch   199] avg loss: 0.412208
[epoch 34, batch   299] avg loss: 0.401137
[epoch 34, batch   399] avg loss: 0.395956
[epoch 34, batch   499] avg loss: 0.404789
[epoch 34, batch   599] avg loss: 0.403270
[epoch 34, batch   699] avg loss: 0.404967
[epoch 34, batch   799] avg loss: 0.399542
[epoch 34, batch   899] avg loss: 0.407100
[epoch 34, batch   999] avg loss: 0.403836
[epoch 34, batch  1099] avg loss: 0.395073
[epoch 34, batch  1199] avg loss: 0.400262
[epoch 34, batch  1299] avg loss: 0.395387
[epoch 34, batch  1399] avg loss: 0.396160
[epoch 34, batch  1499] avg loss: 0.407330
[epoch 34, batch  1599] avg loss: 0.397643
[epoch 34, batch  1699] avg loss: 0.401349
[epoch 34, batch  1799] avg loss: 0.402799
[epoch 34, batch  1899] avg loss: 0.404451
[epoch 34, batch  1999] avg loss: 0.407083
[epoch 34, batch  2099] avg loss: 0.418887
[epoch 34, batch  2199] avg loss: 0.387677
[epoch 34, batch  2299] avg loss: 0.392559
[epoch 34, batch  2399] avg loss: 0.404711
[epoch 35, batch    99] avg loss: 0.397965
[epoch 35, batch   199] avg loss: 0.419603
[epoch 35, batch   299] avg loss: 0.400378
[epoch 35, batch   399] avg loss: 0.399281
[epoch 35, batch   499] avg loss: 0.400446
[epoch 35, batch   599] avg loss: 0.406551
[epoch 35, batch   699] avg loss: 0.394786
[epoch 35, batch   799] avg loss: 0.408123
[epoch 35, batch   899] avg loss: 0.405091
[epoch 35, batch   999] avg loss: 0.399050
[epoch 35, batch  1099] avg loss: 0.395690
[epoch 35, batch  1199] avg loss: 0.400597
[epoch 35, batch  1299] avg loss: 0.402151
[epoch 35, batch  1399] avg loss: 0.401236
[epoch 35, batch  1499] avg loss: 0.395914
[epoch 35, batch  1599] avg loss: 0.405120
[epoch 35, batch  1699] avg loss: 0.387304
[epoch 35, batch  1799] avg loss: 0.395636
[epoch 35, batch  1899] avg loss: 0.396000
[epoch 35, batch  1999] avg loss: 0.391193
[epoch 35, batch  2099] avg loss: 0.394426
[epoch 35, batch  2199] avg loss: 0.405667
[epoch 35, batch  2299] avg loss: 0.411430
[epoch 35, batch  2399] avg loss: 0.399748
[epoch 36, batch    99] avg loss: 0.405631
[epoch 36, batch   199] avg loss: 0.409181
[epoch 36, batch   299] avg loss: 0.397590
[epoch 36, batch   399] avg loss: 0.393648
[epoch 36, batch   499] avg loss: 0.407505
[epoch 36, batch   599] avg loss: 0.388635
[epoch 36, batch   699] avg loss: 0.393222
[epoch 36, batch   799] avg loss: 0.388491
[epoch 36, batch   899] avg loss: 0.388804
[epoch 36, batch   999] avg loss: 0.402622
[epoch 36, batch  1099] avg loss: 0.391404
[epoch 36, batch  1199] avg loss: 0.410374
[epoch 36, batch  1299] avg loss: 0.395921
[epoch 36, batch  1399] avg loss: 0.405805
[epoch 36, batch  1499] avg loss: 0.402551
[epoch 36, batch  1599] avg loss: 0.406676
[epoch 36, batch  1699] avg loss: 0.396971
[epoch 36, batch  1799] avg loss: 0.389875
[epoch 36, batch  1899] avg loss: 0.402077
[epoch 36, batch  1999] avg loss: 0.392908
[epoch 36, batch  2099] avg loss: 0.381910
[epoch 36, batch  2199] avg loss: 0.398685
[epoch 36, batch  2299] avg loss: 0.407600
[epoch 36, batch  2399] avg loss: 0.390125
[epoch 37, batch    99] avg loss: 0.405029
[epoch 37, batch   199] avg loss: 0.394558
[epoch 37, batch   299] avg loss: 0.398390
[epoch 37, batch   399] avg loss: 0.380316
[epoch 37, batch   499] avg loss: 0.402441
[epoch 37, batch   599] avg loss: 0.394390
[epoch 37, batch   699] avg loss: 0.401857
[epoch 37, batch   799] avg loss: 0.386570
[epoch 37, batch   899] avg loss: 0.411213
[epoch 37, batch   999] avg loss: 0.389478
[epoch 37, batch  1099] avg loss: 0.398044
[epoch 37, batch  1199] avg loss: 0.415932
[epoch 37, batch  1299] avg loss: 0.397211
[epoch 37, batch  1399] avg loss: 0.395040
[epoch 37, batch  1499] avg loss: 0.385860
[epoch 37, batch  1599] avg loss: 0.387055
[epoch 37, batch  1699] avg loss: 0.395334
[epoch 37, batch  1799] avg loss: 0.398027
[epoch 37, batch  1899] avg loss: 0.389200
[epoch 37, batch  1999] avg loss: 0.393043
[epoch 37, batch  2099] avg loss: 0.388103
[epoch 37, batch  2199] avg loss: 0.399706
[epoch 37, batch  2299] avg loss: 0.386193
[epoch 37, batch  2399] avg loss: 0.398845
[epoch 38, batch    99] avg loss: 0.397144
[epoch 38, batch   199] avg loss: 0.399110
[epoch 38, batch   299] avg loss: 0.392156
[epoch 38, batch   399] avg loss: 0.402102
[epoch 38, batch   499] avg loss: 0.404571
[epoch 38, batch   599] avg loss: 0.388117
[epoch 38, batch   699] avg loss: 0.382651
[epoch 38, batch   799] avg loss: 0.388783
[epoch 38, batch   899] avg loss: 0.400669
[epoch 38, batch   999] avg loss: 0.387529
[epoch 38, batch  1099] avg loss: 0.394485
[epoch 38, batch  1199] avg loss: 0.393429
[epoch 38, batch  1299] avg loss: 0.393841
[epoch 38, batch  1399] avg loss: 0.391551
[epoch 38, batch  1499] avg loss: 0.408913
[epoch 38, batch  1599] avg loss: 0.404598
[epoch 38, batch  1699] avg loss: 0.377415
[epoch 38, batch  1799] avg loss: 0.391844
[epoch 38, batch  1899] avg loss: 0.388064
[epoch 38, batch  1999] avg loss: 0.401599
[epoch 38, batch  2099] avg loss: 0.390247
[epoch 38, batch  2199] avg loss: 0.387268
[epoch 38, batch  2299] avg loss: 0.399528
[epoch 38, batch  2399] avg loss: 0.392854
[epoch 39, batch    99] avg loss: 0.385066
[epoch 39, batch   199] avg loss: 0.404019
[epoch 39, batch   299] avg loss: 0.396678
[epoch 39, batch   399] avg loss: 0.391329
[epoch 39, batch   499] avg loss: 0.406054
[epoch 39, batch   599] avg loss: 0.379233
[epoch 39, batch   699] avg loss: 0.385158
[epoch 39, batch   799] avg loss: 0.393661
[epoch 39, batch   899] avg loss: 0.393559
[epoch 39, batch   999] avg loss: 0.390898
[epoch 39, batch  1099] avg loss: 0.386267
[epoch 39, batch  1199] avg loss: 0.388680
[epoch 39, batch  1299] avg loss: 0.394636
[epoch 39, batch  1399] avg loss: 0.407209
[epoch 39, batch  1499] avg loss: 0.391803
[epoch 39, batch  1599] avg loss: 0.396676
[epoch 39, batch  1699] avg loss: 0.371469
[epoch 39, batch  1799] avg loss: 0.402772
[epoch 39, batch  1899] avg loss: 0.385438
[epoch 39, batch  1999] avg loss: 0.393745
[epoch 39, batch  2099] avg loss: 0.384399
[epoch 39, batch  2199] avg loss: 0.395223
[epoch 39, batch  2299] avg loss: 0.394776
[epoch 39, batch  2399] avg loss: 0.383926
Model saved to model/20200503-011524.pth.
accuracy/TriangPrismIsosc : 0.804
n_examples/TriangPrismIsosc : 500.0
accuracy/parallelepiped : 0.44
n_examples/parallelepiped : 500.0
accuracy/sphere : 1.0
n_examples/sphere : 102.0
accuracy/wire : 0.965
n_examples/wire : 200.0
accuracy/avg_geom : 0.7043010752688172
loss/validation_geom : 0.6852364767165411
accuracy/Au : 0.0
n_examples/Au : 0.0
accuracy/SiN : 0.0
n_examples/SiN : 0.0
accuracy/SiO2 : 0.05222734254992319
n_examples/SiO2 : 1302.0
accuracy/avg_mat : 0.05222734254992319
loss/validation_mat : 1.3937018346127277
MSE/ShortestDim : 2.264042732719269
MAE/ShortestDim : 0.9522010888189032
MSE/MiddleDim : 7.706639906594647
MAE/MiddleDim : 1.8813640576903172
MSE/LongDim : 134.06536565215174
MAE/LongDim : 6.766882033575149
MSE/log Area/Vol : 7.675437282673591
MAE/log Area/Vol : 2.5198343016218665
loss/validation_dim : 151.71148557413923
loss/validation : 153.7904238854685
Metrics saved to model/20200503-011524_metrics.csv.
Parsed 2604 rows from data/sim_train_spectrum_SiO2.
Parsed 2604 rows from data/sim_train_labels_SiO2.
Parsed 9765 rows from data/gen_spectrum_SiO2_00-of-16.
Parsed 9765 rows from data/gen_labels_SiO2_00-of-16.
Parsed 9765 rows from data/gen_spectrum_SiO2_01-of-16.
Parsed 9765 rows from data/gen_labels_SiO2_01-of-16.
Parsed 9765 rows from data/gen_spectrum_SiO2_02-of-16.
Parsed 9765 rows from data/gen_labels_SiO2_02-of-16.
Parsed 9765 rows from data/gen_spectrum_SiO2_03-of-16.
Parsed 9765 rows from data/gen_labels_SiO2_03-of-16.
Parsed 9765 rows from data/gen_spectrum_SiO2_04-of-16.
Parsed 9765 rows from data/gen_labels_SiO2_04-of-16.
Parsed 9765 rows from data/gen_spectrum_SiO2_05-of-16.
Parsed 9765 rows from data/gen_labels_SiO2_05-of-16.
Parsed 9765 rows from data/gen_spectrum_SiO2_06-of-16.
Parsed 9765 rows from data/gen_labels_SiO2_06-of-16.
Parsed 9765 rows from data/gen_spectrum_SiO2_07-of-16.
Parsed 9765 rows from data/gen_labels_SiO2_07-of-16.
Parsed 9765 rows from data/gen_spectrum_SiO2_08-of-16.
Parsed 9765 rows from data/gen_labels_SiO2_08-of-16.
Parsed 9765 rows from data/gen_spectrum_SiO2_09-of-16.
Parsed 9765 rows from data/gen_labels_SiO2_09-of-16.
Parsed 9765 rows from data/gen_spectrum_SiO2_10-of-16.
Parsed 9765 rows from data/gen_labels_SiO2_10-of-16.
Parsed 9765 rows from data/gen_spectrum_SiO2_11-of-16.
Parsed 9765 rows from data/gen_labels_SiO2_11-of-16.
Parsed 9765 rows from data/gen_spectrum_SiO2_12-of-16.
Parsed 9765 rows from data/gen_labels_SiO2_12-of-16.
Parsed 9765 rows from data/gen_spectrum_SiO2_13-of-16.
Parsed 9765 rows from data/gen_labels_SiO2_13-of-16.
Parsed 9765 rows from data/gen_spectrum_SiO2_14-of-16.
Parsed 9765 rows from data/gen_labels_SiO2_14-of-16.
Parsed 9765 rows from data/gen_spectrum_SiO2_15-of-16.
Parsed 9765 rows from data/gen_labels_SiO2_15-of-16.
Parsed 1302 rows from data/sim_validation_spectrum_SiO2.
Parsed 1302 rows from data/sim_validation_labels_SiO2.
Logging training progress to tensorboard dir runs/alexnet-SiO2-lr_0.000010-trainsize_158844-05_03_2020_01:16-joint.
[epoch 0, batch    99] avg loss: 1.386285
[epoch 0, batch   199] avg loss: 1.385089
[epoch 0, batch   299] avg loss: 1.382124
[epoch 0, batch   399] avg loss: 1.375710
[epoch 0, batch   499] avg loss: 1.360448
[epoch 0, batch   599] avg loss: 1.322693
[epoch 0, batch   699] avg loss: 1.233171
[epoch 0, batch   799] avg loss: 1.082534
[epoch 0, batch   899] avg loss: 0.987699
[epoch 0, batch   999] avg loss: 0.938394
[epoch 0, batch  1099] avg loss: 0.903779
[epoch 0, batch  1199] avg loss: 0.898192
[epoch 0, batch  1299] avg loss: 0.881627
[epoch 0, batch  1399] avg loss: 0.874788
[epoch 0, batch  1499] avg loss: 0.872729
[epoch 0, batch  1599] avg loss: 0.869996
[epoch 0, batch  1699] avg loss: 0.864926
[epoch 0, batch  1799] avg loss: 0.862362
[epoch 0, batch  1899] avg loss: 0.866222
[epoch 0, batch  1999] avg loss: 0.849699
[epoch 0, batch  2099] avg loss: 0.856854
[epoch 0, batch  2199] avg loss: 0.864824
[epoch 0, batch  2299] avg loss: 0.832336
[epoch 0, batch  2399] avg loss: 0.850382
[epoch 1, batch    99] avg loss: 0.829457
[epoch 1, batch   199] avg loss: 0.845891
[epoch 1, batch   299] avg loss: 0.838878
[epoch 1, batch   399] avg loss: 0.840294
[epoch 1, batch   499] avg loss: 0.860192
[epoch 1, batch   599] avg loss: 0.836192
[epoch 1, batch   699] avg loss: 0.845984
[epoch 1, batch   799] avg loss: 0.859226
[epoch 1, batch   899] avg loss: 0.847077
[epoch 1, batch   999] avg loss: 0.836909
[epoch 1, batch  1099] avg loss: 0.845521
[epoch 1, batch  1199] avg loss: 0.840720
[epoch 1, batch  1299] avg loss: 0.842690
[epoch 1, batch  1399] avg loss: 0.829808
[epoch 1, batch  1499] avg loss: 0.839462
[epoch 1, batch  1599] avg loss: 0.836052
[epoch 1, batch  1699] avg loss: 0.834368
[epoch 1, batch  1799] avg loss: 0.834244
[epoch 1, batch  1899] avg loss: 0.832528
[epoch 1, batch  1999] avg loss: 0.832966
[epoch 1, batch  2099] avg loss: 0.844818
[epoch 1, batch  2199] avg loss: 0.840574
[epoch 1, batch  2299] avg loss: 0.832410
[epoch 1, batch  2399] avg loss: 0.840828
[epoch 2, batch    99] avg loss: 0.832264
[epoch 2, batch   199] avg loss: 0.832431
[epoch 2, batch   299] avg loss: 0.832738
[epoch 2, batch   399] avg loss: 0.836247
[epoch 2, batch   499] avg loss: 0.834372
[epoch 2, batch   599] avg loss: 0.834014
[epoch 2, batch   699] avg loss: 0.825863
[epoch 2, batch   799] avg loss: 0.834530
[epoch 2, batch   899] avg loss: 0.839434
[epoch 2, batch   999] avg loss: 0.830039
[epoch 2, batch  1099] avg loss: 0.833696
[epoch 2, batch  1199] avg loss: 0.835715
[epoch 2, batch  1299] avg loss: 0.823984
[epoch 2, batch  1399] avg loss: 0.814877
[epoch 2, batch  1499] avg loss: 0.844519
[epoch 2, batch  1599] avg loss: 0.822361
[epoch 2, batch  1699] avg loss: 0.830340
[epoch 2, batch  1799] avg loss: 0.817310
[epoch 2, batch  1899] avg loss: 0.819424
[epoch 2, batch  1999] avg loss: 0.824372
[epoch 2, batch  2099] avg loss: 0.804116
[epoch 2, batch  2199] avg loss: 0.817020
[epoch 2, batch  2299] avg loss: 0.816042
[epoch 2, batch  2399] avg loss: 0.810545
[epoch 3, batch    99] avg loss: 0.808909
[epoch 3, batch   199] avg loss: 0.817535
[epoch 3, batch   299] avg loss: 0.816414
[epoch 3, batch   399] avg loss: 0.807557
[epoch 3, batch   499] avg loss: 0.796647
[epoch 3, batch   599] avg loss: 0.797284
[epoch 3, batch   699] avg loss: 0.804769
[epoch 3, batch   799] avg loss: 0.803843
[epoch 3, batch   899] avg loss: 0.802221
[epoch 3, batch   999] avg loss: 0.793204
[epoch 3, batch  1099] avg loss: 0.791938
[epoch 3, batch  1199] avg loss: 0.802547
[epoch 3, batch  1299] avg loss: 0.803503
[epoch 3, batch  1399] avg loss: 0.799783
[epoch 3, batch  1499] avg loss: 0.791781
[epoch 3, batch  1599] avg loss: 0.804867
[epoch 3, batch  1699] avg loss: 0.797938
[epoch 3, batch  1799] avg loss: 0.789810
[epoch 3, batch  1899] avg loss: 0.798777
[epoch 3, batch  1999] avg loss: 0.789231
[epoch 3, batch  2099] avg loss: 0.804681
[epoch 3, batch  2199] avg loss: 0.794481
[epoch 3, batch  2299] avg loss: 0.789089
[epoch 3, batch  2399] avg loss: 0.789085
[epoch 4, batch    99] avg loss: 0.792332
[epoch 4, batch   199] avg loss: 0.792776
[epoch 4, batch   299] avg loss: 0.794593
[epoch 4, batch   399] avg loss: 0.788029
[epoch 4, batch   499] avg loss: 0.790344
[epoch 4, batch   599] avg loss: 0.789305
[epoch 4, batch   699] avg loss: 0.786086
[epoch 4, batch   799] avg loss: 0.785474
[epoch 4, batch   899] avg loss: 0.781043
[epoch 4, batch   999] avg loss: 0.792801
[epoch 4, batch  1099] avg loss: 0.779847
[epoch 4, batch  1199] avg loss: 0.791626
[epoch 4, batch  1299] avg loss: 0.786891
[epoch 4, batch  1399] avg loss: 0.794309
[epoch 4, batch  1499] avg loss: 0.778002
[epoch 4, batch  1599] avg loss: 0.781724
[epoch 4, batch  1699] avg loss: 0.790737
[epoch 4, batch  1799] avg loss: 0.782276
[epoch 4, batch  1899] avg loss: 0.772556
[epoch 4, batch  1999] avg loss: 0.780052
[epoch 4, batch  2099] avg loss: 0.785980
[epoch 4, batch  2199] avg loss: 0.779635
[epoch 4, batch  2299] avg loss: 0.776175
[epoch 4, batch  2399] avg loss: 0.784893
[epoch 5, batch    99] avg loss: 0.780738
[epoch 5, batch   199] avg loss: 0.788240
[epoch 5, batch   299] avg loss: 0.778161
[epoch 5, batch   399] avg loss: 0.779473
[epoch 5, batch   499] avg loss: 0.767172
[epoch 5, batch   599] avg loss: 0.780891
[epoch 5, batch   699] avg loss: 0.783166
[epoch 5, batch   799] avg loss: 0.768347
[epoch 5, batch   899] avg loss: 0.768161
[epoch 5, batch   999] avg loss: 0.773544
[epoch 5, batch  1099] avg loss: 0.776485
[epoch 5, batch  1199] avg loss: 0.782201
[epoch 5, batch  1299] avg loss: 0.772282
[epoch 5, batch  1399] avg loss: 0.769212
[epoch 5, batch  1499] avg loss: 0.775167
[epoch 5, batch  1599] avg loss: 0.765970
[epoch 5, batch  1699] avg loss: 0.761694
[epoch 5, batch  1799] avg loss: 0.751407
[epoch 5, batch  1899] avg loss: 0.761272
[epoch 5, batch  1999] avg loss: 0.752587
[epoch 5, batch  2099] avg loss: 0.763968
[epoch 5, batch  2199] avg loss: 0.761737
[epoch 5, batch  2299] avg loss: 0.756406
[epoch 5, batch  2399] avg loss: 0.756467
[epoch 6, batch    99] avg loss: 0.752158
[epoch 6, batch   199] avg loss: 0.773767
[epoch 6, batch   299] avg loss: 0.750572
[epoch 6, batch   399] avg loss: 0.767196
[epoch 6, batch   499] avg loss: 0.766948
[epoch 6, batch   599] avg loss: 0.753551
[epoch 6, batch   699] avg loss: 0.756562
[epoch 6, batch   799] avg loss: 0.744848
[epoch 6, batch   899] avg loss: 0.759929
[epoch 6, batch   999] avg loss: 0.746709
[epoch 6, batch  1099] avg loss: 0.755537
[epoch 6, batch  1199] avg loss: 0.758725
[epoch 6, batch  1299] avg loss: 0.761930
[epoch 6, batch  1399] avg loss: 0.753679
[epoch 6, batch  1499] avg loss: 0.741036
[epoch 6, batch  1599] avg loss: 0.745651
[epoch 6, batch  1699] avg loss: 0.740496
[epoch 6, batch  1799] avg loss: 0.742661
[epoch 6, batch  1899] avg loss: 0.738387
[epoch 6, batch  1999] avg loss: 0.747197
[epoch 6, batch  2099] avg loss: 0.728554
[epoch 6, batch  2199] avg loss: 0.740026
[epoch 6, batch  2299] avg loss: 0.736807
[epoch 6, batch  2399] avg loss: 0.740271
[epoch 7, batch    99] avg loss: 0.723756
[epoch 7, batch   199] avg loss: 0.735080
[epoch 7, batch   299] avg loss: 0.740374
[epoch 7, batch   399] avg loss: 0.747428
[epoch 7, batch   499] avg loss: 0.733435
[epoch 7, batch   599] avg loss: 0.726242
[epoch 7, batch   699] avg loss: 0.733576
[epoch 7, batch   799] avg loss: 0.731283
[epoch 7, batch   899] avg loss: 0.737014
[epoch 7, batch   999] avg loss: 0.735643
[epoch 7, batch  1099] avg loss: 0.733360
[epoch 7, batch  1199] avg loss: 0.729014
[epoch 7, batch  1299] avg loss: 0.731770
[epoch 7, batch  1399] avg loss: 0.708993
[epoch 7, batch  1499] avg loss: 0.724132
[epoch 7, batch  1599] avg loss: 0.726797
[epoch 7, batch  1699] avg loss: 0.714086
[epoch 7, batch  1799] avg loss: 0.699373
[epoch 7, batch  1899] avg loss: 0.711567
[epoch 7, batch  1999] avg loss: 0.710088
[epoch 7, batch  2099] avg loss: 0.709134
[epoch 7, batch  2199] avg loss: 0.711787
[epoch 7, batch  2299] avg loss: 0.711285
[epoch 7, batch  2399] avg loss: 0.706850
[epoch 8, batch    99] avg loss: 0.712367
[epoch 8, batch   199] avg loss: 0.711118
[epoch 8, batch   299] avg loss: 0.702508
[epoch 8, batch   399] avg loss: 0.692176
[epoch 8, batch   499] avg loss: 0.710085
[epoch 8, batch   599] avg loss: 0.688332
[epoch 8, batch   699] avg loss: 0.690211
[epoch 8, batch   799] avg loss: 0.706331
[epoch 8, batch   899] avg loss: 0.697824
[epoch 8, batch   999] avg loss: 0.700036
[epoch 8, batch  1099] avg loss: 0.697554
[epoch 8, batch  1199] avg loss: 0.704725
[epoch 8, batch  1299] avg loss: 0.697869
[epoch 8, batch  1399] avg loss: 0.689978
[epoch 8, batch  1499] avg loss: 0.696456
[epoch 8, batch  1599] avg loss: 0.702377
[epoch 8, batch  1699] avg loss: 0.688086
[epoch 8, batch  1799] avg loss: 0.702643
[epoch 8, batch  1899] avg loss: 0.698499
[epoch 8, batch  1999] avg loss: 0.686237
[epoch 8, batch  2099] avg loss: 0.684581
[epoch 8, batch  2199] avg loss: 0.680990
[epoch 8, batch  2299] avg loss: 0.681264
[epoch 8, batch  2399] avg loss: 0.682489
[epoch 9, batch    99] avg loss: 0.687195
[epoch 9, batch   199] avg loss: 0.676384
[epoch 9, batch   299] avg loss: 0.682857
[epoch 9, batch   399] avg loss: 0.678449
[epoch 9, batch   499] avg loss: 0.676968
[epoch 9, batch   599] avg loss: 0.674845
[epoch 9, batch   699] avg loss: 0.674938
[epoch 9, batch   799] avg loss: 0.670856
[epoch 9, batch   899] avg loss: 0.681088
[epoch 9, batch   999] avg loss: 0.668232
[epoch 9, batch  1099] avg loss: 0.668712
[epoch 9, batch  1199] avg loss: 0.683968
[epoch 9, batch  1299] avg loss: 0.673322
[epoch 9, batch  1399] avg loss: 0.675337
[epoch 9, batch  1499] avg loss: 0.664798
[epoch 9, batch  1599] avg loss: 0.667581
[epoch 9, batch  1699] avg loss: 0.666879
[epoch 9, batch  1799] avg loss: 0.656907
[epoch 9, batch  1899] avg loss: 0.663372
[epoch 9, batch  1999] avg loss: 0.668947
[epoch 9, batch  2099] avg loss: 0.659900
[epoch 9, batch  2199] avg loss: 0.683651
[epoch 9, batch  2299] avg loss: 0.659500
[epoch 9, batch  2399] avg loss: 0.665405
[epoch 10, batch    99] avg loss: 0.664757
[epoch 10, batch   199] avg loss: 0.657176
[epoch 10, batch   299] avg loss: 0.671439
[epoch 10, batch   399] avg loss: 0.673648
[epoch 10, batch   499] avg loss: 0.679386
[epoch 10, batch   599] avg loss: 0.666148
[epoch 10, batch   699] avg loss: 0.637704
[epoch 10, batch   799] avg loss: 0.658037
[epoch 10, batch   899] avg loss: 0.657434
[epoch 10, batch   999] avg loss: 0.649355
[epoch 10, batch  1099] avg loss: 0.654737
[epoch 10, batch  1199] avg loss: 0.651292
[epoch 10, batch  1299] avg loss: 0.648843
[epoch 10, batch  1399] avg loss: 0.650468
[epoch 10, batch  1499] avg loss: 0.643289
[epoch 10, batch  1599] avg loss: 0.651945
[epoch 10, batch  1699] avg loss: 0.645310
[epoch 10, batch  1799] avg loss: 0.639269
[epoch 10, batch  1899] avg loss: 0.656199
[epoch 10, batch  1999] avg loss: 0.644632
[epoch 10, batch  2099] avg loss: 0.647057
[epoch 10, batch  2199] avg loss: 0.647310
[epoch 10, batch  2299] avg loss: 0.645133
[epoch 10, batch  2399] avg loss: 0.642511
[epoch 11, batch    99] avg loss: 0.639335
[epoch 11, batch   199] avg loss: 0.650482
[epoch 11, batch   299] avg loss: 0.643002
[epoch 11, batch   399] avg loss: 0.637101
[epoch 11, batch   499] avg loss: 0.638378
[epoch 11, batch   599] avg loss: 0.639908
[epoch 11, batch   699] avg loss: 0.639918
[epoch 11, batch   799] avg loss: 0.643906
[epoch 11, batch   899] avg loss: 0.634063
[epoch 11, batch   999] avg loss: 0.635481
[epoch 11, batch  1099] avg loss: 0.641361
[epoch 11, batch  1199] avg loss: 0.630019
[epoch 11, batch  1299] avg loss: 0.642537
[epoch 11, batch  1399] avg loss: 0.636986
[epoch 11, batch  1499] avg loss: 0.644472
[epoch 11, batch  1599] avg loss: 0.630877
[epoch 11, batch  1699] avg loss: 0.635461
[epoch 11, batch  1799] avg loss: 0.626422
[epoch 11, batch  1899] avg loss: 0.616967
[epoch 11, batch  1999] avg loss: 0.634127
[epoch 11, batch  2099] avg loss: 0.627620
[epoch 11, batch  2199] avg loss: 0.636207
[epoch 11, batch  2299] avg loss: 0.627813
[epoch 11, batch  2399] avg loss: 0.637729
[epoch 12, batch    99] avg loss: 0.635808
[epoch 12, batch   199] avg loss: 0.624016
[epoch 12, batch   299] avg loss: 0.623822
[epoch 12, batch   399] avg loss: 0.615822
[epoch 12, batch   499] avg loss: 0.620240
[epoch 12, batch   599] avg loss: 0.627628
[epoch 12, batch   699] avg loss: 0.619365
[epoch 12, batch   799] avg loss: 0.616979
[epoch 12, batch   899] avg loss: 0.620501
[epoch 12, batch   999] avg loss: 0.631275
[epoch 12, batch  1099] avg loss: 0.621997
[epoch 12, batch  1199] avg loss: 0.623960
[epoch 12, batch  1299] avg loss: 0.610151
[epoch 12, batch  1399] avg loss: 0.609963
[epoch 12, batch  1499] avg loss: 0.619442
[epoch 12, batch  1599] avg loss: 0.614493
[epoch 12, batch  1699] avg loss: 0.619621
[epoch 12, batch  1799] avg loss: 0.617468
[epoch 12, batch  1899] avg loss: 0.627286
[epoch 12, batch  1999] avg loss: 0.614587
[epoch 12, batch  2099] avg loss: 0.614994
[epoch 12, batch  2199] avg loss: 0.617566
[epoch 12, batch  2299] avg loss: 0.619126
[epoch 12, batch  2399] avg loss: 0.610455
[epoch 13, batch    99] avg loss: 0.623645
[epoch 13, batch   199] avg loss: 0.604215
[epoch 13, batch   299] avg loss: 0.618954
[epoch 13, batch   399] avg loss: 0.619675
[epoch 13, batch   499] avg loss: 0.603913
[epoch 13, batch   599] avg loss: 0.617013
[epoch 13, batch   699] avg loss: 0.619176
[epoch 13, batch   799] avg loss: 0.608865
[epoch 13, batch   899] avg loss: 0.604810
[epoch 13, batch   999] avg loss: 0.611650
[epoch 13, batch  1099] avg loss: 0.619278
[epoch 13, batch  1199] avg loss: 0.605261
[epoch 13, batch  1299] avg loss: 0.608042
[epoch 13, batch  1399] avg loss: 0.593635
[epoch 13, batch  1499] avg loss: 0.618936
[epoch 13, batch  1599] avg loss: 0.603466
[epoch 13, batch  1699] avg loss: 0.594694
[epoch 13, batch  1799] avg loss: 0.595464
[epoch 13, batch  1899] avg loss: 0.603827
[epoch 13, batch  1999] avg loss: 0.605093
[epoch 13, batch  2099] avg loss: 0.597260
[epoch 13, batch  2199] avg loss: 0.600295
[epoch 13, batch  2299] avg loss: 0.586887
[epoch 13, batch  2399] avg loss: 0.606038
[epoch 14, batch    99] avg loss: 0.588274
[epoch 14, batch   199] avg loss: 0.600939
[epoch 14, batch   299] avg loss: 0.600091
[epoch 14, batch   399] avg loss: 0.619536
[epoch 14, batch   499] avg loss: 0.594492
[epoch 14, batch   599] avg loss: 0.586245
[epoch 14, batch   699] avg loss: 0.590615
[epoch 14, batch   799] avg loss: 0.593446
[epoch 14, batch   899] avg loss: 0.597291
[epoch 14, batch   999] avg loss: 0.607196
[epoch 14, batch  1099] avg loss: 0.608555
[epoch 14, batch  1199] avg loss: 0.594977
[epoch 14, batch  1299] avg loss: 0.598499
[epoch 14, batch  1399] avg loss: 0.596982
[epoch 14, batch  1499] avg loss: 0.597093
[epoch 14, batch  1599] avg loss: 0.594238
[epoch 14, batch  1699] avg loss: 0.590448
[epoch 14, batch  1799] avg loss: 0.588332
[epoch 14, batch  1899] avg loss: 0.592612
[epoch 14, batch  1999] avg loss: 0.600121
[epoch 14, batch  2099] avg loss: 0.600223
[epoch 14, batch  2199] avg loss: 0.590341
[epoch 14, batch  2299] avg loss: 0.590175
[epoch 14, batch  2399] avg loss: 0.599075
[epoch 15, batch    99] avg loss: 0.595689
[epoch 15, batch   199] avg loss: 0.586042
[epoch 15, batch   299] avg loss: 0.591501
[epoch 15, batch   399] avg loss: 0.600676
[epoch 15, batch   499] avg loss: 0.582365
[epoch 15, batch   599] avg loss: 0.593691
[epoch 15, batch   699] avg loss: 0.594925
[epoch 15, batch   799] avg loss: 0.579780
[epoch 15, batch   899] avg loss: 0.588581
[epoch 15, batch   999] avg loss: 0.594577
[epoch 15, batch  1099] avg loss: 0.589932
[epoch 15, batch  1199] avg loss: 0.597219
[epoch 15, batch  1299] avg loss: 0.578910
[epoch 15, batch  1399] avg loss: 0.584866
[epoch 15, batch  1499] avg loss: 0.597881
[epoch 15, batch  1599] avg loss: 0.584731
[epoch 15, batch  1699] avg loss: 0.588388
[epoch 15, batch  1799] avg loss: 0.591396
[epoch 15, batch  1899] avg loss: 0.580748
[epoch 15, batch  1999] avg loss: 0.592705
[epoch 15, batch  2099] avg loss: 0.581389
[epoch 15, batch  2199] avg loss: 0.590132
[epoch 15, batch  2299] avg loss: 0.580895
[epoch 15, batch  2399] avg loss: 0.584309
[epoch 16, batch    99] avg loss: 0.593447
[epoch 16, batch   199] avg loss: 0.578764
[epoch 16, batch   299] avg loss: 0.598499
[epoch 16, batch   399] avg loss: 0.589440
[epoch 16, batch   499] avg loss: 0.585616
[epoch 16, batch   599] avg loss: 0.584751
[epoch 16, batch   699] avg loss: 0.582734
[epoch 16, batch   799] avg loss: 0.583890
[epoch 16, batch   899] avg loss: 0.587306
[epoch 16, batch   999] avg loss: 0.583309
[epoch 16, batch  1099] avg loss: 0.579564
[epoch 16, batch  1199] avg loss: 0.582629
[epoch 16, batch  1299] avg loss: 0.576318
[epoch 16, batch  1399] avg loss: 0.584883
[epoch 16, batch  1499] avg loss: 0.597897
[epoch 16, batch  1599] avg loss: 0.592015
[epoch 16, batch  1699] avg loss: 0.585021
[epoch 16, batch  1799] avg loss: 0.580877
[epoch 16, batch  1899] avg loss: 0.568384
[epoch 16, batch  1999] avg loss: 0.578879
[epoch 16, batch  2099] avg loss: 0.584713
[epoch 16, batch  2199] avg loss: 0.584151
[epoch 16, batch  2299] avg loss: 0.587957
[epoch 16, batch  2399] avg loss: 0.577652
[epoch 17, batch    99] avg loss: 0.583367
[epoch 17, batch   199] avg loss: 0.575529
[epoch 17, batch   299] avg loss: 0.585128
[epoch 17, batch   399] avg loss: 0.582819
[epoch 17, batch   499] avg loss: 0.585865
[epoch 17, batch   599] avg loss: 0.575333
[epoch 17, batch   699] avg loss: 0.585400
[epoch 17, batch   799] avg loss: 0.587167
[epoch 17, batch   899] avg loss: 0.586297
[epoch 17, batch   999] avg loss: 0.573108
[epoch 17, batch  1099] avg loss: 0.568395
[epoch 17, batch  1199] avg loss: 0.574033
[epoch 17, batch  1299] avg loss: 0.590983
[epoch 17, batch  1399] avg loss: 0.583768
[epoch 17, batch  1499] avg loss: 0.563533
[epoch 17, batch  1599] avg loss: 0.588237
[epoch 17, batch  1699] avg loss: 0.565223
[epoch 17, batch  1799] avg loss: 0.581039
[epoch 17, batch  1899] avg loss: 0.572054
[epoch 17, batch  1999] avg loss: 0.575980
[epoch 17, batch  2099] avg loss: 0.560568
[epoch 17, batch  2199] avg loss: 0.575967
[epoch 17, batch  2299] avg loss: 0.580816
[epoch 17, batch  2399] avg loss: 0.574918
[epoch 18, batch    99] avg loss: 0.561106
[epoch 18, batch   199] avg loss: 0.567560
[epoch 18, batch   299] avg loss: 0.584809
[epoch 18, batch   399] avg loss: 0.572991
[epoch 18, batch   499] avg loss: 0.570733
[epoch 18, batch   599] avg loss: 0.571687
[epoch 18, batch   699] avg loss: 0.569060
[epoch 18, batch   799] avg loss: 0.584927
[epoch 18, batch   899] avg loss: 0.579103
[epoch 18, batch   999] avg loss: 0.584671
[epoch 18, batch  1099] avg loss: 0.578580
[epoch 18, batch  1199] avg loss: 0.583604
[epoch 18, batch  1299] avg loss: 0.573521
[epoch 18, batch  1399] avg loss: 0.573586
[epoch 18, batch  1499] avg loss: 0.574741
[epoch 18, batch  1599] avg loss: 0.569822
[epoch 18, batch  1699] avg loss: 0.564912
[epoch 18, batch  1799] avg loss: 0.561751
[epoch 18, batch  1899] avg loss: 0.584628
[epoch 18, batch  1999] avg loss: 0.567456
[epoch 18, batch  2099] avg loss: 0.577808
[epoch 18, batch  2199] avg loss: 0.563191
[epoch 18, batch  2299] avg loss: 0.564184
[epoch 18, batch  2399] avg loss: 0.569732
[epoch 19, batch    99] avg loss: 0.578644
[epoch 19, batch   199] avg loss: 0.568921
[epoch 19, batch   299] avg loss: 0.582799
[epoch 19, batch   399] avg loss: 0.569897
[epoch 19, batch   499] avg loss: 0.572159
[epoch 19, batch   599] avg loss: 0.562683
[epoch 19, batch   699] avg loss: 0.572906
[epoch 19, batch   799] avg loss: 0.566746
[epoch 19, batch   899] avg loss: 0.563734
[epoch 19, batch   999] avg loss: 0.564980
[epoch 19, batch  1099] avg loss: 0.571140
[epoch 19, batch  1199] avg loss: 0.568622
[epoch 19, batch  1299] avg loss: 0.561224
[epoch 19, batch  1399] avg loss: 0.577447
[epoch 19, batch  1499] avg loss: 0.557700
[epoch 19, batch  1599] avg loss: 0.571070
[epoch 19, batch  1699] avg loss: 0.566683
[epoch 19, batch  1799] avg loss: 0.577712
[epoch 19, batch  1899] avg loss: 0.564461
[epoch 19, batch  1999] avg loss: 0.565437
[epoch 19, batch  2099] avg loss: 0.572054
[epoch 19, batch  2199] avg loss: 0.561951
[epoch 19, batch  2299] avg loss: 0.567527
[epoch 19, batch  2399] avg loss: 0.574917
[epoch 20, batch    99] avg loss: 0.582828
[epoch 20, batch   199] avg loss: 0.575644
[epoch 20, batch   299] avg loss: 0.572876
[epoch 20, batch   399] avg loss: 0.569928
[epoch 20, batch   499] avg loss: 0.569774
[epoch 20, batch   599] avg loss: 0.550487
[epoch 20, batch   699] avg loss: 0.564623
[epoch 20, batch   799] avg loss: 0.560992
[epoch 20, batch   899] avg loss: 0.561453
[epoch 20, batch   999] avg loss: 0.573605
[epoch 20, batch  1099] avg loss: 0.564284
[epoch 20, batch  1199] avg loss: 0.557628
[epoch 20, batch  1299] avg loss: 0.558089
[epoch 20, batch  1399] avg loss: 0.564563
[epoch 20, batch  1499] avg loss: 0.569547
[epoch 20, batch  1599] avg loss: 0.552761
[epoch 20, batch  1699] avg loss: 0.560232
[epoch 20, batch  1799] avg loss: 0.573207
[epoch 20, batch  1899] avg loss: 0.574397
[epoch 20, batch  1999] avg loss: 0.560336
[epoch 20, batch  2099] avg loss: 0.562443
[epoch 20, batch  2199] avg loss: 0.555992
[epoch 20, batch  2299] avg loss: 0.559044
[epoch 20, batch  2399] avg loss: 0.576959
[epoch 21, batch    99] avg loss: 0.562676
[epoch 21, batch   199] avg loss: 0.571282
[epoch 21, batch   299] avg loss: 0.563531
[epoch 21, batch   399] avg loss: 0.571219
[epoch 21, batch   499] avg loss: 0.556657
[epoch 21, batch   599] avg loss: 0.552348
[epoch 21, batch   699] avg loss: 0.557542
[epoch 21, batch   799] avg loss: 0.547914
[epoch 21, batch   899] avg loss: 0.565922
[epoch 21, batch   999] avg loss: 0.563502
[epoch 21, batch  1099] avg loss: 0.562100
[epoch 21, batch  1199] avg loss: 0.561513
[epoch 21, batch  1299] avg loss: 0.566491
[epoch 21, batch  1399] avg loss: 0.549785
[epoch 21, batch  1499] avg loss: 0.556672
[epoch 21, batch  1599] avg loss: 0.554256
[epoch 21, batch  1699] avg loss: 0.561879
[epoch 21, batch  1799] avg loss: 0.556453
[epoch 21, batch  1899] avg loss: 0.576817
[epoch 21, batch  1999] avg loss: 0.555770
[epoch 21, batch  2099] avg loss: 0.562782
[epoch 21, batch  2199] avg loss: 0.552623
[epoch 21, batch  2299] avg loss: 0.565390
[epoch 21, batch  2399] avg loss: 0.559449
[epoch 22, batch    99] avg loss: 0.557334
[epoch 22, batch   199] avg loss: 0.549701
[epoch 22, batch   299] avg loss: 0.564172
[epoch 22, batch   399] avg loss: 0.559110
[epoch 22, batch   499] avg loss: 0.556951
[epoch 22, batch   599] avg loss: 0.548633
[epoch 22, batch   699] avg loss: 0.553624
[epoch 22, batch   799] avg loss: 0.557229
[epoch 22, batch   899] avg loss: 0.563117
[epoch 22, batch   999] avg loss: 0.562482
[epoch 22, batch  1099] avg loss: 0.561736
[epoch 22, batch  1199] avg loss: 0.558320
[epoch 22, batch  1299] avg loss: 0.564853
[epoch 22, batch  1399] avg loss: 0.570955
[epoch 22, batch  1499] avg loss: 0.550007
[epoch 22, batch  1599] avg loss: 0.550411
[epoch 22, batch  1699] avg loss: 0.558971
[epoch 22, batch  1799] avg loss: 0.556734
[epoch 22, batch  1899] avg loss: 0.550127
[epoch 22, batch  1999] avg loss: 0.547147
[epoch 22, batch  2099] avg loss: 0.566925
[epoch 22, batch  2199] avg loss: 0.547922
[epoch 22, batch  2299] avg loss: 0.562530
[epoch 22, batch  2399] avg loss: 0.550683
[epoch 23, batch    99] avg loss: 0.559842
[epoch 23, batch   199] avg loss: 0.550672
[epoch 23, batch   299] avg loss: 0.561360
[epoch 23, batch   399] avg loss: 0.558679
[epoch 23, batch   499] avg loss: 0.550630
[epoch 23, batch   599] avg loss: 0.562934
[epoch 23, batch   699] avg loss: 0.559265
[epoch 23, batch   799] avg loss: 0.547581
[epoch 23, batch   899] avg loss: 0.560020
[epoch 23, batch   999] avg loss: 0.549321
[epoch 23, batch  1099] avg loss: 0.543390
[epoch 23, batch  1199] avg loss: 0.550126
[epoch 23, batch  1299] avg loss: 0.558491
[epoch 23, batch  1399] avg loss: 0.556306
[epoch 23, batch  1499] avg loss: 0.551077
[epoch 23, batch  1599] avg loss: 0.547580
[epoch 23, batch  1699] avg loss: 0.553388
[epoch 23, batch  1799] avg loss: 0.548269
[epoch 23, batch  1899] avg loss: 0.550552
[epoch 23, batch  1999] avg loss: 0.564992
[epoch 23, batch  2099] avg loss: 0.551626
[epoch 23, batch  2199] avg loss: 0.552828
[epoch 23, batch  2299] avg loss: 0.554524
[epoch 23, batch  2399] avg loss: 0.552229
[epoch 24, batch    99] avg loss: 0.548114
[epoch 24, batch   199] avg loss: 0.544001
[epoch 24, batch   299] avg loss: 0.557821
[epoch 24, batch   399] avg loss: 0.552960
[epoch 24, batch   499] avg loss: 0.544525
[epoch 24, batch   599] avg loss: 0.562164
[epoch 24, batch   699] avg loss: 0.551769
[epoch 24, batch   799] avg loss: 0.545418
[epoch 24, batch   899] avg loss: 0.556088
[epoch 24, batch   999] avg loss: 0.556788
[epoch 24, batch  1099] avg loss: 0.545654
[epoch 24, batch  1199] avg loss: 0.558608
[epoch 24, batch  1299] avg loss: 0.552001
[epoch 24, batch  1399] avg loss: 0.550029
[epoch 24, batch  1499] avg loss: 0.549928
[epoch 24, batch  1599] avg loss: 0.559827
[epoch 24, batch  1699] avg loss: 0.544298
[epoch 24, batch  1799] avg loss: 0.557118
[epoch 24, batch  1899] avg loss: 0.555922
[epoch 24, batch  1999] avg loss: 0.537915
[epoch 24, batch  2099] avg loss: 0.559892
[epoch 24, batch  2199] avg loss: 0.546680
[epoch 24, batch  2299] avg loss: 0.554046
[epoch 24, batch  2399] avg loss: 0.546933
[epoch 25, batch    99] avg loss: 0.555777
[epoch 25, batch   199] avg loss: 0.560231
[epoch 25, batch   299] avg loss: 0.551484
[epoch 25, batch   399] avg loss: 0.552830
[epoch 25, batch   499] avg loss: 0.560827
[epoch 25, batch   599] avg loss: 0.531372
[epoch 25, batch   699] avg loss: 0.540310
[epoch 25, batch   799] avg loss: 0.557491
[epoch 25, batch   899] avg loss: 0.538984
[epoch 25, batch   999] avg loss: 0.544009
[epoch 25, batch  1099] avg loss: 0.548657
[epoch 25, batch  1199] avg loss: 0.540045
[epoch 25, batch  1299] avg loss: 0.556711
[epoch 25, batch  1399] avg loss: 0.551295
[epoch 25, batch  1499] avg loss: 0.551132
[epoch 25, batch  1599] avg loss: 0.540728
[epoch 25, batch  1699] avg loss: 0.546018
[epoch 25, batch  1799] avg loss: 0.546932
[epoch 25, batch  1899] avg loss: 0.543669
[epoch 25, batch  1999] avg loss: 0.533573
[epoch 25, batch  2099] avg loss: 0.564605
[epoch 25, batch  2199] avg loss: 0.542445
[epoch 25, batch  2299] avg loss: 0.561195
[epoch 25, batch  2399] avg loss: 0.554212
[epoch 26, batch    99] avg loss: 0.552511
[epoch 26, batch   199] avg loss: 0.542867
[epoch 26, batch   299] avg loss: 0.547752
[epoch 26, batch   399] avg loss: 0.542273
[epoch 26, batch   499] avg loss: 0.547952
[epoch 26, batch   599] avg loss: 0.542095
[epoch 26, batch   699] avg loss: 0.551167
[epoch 26, batch   799] avg loss: 0.537505
[epoch 26, batch   899] avg loss: 0.547126
[epoch 26, batch   999] avg loss: 0.550734
[epoch 26, batch  1099] avg loss: 0.547175
[epoch 26, batch  1199] avg loss: 0.549606
[epoch 26, batch  1299] avg loss: 0.543901
[epoch 26, batch  1399] avg loss: 0.540514
[epoch 26, batch  1499] avg loss: 0.553249
[epoch 26, batch  1599] avg loss: 0.540085
[epoch 26, batch  1699] avg loss: 0.541288
[epoch 26, batch  1799] avg loss: 0.553667
[epoch 26, batch  1899] avg loss: 0.538056
[epoch 26, batch  1999] avg loss: 0.551874
[epoch 26, batch  2099] avg loss: 0.546923
[epoch 26, batch  2199] avg loss: 0.545721
[epoch 26, batch  2299] avg loss: 0.537228
[epoch 26, batch  2399] avg loss: 0.549025
[epoch 27, batch    99] avg loss: 0.540726
[epoch 27, batch   199] avg loss: 0.553355
[epoch 27, batch   299] avg loss: 0.546036
[epoch 27, batch   399] avg loss: 0.537574
[epoch 27, batch   499] avg loss: 0.551712
[epoch 27, batch   599] avg loss: 0.544896
[epoch 27, batch   699] avg loss: 0.542063
[epoch 27, batch   799] avg loss: 0.541486
[epoch 27, batch   899] avg loss: 0.547763
[epoch 27, batch   999] avg loss: 0.540428
[epoch 27, batch  1099] avg loss: 0.532479
[epoch 27, batch  1199] avg loss: 0.550765
[epoch 27, batch  1299] avg loss: 0.552708
[epoch 27, batch  1399] avg loss: 0.544412
[epoch 27, batch  1499] avg loss: 0.538182
[epoch 27, batch  1599] avg loss: 0.536758
[epoch 27, batch  1699] avg loss: 0.553785
[epoch 27, batch  1799] avg loss: 0.538549
[epoch 27, batch  1899] avg loss: 0.539362
[epoch 27, batch  1999] avg loss: 0.547379
[epoch 27, batch  2099] avg loss: 0.540611
[epoch 27, batch  2199] avg loss: 0.538647
[epoch 27, batch  2299] avg loss: 0.552235
[epoch 27, batch  2399] avg loss: 0.543319
[epoch 28, batch    99] avg loss: 0.540080
[epoch 28, batch   199] avg loss: 0.537985
[epoch 28, batch   299] avg loss: 0.554797
[epoch 28, batch   399] avg loss: 0.534585
[epoch 28, batch   499] avg loss: 0.552588
[epoch 28, batch   599] avg loss: 0.543646
[epoch 28, batch   699] avg loss: 0.552371
[epoch 28, batch   799] avg loss: 0.542491
[epoch 28, batch   899] avg loss: 0.545468
[epoch 28, batch   999] avg loss: 0.536190
[epoch 28, batch  1099] avg loss: 0.543052
[epoch 28, batch  1199] avg loss: 0.543429
[epoch 28, batch  1299] avg loss: 0.532646
[epoch 28, batch  1399] avg loss: 0.546668
[epoch 28, batch  1499] avg loss: 0.542334
[epoch 28, batch  1599] avg loss: 0.545082
[epoch 28, batch  1699] avg loss: 0.535109
[epoch 28, batch  1799] avg loss: 0.524084
[epoch 28, batch  1899] avg loss: 0.548707
[epoch 28, batch  1999] avg loss: 0.545180
[epoch 28, batch  2099] avg loss: 0.540047
[epoch 28, batch  2199] avg loss: 0.539705
[epoch 28, batch  2299] avg loss: 0.544690
[epoch 28, batch  2399] avg loss: 0.549296
[epoch 29, batch    99] avg loss: 0.529546
[epoch 29, batch   199] avg loss: 0.551025
[epoch 29, batch   299] avg loss: 0.534459
[epoch 29, batch   399] avg loss: 0.547546
[epoch 29, batch   499] avg loss: 0.553347
[epoch 29, batch   599] avg loss: 0.549682
[epoch 29, batch   699] avg loss: 0.541842
[epoch 29, batch   799] avg loss: 0.527790
[epoch 29, batch   899] avg loss: 0.544218
[epoch 29, batch   999] avg loss: 0.551710
[epoch 29, batch  1099] avg loss: 0.541256
[epoch 29, batch  1199] avg loss: 0.537022
[epoch 29, batch  1299] avg loss: 0.532357
[epoch 29, batch  1399] avg loss: 0.544504
[epoch 29, batch  1499] avg loss: 0.548363
[epoch 29, batch  1599] avg loss: 0.534207
[epoch 29, batch  1699] avg loss: 0.539902
[epoch 29, batch  1799] avg loss: 0.549899
[epoch 29, batch  1899] avg loss: 0.536579
[epoch 29, batch  1999] avg loss: 0.541208
[epoch 29, batch  2099] avg loss: 0.549000
[epoch 29, batch  2199] avg loss: 0.545208
[epoch 29, batch  2299] avg loss: 0.534548
[epoch 29, batch  2399] avg loss: 0.533532
[epoch 30, batch    99] avg loss: 0.547688
[epoch 30, batch   199] avg loss: 0.545740
[epoch 30, batch   299] avg loss: 0.540530
[epoch 30, batch   399] avg loss: 0.542995
[epoch 30, batch   499] avg loss: 0.534527
[epoch 30, batch   599] avg loss: 0.537884
[epoch 30, batch   699] avg loss: 0.544260
[epoch 30, batch   799] avg loss: 0.537767
[epoch 30, batch   899] avg loss: 0.536607
[epoch 30, batch   999] avg loss: 0.547172
[epoch 30, batch  1099] avg loss: 0.536139
[epoch 30, batch  1199] avg loss: 0.535782
[epoch 30, batch  1299] avg loss: 0.541617
[epoch 30, batch  1399] avg loss: 0.542630
[epoch 30, batch  1499] avg loss: 0.540983
[epoch 30, batch  1599] avg loss: 0.536825
[epoch 30, batch  1699] avg loss: 0.536643
[epoch 30, batch  1799] avg loss: 0.545597
[epoch 30, batch  1899] avg loss: 0.533002
[epoch 30, batch  1999] avg loss: 0.538407
[epoch 30, batch  2099] avg loss: 0.539339
[epoch 30, batch  2199] avg loss: 0.530429
[epoch 30, batch  2299] avg loss: 0.536927
[epoch 30, batch  2399] avg loss: 0.535467
[epoch 31, batch    99] avg loss: 0.534206
[epoch 31, batch   199] avg loss: 0.532654
[epoch 31, batch   299] avg loss: 0.530664
[epoch 31, batch   399] avg loss: 0.536223
[epoch 31, batch   499] avg loss: 0.544049
[epoch 31, batch   599] avg loss: 0.533894
[epoch 31, batch   699] avg loss: 0.541598
[epoch 31, batch   799] avg loss: 0.542507
[epoch 31, batch   899] avg loss: 0.539803
[epoch 31, batch   999] avg loss: 0.533536
[epoch 31, batch  1099] avg loss: 0.536341
[epoch 31, batch  1199] avg loss: 0.524590
[epoch 31, batch  1299] avg loss: 0.537120
[epoch 31, batch  1399] avg loss: 0.542888
[epoch 31, batch  1499] avg loss: 0.535683
[epoch 31, batch  1599] avg loss: 0.534826
[epoch 31, batch  1699] avg loss: 0.546854
[epoch 31, batch  1799] avg loss: 0.534280
[epoch 31, batch  1899] avg loss: 0.547462
[epoch 31, batch  1999] avg loss: 0.534141
[epoch 31, batch  2099] avg loss: 0.538976
[epoch 31, batch  2199] avg loss: 0.526433
[epoch 31, batch  2299] avg loss: 0.546901
[epoch 31, batch  2399] avg loss: 0.541656
[epoch 32, batch    99] avg loss: 0.539603
[epoch 32, batch   199] avg loss: 0.538180
[epoch 32, batch   299] avg loss: 0.552646
[epoch 32, batch   399] avg loss: 0.540986
[epoch 32, batch   499] avg loss: 0.529484
[epoch 32, batch   599] avg loss: 0.536264
[epoch 32, batch   699] avg loss: 0.536606
[epoch 32, batch   799] avg loss: 0.534246
[epoch 32, batch   899] avg loss: 0.525028
[epoch 32, batch   999] avg loss: 0.538757
[epoch 32, batch  1099] avg loss: 0.536258
[epoch 32, batch  1199] avg loss: 0.530359
[epoch 32, batch  1299] avg loss: 0.533751
[epoch 32, batch  1399] avg loss: 0.533273
[epoch 32, batch  1499] avg loss: 0.531899
[epoch 32, batch  1599] avg loss: 0.529177
[epoch 32, batch  1699] avg loss: 0.544437
[epoch 32, batch  1799] avg loss: 0.548058
[epoch 32, batch  1899] avg loss: 0.540450
[epoch 32, batch  1999] avg loss: 0.537204
[epoch 32, batch  2099] avg loss: 0.532647
[epoch 32, batch  2199] avg loss: 0.532466
[epoch 32, batch  2299] avg loss: 0.537340
[epoch 32, batch  2399] avg loss: 0.528758
[epoch 33, batch    99] avg loss: 0.525579
[epoch 33, batch   199] avg loss: 0.524508
[epoch 33, batch   299] avg loss: 0.564186
[epoch 33, batch   399] avg loss: 0.529597
[epoch 33, batch   499] avg loss: 0.533233
[epoch 33, batch   599] avg loss: 0.526298
[epoch 33, batch   699] avg loss: 0.547957
[epoch 33, batch   799] avg loss: 0.530795
[epoch 33, batch   899] avg loss: 0.531071
[epoch 33, batch   999] avg loss: 0.537372
[epoch 33, batch  1099] avg loss: 0.535878
[epoch 33, batch  1199] avg loss: 0.544609
[epoch 33, batch  1299] avg loss: 0.537171
[epoch 33, batch  1399] avg loss: 0.532250
[epoch 33, batch  1499] avg loss: 0.526657
[epoch 33, batch  1599] avg loss: 0.522466
[epoch 33, batch  1699] avg loss: 0.538490
[epoch 33, batch  1799] avg loss: 0.534418
[epoch 33, batch  1899] avg loss: 0.541930
[epoch 33, batch  1999] avg loss: 0.530087
[epoch 33, batch  2099] avg loss: 0.537010
[epoch 33, batch  2199] avg loss: 0.532855
[epoch 33, batch  2299] avg loss: 0.529338
[epoch 33, batch  2399] avg loss: 0.539791
[epoch 34, batch    99] avg loss: 0.526058
[epoch 34, batch   199] avg loss: 0.513411
[epoch 34, batch   299] avg loss: 0.521854
[epoch 34, batch   399] avg loss: 0.531332
[epoch 34, batch   499] avg loss: 0.541508
[epoch 34, batch   599] avg loss: 0.525904
[epoch 34, batch   699] avg loss: 0.536686
[epoch 34, batch   799] avg loss: 0.541510
[epoch 34, batch   899] avg loss: 0.523782
[epoch 34, batch   999] avg loss: 0.533552
[epoch 34, batch  1099] avg loss: 0.543869
[epoch 34, batch  1199] avg loss: 0.527129
[epoch 34, batch  1299] avg loss: 0.525559
[epoch 34, batch  1399] avg loss: 0.533740
[epoch 34, batch  1499] avg loss: 0.539568
[epoch 34, batch  1599] avg loss: 0.523629
[epoch 34, batch  1699] avg loss: 0.532153
[epoch 34, batch  1799] avg loss: 0.534526
[epoch 34, batch  1899] avg loss: 0.537394
[epoch 34, batch  1999] avg loss: 0.538753
[epoch 34, batch  2099] avg loss: 0.536335
[epoch 34, batch  2199] avg loss: 0.522834
[epoch 34, batch  2299] avg loss: 0.538590
[epoch 34, batch  2399] avg loss: 0.533832
[epoch 35, batch    99] avg loss: 0.530858
[epoch 35, batch   199] avg loss: 0.526981
[epoch 35, batch   299] avg loss: 0.523696
[epoch 35, batch   399] avg loss: 0.531147
[epoch 35, batch   499] avg loss: 0.530297
[epoch 35, batch   599] avg loss: 0.533777
[epoch 35, batch   699] avg loss: 0.533121
[epoch 35, batch   799] avg loss: 0.549614
[epoch 35, batch   899] avg loss: 0.527984
[epoch 35, batch   999] avg loss: 0.532633
[epoch 35, batch  1099] avg loss: 0.538495
[epoch 35, batch  1199] avg loss: 0.534317
[epoch 35, batch  1299] avg loss: 0.542497
[epoch 35, batch  1399] avg loss: 0.531924
[epoch 35, batch  1499] avg loss: 0.527880
[epoch 35, batch  1599] avg loss: 0.536452
[epoch 35, batch  1699] avg loss: 0.529751
[epoch 35, batch  1799] avg loss: 0.537184
[epoch 35, batch  1899] avg loss: 0.533964
[epoch 35, batch  1999] avg loss: 0.530014
[epoch 35, batch  2099] avg loss: 0.524493
[epoch 35, batch  2199] avg loss: 0.526407
[epoch 35, batch  2299] avg loss: 0.522944
[epoch 35, batch  2399] avg loss: 0.530351
[epoch 36, batch    99] avg loss: 0.533196
[epoch 36, batch   199] avg loss: 0.541127
[epoch 36, batch   299] avg loss: 0.532859
[epoch 36, batch   399] avg loss: 0.535030
[epoch 36, batch   499] avg loss: 0.519475
[epoch 36, batch   599] avg loss: 0.538259
[epoch 36, batch   699] avg loss: 0.530029
[epoch 36, batch   799] avg loss: 0.548836
[epoch 36, batch   899] avg loss: 0.536928
[epoch 36, batch   999] avg loss: 0.529389
[epoch 36, batch  1099] avg loss: 0.536112
[epoch 36, batch  1199] avg loss: 0.531832
[epoch 36, batch  1299] avg loss: 0.532875
[epoch 36, batch  1399] avg loss: 0.520672
[epoch 36, batch  1499] avg loss: 0.515227
[epoch 36, batch  1599] avg loss: 0.527156
[epoch 36, batch  1699] avg loss: 0.522063
[epoch 36, batch  1799] avg loss: 0.527196
[epoch 36, batch  1899] avg loss: 0.529548
[epoch 36, batch  1999] avg loss: 0.516374
[epoch 36, batch  2099] avg loss: 0.515326
[epoch 36, batch  2199] avg loss: 0.538644
[epoch 36, batch  2299] avg loss: 0.534291
[epoch 36, batch  2399] avg loss: 0.547766
[epoch 37, batch    99] avg loss: 0.527261
[epoch 37, batch   199] avg loss: 0.529101
[epoch 37, batch   299] avg loss: 0.515908
[epoch 37, batch   399] avg loss: 0.528712
[epoch 37, batch   499] avg loss: 0.521263
[epoch 37, batch   599] avg loss: 0.522601
[epoch 37, batch   699] avg loss: 0.524980
[epoch 37, batch   799] avg loss: 0.537514
[epoch 37, batch   899] avg loss: 0.536394
[epoch 37, batch   999] avg loss: 0.527308
[epoch 37, batch  1099] avg loss: 0.536613
[epoch 37, batch  1199] avg loss: 0.535890
[epoch 37, batch  1299] avg loss: 0.537245
[epoch 37, batch  1399] avg loss: 0.532890
[epoch 37, batch  1499] avg loss: 0.532994
[epoch 37, batch  1599] avg loss: 0.528321
[epoch 37, batch  1699] avg loss: 0.528945
[epoch 37, batch  1799] avg loss: 0.528312
[epoch 37, batch  1899] avg loss: 0.519692
[epoch 37, batch  1999] avg loss: 0.532252
[epoch 37, batch  2099] avg loss: 0.536828
[epoch 37, batch  2199] avg loss: 0.530774
[epoch 37, batch  2299] avg loss: 0.527319
[epoch 37, batch  2399] avg loss: 0.517785
[epoch 38, batch    99] avg loss: 0.531848
[epoch 38, batch   199] avg loss: 0.534909
[epoch 38, batch   299] avg loss: 0.527989
[epoch 38, batch   399] avg loss: 0.524980
[epoch 38, batch   499] avg loss: 0.527968
[epoch 38, batch   599] avg loss: 0.532641
[epoch 38, batch   699] avg loss: 0.527310
[epoch 38, batch   799] avg loss: 0.515833
[epoch 38, batch   899] avg loss: 0.526307
[epoch 38, batch   999] avg loss: 0.536580
[epoch 38, batch  1099] avg loss: 0.518890
[epoch 38, batch  1199] avg loss: 0.525243
[epoch 38, batch  1299] avg loss: 0.529034
[epoch 38, batch  1399] avg loss: 0.521783
[epoch 38, batch  1499] avg loss: 0.519815
[epoch 38, batch  1599] avg loss: 0.517280
[epoch 38, batch  1699] avg loss: 0.527392
[epoch 38, batch  1799] avg loss: 0.533747
[epoch 38, batch  1899] avg loss: 0.548285
[epoch 38, batch  1999] avg loss: 0.523682
[epoch 38, batch  2099] avg loss: 0.524457
[epoch 38, batch  2199] avg loss: 0.540006
[epoch 38, batch  2299] avg loss: 0.525469
[epoch 38, batch  2399] avg loss: 0.531114
[epoch 39, batch    99] avg loss: 0.528299
[epoch 39, batch   199] avg loss: 0.521808
[epoch 39, batch   299] avg loss: 0.523272
[epoch 39, batch   399] avg loss: 0.530563
[epoch 39, batch   499] avg loss: 0.528520
[epoch 39, batch   599] avg loss: 0.528654
[epoch 39, batch   699] avg loss: 0.528390
[epoch 39, batch   799] avg loss: 0.542244
[epoch 39, batch   899] avg loss: 0.532325
[epoch 39, batch   999] avg loss: 0.522993
[epoch 39, batch  1099] avg loss: 0.534094
[epoch 39, batch  1199] avg loss: 0.532164
[epoch 39, batch  1299] avg loss: 0.532432
[epoch 39, batch  1399] avg loss: 0.524085
[epoch 39, batch  1499] avg loss: 0.527922
[epoch 39, batch  1599] avg loss: 0.510703
[epoch 39, batch  1699] avg loss: 0.532825
[epoch 39, batch  1799] avg loss: 0.532035
[epoch 39, batch  1899] avg loss: 0.529199
[epoch 39, batch  1999] avg loss: 0.525590
[epoch 39, batch  2099] avg loss: 0.511333
[epoch 39, batch  2199] avg loss: 0.518752
[epoch 39, batch  2299] avg loss: 0.540518
[epoch 39, batch  2399] avg loss: 0.533028
Model saved to model/20200503-014836.pth.
accuracy/TriangPrismIsosc : 0.706
n_examples/TriangPrismIsosc : 500.0
accuracy/parallelepiped : 0.218
n_examples/parallelepiped : 500.0
accuracy/sphere : 1.0
n_examples/sphere : 102.0
accuracy/wire : 0.905
n_examples/wire : 200.0
accuracy/avg_geom : 0.5721966205837173
loss/validation_geom : 0.9008914549046764
accuracy/Au : 0.0
n_examples/Au : 0.0
accuracy/SiN : 0.0
n_examples/SiN : 0.0
accuracy/SiO2 : 0.8663594470046083
n_examples/SiO2 : 1302.0
accuracy/avg_mat : 0.8663594470046083
loss/validation_mat : 0.8002376849383985
MSE/ShortestDim : 1.0412235670192267
MAE/ShortestDim : 0.6706084868142499
MSE/MiddleDim : 13.991671015773134
MAE/MiddleDim : 2.5343846820649647
MSE/LongDim : 144.24483385686685
MAE/LongDim : 7.296712948613086
MSE/log Area/Vol : 21.85371316889281
MAE/log Area/Vol : 4.205107107323618
loss/validation_dim : 181.131441608552
loss/validation : 182.83257074839506
Metrics saved to model/20200503-014836_metrics.csv.
Parsed 2604 rows from data/sim_train_spectrum_SiO2.
Parsed 2604 rows from data/sim_train_labels_SiO2.
Parsed 9765 rows from data/gen_spectrum_SiO2_00-of-16.
Parsed 9765 rows from data/gen_labels_SiO2_00-of-16.
Parsed 9765 rows from data/gen_spectrum_SiO2_01-of-16.
Parsed 9765 rows from data/gen_labels_SiO2_01-of-16.
Parsed 9765 rows from data/gen_spectrum_SiO2_02-of-16.
Parsed 9765 rows from data/gen_labels_SiO2_02-of-16.
Parsed 9765 rows from data/gen_spectrum_SiO2_03-of-16.
Parsed 9765 rows from data/gen_labels_SiO2_03-of-16.
Parsed 9765 rows from data/gen_spectrum_SiO2_04-of-16.
Parsed 9765 rows from data/gen_labels_SiO2_04-of-16.
Parsed 9765 rows from data/gen_spectrum_SiO2_05-of-16.
Parsed 9765 rows from data/gen_labels_SiO2_05-of-16.
Parsed 9765 rows from data/gen_spectrum_SiO2_06-of-16.
Parsed 9765 rows from data/gen_labels_SiO2_06-of-16.
Parsed 9765 rows from data/gen_spectrum_SiO2_07-of-16.
Parsed 9765 rows from data/gen_labels_SiO2_07-of-16.
Parsed 9765 rows from data/gen_spectrum_SiO2_08-of-16.
Parsed 9765 rows from data/gen_labels_SiO2_08-of-16.
Parsed 9765 rows from data/gen_spectrum_SiO2_09-of-16.
Parsed 9765 rows from data/gen_labels_SiO2_09-of-16.
Parsed 9765 rows from data/gen_spectrum_SiO2_10-of-16.
Parsed 9765 rows from data/gen_labels_SiO2_10-of-16.
Parsed 9765 rows from data/gen_spectrum_SiO2_11-of-16.
Parsed 9765 rows from data/gen_labels_SiO2_11-of-16.
Parsed 9765 rows from data/gen_spectrum_SiO2_12-of-16.
Parsed 9765 rows from data/gen_labels_SiO2_12-of-16.
Parsed 9765 rows from data/gen_spectrum_SiO2_13-of-16.
Parsed 9765 rows from data/gen_labels_SiO2_13-of-16.
Parsed 9765 rows from data/gen_spectrum_SiO2_14-of-16.
Parsed 9765 rows from data/gen_labels_SiO2_14-of-16.
Parsed 9765 rows from data/gen_spectrum_SiO2_15-of-16.
Parsed 9765 rows from data/gen_labels_SiO2_15-of-16.
Parsed 1302 rows from data/sim_validation_spectrum_SiO2.
Parsed 1302 rows from data/sim_validation_labels_SiO2.
Logging training progress to tensorboard dir runs/alexnet-SiO2-lr_0.000050-trainsize_158844-05_03_2020_01:49-joint.
[epoch 0, batch    99] avg loss: 1.383027
[epoch 0, batch   199] avg loss: 1.223140
[epoch 0, batch   299] avg loss: 0.897027
[epoch 0, batch   399] avg loss: 0.861271
[epoch 0, batch   499] avg loss: 0.849283
[epoch 0, batch   599] avg loss: 0.841556
[epoch 0, batch   699] avg loss: 0.844034
[epoch 0, batch   799] avg loss: 0.836149
[epoch 0, batch   899] avg loss: 0.816884
[epoch 0, batch   999] avg loss: 0.805220
[epoch 0, batch  1099] avg loss: 0.805917
[epoch 0, batch  1199] avg loss: 0.797019
[epoch 0, batch  1299] avg loss: 0.803600
[epoch 0, batch  1399] avg loss: 0.800104
[epoch 0, batch  1499] avg loss: 0.795788
[epoch 0, batch  1599] avg loss: 0.792170
[epoch 0, batch  1699] avg loss: 0.781425
[epoch 0, batch  1799] avg loss: 0.790639
[epoch 0, batch  1899] avg loss: 0.782038
[epoch 0, batch  1999] avg loss: 0.784383
[epoch 0, batch  2099] avg loss: 0.792076
[epoch 0, batch  2199] avg loss: 0.768513
[epoch 0, batch  2299] avg loss: 0.771029
[epoch 0, batch  2399] avg loss: 0.775462
[epoch 1, batch    99] avg loss: 0.759271
[epoch 1, batch   199] avg loss: 0.765554
[epoch 1, batch   299] avg loss: 0.741577
[epoch 1, batch   399] avg loss: 0.762641
[epoch 1, batch   499] avg loss: 0.743763
[epoch 1, batch   599] avg loss: 0.733597
[epoch 1, batch   699] avg loss: 0.721269
[epoch 1, batch   799] avg loss: 0.719495
[epoch 1, batch   899] avg loss: 0.712198
[epoch 1, batch   999] avg loss: 0.701201
[epoch 1, batch  1099] avg loss: 0.693500
[epoch 1, batch  1199] avg loss: 0.691328
[epoch 1, batch  1299] avg loss: 0.690694
[epoch 1, batch  1399] avg loss: 0.674449
[epoch 1, batch  1499] avg loss: 0.662836
[epoch 1, batch  1599] avg loss: 0.683818
[epoch 1, batch  1699] avg loss: 0.666562
[epoch 1, batch  1799] avg loss: 0.651991
[epoch 1, batch  1899] avg loss: 0.662836
[epoch 1, batch  1999] avg loss: 0.658006
[epoch 1, batch  2099] avg loss: 0.654985
[epoch 1, batch  2199] avg loss: 0.641628
[epoch 1, batch  2299] avg loss: 0.644732
[epoch 1, batch  2399] avg loss: 0.646835
[epoch 2, batch    99] avg loss: 0.637776
[epoch 2, batch   199] avg loss: 0.625113
[epoch 2, batch   299] avg loss: 0.626767
[epoch 2, batch   399] avg loss: 0.624685
[epoch 2, batch   499] avg loss: 0.630241
[epoch 2, batch   599] avg loss: 0.627271
[epoch 2, batch   699] avg loss: 0.631753
[epoch 2, batch   799] avg loss: 0.613238
[epoch 2, batch   899] avg loss: 0.615863
[epoch 2, batch   999] avg loss: 0.625069
[epoch 2, batch  1099] avg loss: 0.624705
[epoch 2, batch  1199] avg loss: 0.628894
[epoch 2, batch  1299] avg loss: 0.615071
[epoch 2, batch  1399] avg loss: 0.621854
[epoch 2, batch  1499] avg loss: 0.616964
[epoch 2, batch  1599] avg loss: 0.605408
[epoch 2, batch  1699] avg loss: 0.611932
[epoch 2, batch  1799] avg loss: 0.608802
[epoch 2, batch  1899] avg loss: 0.611124
[epoch 2, batch  1999] avg loss: 0.593205
[epoch 2, batch  2099] avg loss: 0.591652
[epoch 2, batch  2199] avg loss: 0.590322
[epoch 2, batch  2299] avg loss: 0.600149
[epoch 2, batch  2399] avg loss: 0.584189
[epoch 3, batch    99] avg loss: 0.580949
[epoch 3, batch   199] avg loss: 0.585547
[epoch 3, batch   299] avg loss: 0.592452
[epoch 3, batch   399] avg loss: 0.584946
[epoch 3, batch   499] avg loss: 0.586600
[epoch 3, batch   599] avg loss: 0.596029
[epoch 3, batch   699] avg loss: 0.587915
[epoch 3, batch   799] avg loss: 0.581453
[epoch 3, batch   899] avg loss: 0.577500
[epoch 3, batch   999] avg loss: 0.590025
[epoch 3, batch  1099] avg loss: 0.585711
[epoch 3, batch  1199] avg loss: 0.581024
[epoch 3, batch  1299] avg loss: 0.573891
[epoch 3, batch  1399] avg loss: 0.590157
[epoch 3, batch  1499] avg loss: 0.578950
[epoch 3, batch  1599] avg loss: 0.583080
[epoch 3, batch  1699] avg loss: 0.589181
[epoch 3, batch  1799] avg loss: 0.573891
[epoch 3, batch  1899] avg loss: 0.573984
[epoch 3, batch  1999] avg loss: 0.577163
[epoch 3, batch  2099] avg loss: 0.555478
[epoch 3, batch  2199] avg loss: 0.576496
[epoch 3, batch  2299] avg loss: 0.571598
[epoch 3, batch  2399] avg loss: 0.573793
[epoch 4, batch    99] avg loss: 0.581895
[epoch 4, batch   199] avg loss: 0.581279
[epoch 4, batch   299] avg loss: 0.566388
[epoch 4, batch   399] avg loss: 0.564864
[epoch 4, batch   499] avg loss: 0.571180
[epoch 4, batch   599] avg loss: 0.555878
[epoch 4, batch   699] avg loss: 0.572561
[epoch 4, batch   799] avg loss: 0.560520
[epoch 4, batch   899] avg loss: 0.560475
[epoch 4, batch   999] avg loss: 0.565974
[epoch 4, batch  1099] avg loss: 0.553997
[epoch 4, batch  1199] avg loss: 0.565270
[epoch 4, batch  1299] avg loss: 0.548952
[epoch 4, batch  1399] avg loss: 0.559279
[epoch 4, batch  1499] avg loss: 0.561268
[epoch 4, batch  1599] avg loss: 0.559912
[epoch 4, batch  1699] avg loss: 0.545068
[epoch 4, batch  1799] avg loss: 0.564685
[epoch 4, batch  1899] avg loss: 0.557393
[epoch 4, batch  1999] avg loss: 0.559326
[epoch 4, batch  2099] avg loss: 0.558834
[epoch 4, batch  2199] avg loss: 0.551926
[epoch 4, batch  2299] avg loss: 0.547594
[epoch 4, batch  2399] avg loss: 0.571587
[epoch 5, batch    99] avg loss: 0.550293
[epoch 5, batch   199] avg loss: 0.545691
[epoch 5, batch   299] avg loss: 0.553441
[epoch 5, batch   399] avg loss: 0.551303
[epoch 5, batch   499] avg loss: 0.565156
[epoch 5, batch   599] avg loss: 0.557179
[epoch 5, batch   699] avg loss: 0.554073
[epoch 5, batch   799] avg loss: 0.547156
[epoch 5, batch   899] avg loss: 0.544043
[epoch 5, batch   999] avg loss: 0.545649
[epoch 5, batch  1099] avg loss: 0.552710
[epoch 5, batch  1199] avg loss: 0.544862
[epoch 5, batch  1299] avg loss: 0.548864
[epoch 5, batch  1399] avg loss: 0.547624
[epoch 5, batch  1499] avg loss: 0.542700
[epoch 5, batch  1599] avg loss: 0.540940
[epoch 5, batch  1699] avg loss: 0.537435
[epoch 5, batch  1799] avg loss: 0.544983
[epoch 5, batch  1899] avg loss: 0.544018
[epoch 5, batch  1999] avg loss: 0.542099
[epoch 5, batch  2099] avg loss: 0.538670
[epoch 5, batch  2199] avg loss: 0.539219
[epoch 5, batch  2299] avg loss: 0.542805
[epoch 5, batch  2399] avg loss: 0.551214
[epoch 6, batch    99] avg loss: 0.541056
[epoch 6, batch   199] avg loss: 0.548788
[epoch 6, batch   299] avg loss: 0.539457
[epoch 6, batch   399] avg loss: 0.547521
[epoch 6, batch   499] avg loss: 0.544099
[epoch 6, batch   599] avg loss: 0.543191
[epoch 6, batch   699] avg loss: 0.536757
[epoch 6, batch   799] avg loss: 0.533957
[epoch 6, batch   899] avg loss: 0.533410
[epoch 6, batch   999] avg loss: 0.534544
[epoch 6, batch  1099] avg loss: 0.536896
[epoch 6, batch  1199] avg loss: 0.533154
[epoch 6, batch  1299] avg loss: 0.544684
[epoch 6, batch  1399] avg loss: 0.539560
[epoch 6, batch  1499] avg loss: 0.541047
[epoch 6, batch  1599] avg loss: 0.533408
[epoch 6, batch  1699] avg loss: 0.544968
[epoch 6, batch  1799] avg loss: 0.530276
[epoch 6, batch  1899] avg loss: 0.517158
[epoch 6, batch  1999] avg loss: 0.538332
[epoch 6, batch  2099] avg loss: 0.538287
[epoch 6, batch  2199] avg loss: 0.529524
[epoch 6, batch  2299] avg loss: 0.534499
[epoch 6, batch  2399] avg loss: 0.537968
[epoch 7, batch    99] avg loss: 0.529958
[epoch 7, batch   199] avg loss: 0.527390
[epoch 7, batch   299] avg loss: 0.525154
[epoch 7, batch   399] avg loss: 0.537949
[epoch 7, batch   499] avg loss: 0.529530
[epoch 7, batch   599] avg loss: 0.543277
[epoch 7, batch   699] avg loss: 0.536695
[epoch 7, batch   799] avg loss: 0.527106
[epoch 7, batch   899] avg loss: 0.528025
[epoch 7, batch   999] avg loss: 0.521313
[epoch 7, batch  1099] avg loss: 0.522499
[epoch 7, batch  1199] avg loss: 0.533561
[epoch 7, batch  1299] avg loss: 0.533670
[epoch 7, batch  1399] avg loss: 0.540484
[epoch 7, batch  1499] avg loss: 0.519880
[epoch 7, batch  1599] avg loss: 0.533417
[epoch 7, batch  1699] avg loss: 0.518291
[epoch 7, batch  1799] avg loss: 0.532922
[epoch 7, batch  1899] avg loss: 0.524159
[epoch 7, batch  1999] avg loss: 0.531396
[epoch 7, batch  2099] avg loss: 0.540291
[epoch 7, batch  2199] avg loss: 0.540111
[epoch 7, batch  2299] avg loss: 0.519732
[epoch 7, batch  2399] avg loss: 0.527345
[epoch 8, batch    99] avg loss: 0.529856
[epoch 8, batch   199] avg loss: 0.519174
[epoch 8, batch   299] avg loss: 0.525542
[epoch 8, batch   399] avg loss: 0.522770
[epoch 8, batch   499] avg loss: 0.523129
[epoch 8, batch   599] avg loss: 0.525969
[epoch 8, batch   699] avg loss: 0.517570
[epoch 8, batch   799] avg loss: 0.522937
[epoch 8, batch   899] avg loss: 0.520672
[epoch 8, batch   999] avg loss: 0.529170
[epoch 8, batch  1099] avg loss: 0.517972
[epoch 8, batch  1199] avg loss: 0.529798
[epoch 8, batch  1299] avg loss: 0.531439
[epoch 8, batch  1399] avg loss: 0.520495
[epoch 8, batch  1499] avg loss: 0.537204
[epoch 8, batch  1599] avg loss: 0.528340
[epoch 8, batch  1699] avg loss: 0.536064
[epoch 8, batch  1799] avg loss: 0.538649
[epoch 8, batch  1899] avg loss: 0.521458
[epoch 8, batch  1999] avg loss: 0.528150
[epoch 8, batch  2099] avg loss: 0.516417
[epoch 8, batch  2199] avg loss: 0.531291
[epoch 8, batch  2299] avg loss: 0.539717
[epoch 8, batch  2399] avg loss: 0.526150
[epoch 9, batch    99] avg loss: 0.530363
[epoch 9, batch   199] avg loss: 0.529193
[epoch 9, batch   299] avg loss: 0.527505
[epoch 9, batch   399] avg loss: 0.525030
[epoch 9, batch   499] avg loss: 0.532657
[epoch 9, batch   599] avg loss: 0.528202
[epoch 9, batch   699] avg loss: 0.508828
[epoch 9, batch   799] avg loss: 0.514131
[epoch 9, batch   899] avg loss: 0.523075
[epoch 9, batch   999] avg loss: 0.529712
[epoch 9, batch  1099] avg loss: 0.520764
[epoch 9, batch  1199] avg loss: 0.520807
[epoch 9, batch  1299] avg loss: 0.509086
[epoch 9, batch  1399] avg loss: 0.530395
[epoch 9, batch  1499] avg loss: 0.517741
[epoch 9, batch  1599] avg loss: 0.524922
[epoch 9, batch  1699] avg loss: 0.509167
[epoch 9, batch  1799] avg loss: 0.525264
[epoch 9, batch  1899] avg loss: 0.509242
[epoch 9, batch  1999] avg loss: 0.535253
[epoch 9, batch  2099] avg loss: 0.522017
[epoch 9, batch  2199] avg loss: 0.527913
[epoch 9, batch  2299] avg loss: 0.521102
[epoch 9, batch  2399] avg loss: 0.506433
[epoch 10, batch    99] avg loss: 0.523671
[epoch 10, batch   199] avg loss: 0.513504
[epoch 10, batch   299] avg loss: 0.513896
[epoch 10, batch   399] avg loss: 0.542299
[epoch 10, batch   499] avg loss: 0.521368
[epoch 10, batch   599] avg loss: 0.526674
[epoch 10, batch   699] avg loss: 0.520988
[epoch 10, batch   799] avg loss: 0.511764
[epoch 10, batch   899] avg loss: 0.513451
[epoch 10, batch   999] avg loss: 0.511252
[epoch 10, batch  1099] avg loss: 0.496520
[epoch 10, batch  1199] avg loss: 0.521589
[epoch 10, batch  1299] avg loss: 0.520881
[epoch 10, batch  1399] avg loss: 0.525554
[epoch 10, batch  1499] avg loss: 0.521264
[epoch 10, batch  1599] avg loss: 0.516725
[epoch 10, batch  1699] avg loss: 0.516735
[epoch 10, batch  1799] avg loss: 0.502604
[epoch 10, batch  1899] avg loss: 0.502230
[epoch 10, batch  1999] avg loss: 0.515688
[epoch 10, batch  2099] avg loss: 0.510078
[epoch 10, batch  2199] avg loss: 0.500678
[epoch 10, batch  2299] avg loss: 0.519070
[epoch 10, batch  2399] avg loss: 0.506592
[epoch 11, batch    99] avg loss: 0.516963
[epoch 11, batch   199] avg loss: 0.510995
[epoch 11, batch   299] avg loss: 0.519547
[epoch 11, batch   399] avg loss: 0.515377
[epoch 11, batch   499] avg loss: 0.519591
[epoch 11, batch   599] avg loss: 0.506771
[epoch 11, batch   699] avg loss: 0.512833
[epoch 11, batch   799] avg loss: 0.519776
[epoch 11, batch   899] avg loss: 0.517516
[epoch 11, batch   999] avg loss: 0.502841
[epoch 11, batch  1099] avg loss: 0.521825
[epoch 11, batch  1199] avg loss: 0.498054
[epoch 11, batch  1299] avg loss: 0.521693
[epoch 11, batch  1399] avg loss: 0.513354
[epoch 11, batch  1499] avg loss: 0.510398
[epoch 11, batch  1599] avg loss: 0.511633
[epoch 11, batch  1699] avg loss: 0.517697
[epoch 11, batch  1799] avg loss: 0.516652
[epoch 11, batch  1899] avg loss: 0.497413
[epoch 11, batch  1999] avg loss: 0.518537
[epoch 11, batch  2099] avg loss: 0.511204
[epoch 11, batch  2199] avg loss: 0.523973
[epoch 11, batch  2299] avg loss: 0.516931
[epoch 11, batch  2399] avg loss: 0.514464
[epoch 12, batch    99] avg loss: 0.512867
[epoch 12, batch   199] avg loss: 0.508142
[epoch 12, batch   299] avg loss: 0.503613
[epoch 12, batch   399] avg loss: 0.511818
[epoch 12, batch   499] avg loss: 0.499129
[epoch 12, batch   599] avg loss: 0.523493
[epoch 12, batch   699] avg loss: 0.509946
[epoch 12, batch   799] avg loss: 0.513689
[epoch 12, batch   899] avg loss: 0.504751
[epoch 12, batch   999] avg loss: 0.511278
[epoch 12, batch  1099] avg loss: 0.513317
[epoch 12, batch  1199] avg loss: 0.510776
[epoch 12, batch  1299] avg loss: 0.500064
[epoch 12, batch  1399] avg loss: 0.509038
[epoch 12, batch  1499] avg loss: 0.526445
[epoch 12, batch  1599] avg loss: 0.521188
[epoch 12, batch  1699] avg loss: 0.515378
[epoch 12, batch  1799] avg loss: 0.525504
[epoch 12, batch  1899] avg loss: 0.509877
[epoch 12, batch  1999] avg loss: 0.510585
[epoch 12, batch  2099] avg loss: 0.499404
[epoch 12, batch  2199] avg loss: 0.501645
[epoch 12, batch  2299] avg loss: 0.494225
[epoch 12, batch  2399] avg loss: 0.517513
[epoch 13, batch    99] avg loss: 0.508464
[epoch 13, batch   199] avg loss: 0.508594
[epoch 13, batch   299] avg loss: 0.505079
[epoch 13, batch   399] avg loss: 0.512178
[epoch 13, batch   499] avg loss: 0.521354
[epoch 13, batch   599] avg loss: 0.513036
[epoch 13, batch   699] avg loss: 0.511486
[epoch 13, batch   799] avg loss: 0.511295
[epoch 13, batch   899] avg loss: 0.516708
[epoch 13, batch   999] avg loss: 0.498870
[epoch 13, batch  1099] avg loss: 0.518061
[epoch 13, batch  1199] avg loss: 0.497816
[epoch 13, batch  1299] avg loss: 0.497102
[epoch 13, batch  1399] avg loss: 0.501098
[epoch 13, batch  1499] avg loss: 0.505179
[epoch 13, batch  1599] avg loss: 0.508557
[epoch 13, batch  1699] avg loss: 0.507434
[epoch 13, batch  1799] avg loss: 0.515120
[epoch 13, batch  1899] avg loss: 0.503200
[epoch 13, batch  1999] avg loss: 0.503421
[epoch 13, batch  2099] avg loss: 0.506527
[epoch 13, batch  2199] avg loss: 0.512292
[epoch 13, batch  2299] avg loss: 0.514508
[epoch 13, batch  2399] avg loss: 0.502023
[epoch 14, batch    99] avg loss: 0.500595
[epoch 14, batch   199] avg loss: 0.508868
[epoch 14, batch   299] avg loss: 0.507337
[epoch 14, batch   399] avg loss: 0.509635
[epoch 14, batch   499] avg loss: 0.506351
[epoch 14, batch   599] avg loss: 0.515561
[epoch 14, batch   699] avg loss: 0.491769
[epoch 14, batch   799] avg loss: 0.501164
[epoch 14, batch   899] avg loss: 0.512454
[epoch 14, batch   999] avg loss: 0.506315
[epoch 14, batch  1099] avg loss: 0.494237
[epoch 14, batch  1199] avg loss: 0.513449
[epoch 14, batch  1299] avg loss: 0.499960
[epoch 14, batch  1399] avg loss: 0.514434
[epoch 14, batch  1499] avg loss: 0.508937
[epoch 14, batch  1599] avg loss: 0.517209
[epoch 14, batch  1699] avg loss: 0.504861
[epoch 14, batch  1799] avg loss: 0.497832
[epoch 14, batch  1899] avg loss: 0.497801
[epoch 14, batch  1999] avg loss: 0.507103
[epoch 14, batch  2099] avg loss: 0.497318
[epoch 14, batch  2199] avg loss: 0.503764
[epoch 14, batch  2299] avg loss: 0.520053
[epoch 14, batch  2399] avg loss: 0.502946
[epoch 15, batch    99] avg loss: 0.494862
[epoch 15, batch   199] avg loss: 0.501148
[epoch 15, batch   299] avg loss: 0.498535
[epoch 15, batch   399] avg loss: 0.494019
[epoch 15, batch   499] avg loss: 0.500713
[epoch 15, batch   599] avg loss: 0.517183
[epoch 15, batch   699] avg loss: 0.504506
[epoch 15, batch   799] avg loss: 0.505567
[epoch 15, batch   899] avg loss: 0.502739
[epoch 15, batch   999] avg loss: 0.504497
[epoch 15, batch  1099] avg loss: 0.503705
[epoch 15, batch  1199] avg loss: 0.506003
[epoch 15, batch  1299] avg loss: 0.492555
[epoch 15, batch  1399] avg loss: 0.512274
[epoch 15, batch  1499] avg loss: 0.512481
[epoch 15, batch  1599] avg loss: 0.500586
[epoch 15, batch  1699] avg loss: 0.507009
[epoch 15, batch  1799] avg loss: 0.497910
[epoch 15, batch  1899] avg loss: 0.497965
[epoch 15, batch  1999] avg loss: 0.512889
[epoch 15, batch  2099] avg loss: 0.500620
[epoch 15, batch  2199] avg loss: 0.501672
[epoch 15, batch  2299] avg loss: 0.491841
[epoch 15, batch  2399] avg loss: 0.510566
[epoch 16, batch    99] avg loss: 0.497520
[epoch 16, batch   199] avg loss: 0.506021
[epoch 16, batch   299] avg loss: 0.498888
[epoch 16, batch   399] avg loss: 0.497515
[epoch 16, batch   499] avg loss: 0.496155
[epoch 16, batch   599] avg loss: 0.491079
[epoch 16, batch   699] avg loss: 0.493322
[epoch 16, batch   799] avg loss: 0.508198
[epoch 16, batch   899] avg loss: 0.507871
[epoch 16, batch   999] avg loss: 0.505372
[epoch 16, batch  1099] avg loss: 0.493499
[epoch 16, batch  1199] avg loss: 0.497985
[epoch 16, batch  1299] avg loss: 0.506732
[epoch 16, batch  1399] avg loss: 0.492294
[epoch 16, batch  1499] avg loss: 0.504216
[epoch 16, batch  1599] avg loss: 0.499103
[epoch 16, batch  1699] avg loss: 0.506385
[epoch 16, batch  1799] avg loss: 0.503100
[epoch 16, batch  1899] avg loss: 0.500506
[epoch 16, batch  1999] avg loss: 0.502566
[epoch 16, batch  2099] avg loss: 0.498344
[epoch 16, batch  2199] avg loss: 0.509824
[epoch 16, batch  2299] avg loss: 0.497589
[epoch 16, batch  2399] avg loss: 0.498220
[epoch 17, batch    99] avg loss: 0.506001
[epoch 17, batch   199] avg loss: 0.501466
[epoch 17, batch   299] avg loss: 0.505401
[epoch 17, batch   399] avg loss: 0.487433
[epoch 17, batch   499] avg loss: 0.493003
[epoch 17, batch   599] avg loss: 0.504894
[epoch 17, batch   699] avg loss: 0.494397
[epoch 17, batch   799] avg loss: 0.508895
[epoch 17, batch   899] avg loss: 0.491487
[epoch 17, batch   999] avg loss: 0.495985
[epoch 17, batch  1099] avg loss: 0.490281
[epoch 17, batch  1199] avg loss: 0.491969
[epoch 17, batch  1299] avg loss: 0.501551
[epoch 17, batch  1399] avg loss: 0.490050
[epoch 17, batch  1499] avg loss: 0.491978
[epoch 17, batch  1599] avg loss: 0.499327
[epoch 17, batch  1699] avg loss: 0.493530
[epoch 17, batch  1799] avg loss: 0.496353
[epoch 17, batch  1899] avg loss: 0.492193
[epoch 17, batch  1999] avg loss: 0.509387
[epoch 17, batch  2099] avg loss: 0.504908
[epoch 17, batch  2199] avg loss: 0.491104
[epoch 17, batch  2299] avg loss: 0.503959
[epoch 17, batch  2399] avg loss: 0.499070
[epoch 18, batch    99] avg loss: 0.502106
[epoch 18, batch   199] avg loss: 0.508100
[epoch 18, batch   299] avg loss: 0.494560
[epoch 18, batch   399] avg loss: 0.491269
[epoch 18, batch   499] avg loss: 0.484405
[epoch 18, batch   599] avg loss: 0.495765
[epoch 18, batch   699] avg loss: 0.499554
[epoch 18, batch   799] avg loss: 0.501839
[epoch 18, batch   899] avg loss: 0.503181
[epoch 18, batch   999] avg loss: 0.488382
[epoch 18, batch  1099] avg loss: 0.493432
[epoch 18, batch  1199] avg loss: 0.499585
[epoch 18, batch  1299] avg loss: 0.507535
[epoch 18, batch  1399] avg loss: 0.495213
[epoch 18, batch  1499] avg loss: 0.498615
[epoch 18, batch  1599] avg loss: 0.496143
[epoch 18, batch  1699] avg loss: 0.495428
[epoch 18, batch  1799] avg loss: 0.494253
[epoch 18, batch  1899] avg loss: 0.497687
[epoch 18, batch  1999] avg loss: 0.497428
[epoch 18, batch  2099] avg loss: 0.493718
[epoch 18, batch  2199] avg loss: 0.512152
[epoch 18, batch  2299] avg loss: 0.488294
[epoch 18, batch  2399] avg loss: 0.497222
[epoch 19, batch    99] avg loss: 0.492462
[epoch 19, batch   199] avg loss: 0.480378
[epoch 19, batch   299] avg loss: 0.499024
[epoch 19, batch   399] avg loss: 0.499156
[epoch 19, batch   499] avg loss: 0.495347
[epoch 19, batch   599] avg loss: 0.480567
[epoch 19, batch   699] avg loss: 0.509400
[epoch 19, batch   799] avg loss: 0.487375
[epoch 19, batch   899] avg loss: 0.491230
[epoch 19, batch   999] avg loss: 0.494719
[epoch 19, batch  1099] avg loss: 0.503567
[epoch 19, batch  1199] avg loss: 0.495894
[epoch 19, batch  1299] avg loss: 0.494071
[epoch 19, batch  1399] avg loss: 0.485306
[epoch 19, batch  1499] avg loss: 0.505211
[epoch 19, batch  1599] avg loss: 0.492514
[epoch 19, batch  1699] avg loss: 0.498496
[epoch 19, batch  1799] avg loss: 0.492872
[epoch 19, batch  1899] avg loss: 0.506422
[epoch 19, batch  1999] avg loss: 0.493228
[epoch 19, batch  2099] avg loss: 0.489602
[epoch 19, batch  2199] avg loss: 0.492263
[epoch 19, batch  2299] avg loss: 0.502773
[epoch 19, batch  2399] avg loss: 0.488138
[epoch 20, batch    99] avg loss: 0.501231
[epoch 20, batch   199] avg loss: 0.487295
[epoch 20, batch   299] avg loss: 0.497377
[epoch 20, batch   399] avg loss: 0.499793
[epoch 20, batch   499] avg loss: 0.494610
[epoch 20, batch   599] avg loss: 0.495856
[epoch 20, batch   699] avg loss: 0.494099
[epoch 20, batch   799] avg loss: 0.495471
[epoch 20, batch   899] avg loss: 0.495815
[epoch 20, batch   999] avg loss: 0.486879
[epoch 20, batch  1099] avg loss: 0.483874
[epoch 20, batch  1199] avg loss: 0.486387
[epoch 20, batch  1299] avg loss: 0.484785
[epoch 20, batch  1399] avg loss: 0.501087
[epoch 20, batch  1499] avg loss: 0.494281
[epoch 20, batch  1599] avg loss: 0.488609
[epoch 20, batch  1699] avg loss: 0.502076
[epoch 20, batch  1799] avg loss: 0.490063
[epoch 20, batch  1899] avg loss: 0.493962
[epoch 20, batch  1999] avg loss: 0.482270
[epoch 20, batch  2099] avg loss: 0.479255
[epoch 20, batch  2199] avg loss: 0.490035
[epoch 20, batch  2299] avg loss: 0.489043
[epoch 20, batch  2399] avg loss: 0.486600
[epoch 21, batch    99] avg loss: 0.489726
[epoch 21, batch   199] avg loss: 0.495675
[epoch 21, batch   299] avg loss: 0.492070
[epoch 21, batch   399] avg loss: 0.484017
[epoch 21, batch   499] avg loss: 0.496076
[epoch 21, batch   599] avg loss: 0.477675
[epoch 21, batch   699] avg loss: 0.479989
[epoch 21, batch   799] avg loss: 0.486359
[epoch 21, batch   899] avg loss: 0.494536
[epoch 21, batch   999] avg loss: 0.491270
[epoch 21, batch  1099] avg loss: 0.483574
[epoch 21, batch  1199] avg loss: 0.496175
[epoch 21, batch  1299] avg loss: 0.494344
[epoch 21, batch  1399] avg loss: 0.484222
[epoch 21, batch  1499] avg loss: 0.491207
[epoch 21, batch  1599] avg loss: 0.488370
[epoch 21, batch  1699] avg loss: 0.483845
[epoch 21, batch  1799] avg loss: 0.487362
[epoch 21, batch  1899] avg loss: 0.495316
[epoch 21, batch  1999] avg loss: 0.495291
[epoch 21, batch  2099] avg loss: 0.487458
[epoch 21, batch  2199] avg loss: 0.480974
[epoch 21, batch  2299] avg loss: 0.504962
[epoch 21, batch  2399] avg loss: 0.479059
[epoch 22, batch    99] avg loss: 0.493161
[epoch 22, batch   199] avg loss: 0.490841
[epoch 22, batch   299] avg loss: 0.490611
[epoch 22, batch   399] avg loss: 0.492156
[epoch 22, batch   499] avg loss: 0.508745
[epoch 22, batch   599] avg loss: 0.488722
[epoch 22, batch   699] avg loss: 0.491196
[epoch 22, batch   799] avg loss: 0.496804
[epoch 22, batch   899] avg loss: 0.481383
[epoch 22, batch   999] avg loss: 0.486162
[epoch 22, batch  1099] avg loss: 0.494330
[epoch 22, batch  1199] avg loss: 0.474508
[epoch 22, batch  1299] avg loss: 0.477912
[epoch 22, batch  1399] avg loss: 0.498084
[epoch 22, batch  1499] avg loss: 0.496757
[epoch 22, batch  1599] avg loss: 0.475340
[epoch 22, batch  1699] avg loss: 0.476669
[epoch 22, batch  1799] avg loss: 0.479127
[epoch 22, batch  1899] avg loss: 0.480857
[epoch 22, batch  1999] avg loss: 0.480305
[epoch 22, batch  2099] avg loss: 0.476935
[epoch 22, batch  2199] avg loss: 0.483823
[epoch 22, batch  2299] avg loss: 0.484036
[epoch 22, batch  2399] avg loss: 0.483443
[epoch 23, batch    99] avg loss: 0.480006
[epoch 23, batch   199] avg loss: 0.481657
[epoch 23, batch   299] avg loss: 0.466688
[epoch 23, batch   399] avg loss: 0.489021
[epoch 23, batch   499] avg loss: 0.477793
[epoch 23, batch   599] avg loss: 0.484726
[epoch 23, batch   699] avg loss: 0.475959
[epoch 23, batch   799] avg loss: 0.496115
[epoch 23, batch   899] avg loss: 0.481982
[epoch 23, batch   999] avg loss: 0.470221
[epoch 23, batch  1099] avg loss: 0.488568
[epoch 23, batch  1199] avg loss: 0.473423
[epoch 23, batch  1299] avg loss: 0.490858
[epoch 23, batch  1399] avg loss: 0.480927
[epoch 23, batch  1499] avg loss: 0.482983
[epoch 23, batch  1599] avg loss: 0.492685
[epoch 23, batch  1699] avg loss: 0.478241
[epoch 23, batch  1799] avg loss: 0.483046
[epoch 23, batch  1899] avg loss: 0.474471
[epoch 23, batch  1999] avg loss: 0.478401
[epoch 23, batch  2099] avg loss: 0.484041
[epoch 23, batch  2199] avg loss: 0.483232
[epoch 23, batch  2299] avg loss: 0.479809
[epoch 23, batch  2399] avg loss: 0.490585
[epoch 24, batch    99] avg loss: 0.481684
[epoch 24, batch   199] avg loss: 0.482755
[epoch 24, batch   299] avg loss: 0.485894
[epoch 24, batch   399] avg loss: 0.482325
[epoch 24, batch   499] avg loss: 0.476733
[epoch 24, batch   599] avg loss: 0.479790
[epoch 24, batch   699] avg loss: 0.472262
[epoch 24, batch   799] avg loss: 0.476375
[epoch 24, batch   899] avg loss: 0.464439
[epoch 24, batch   999] avg loss: 0.481567
[epoch 24, batch  1099] avg loss: 0.482834
[epoch 24, batch  1199] avg loss: 0.487179
[epoch 24, batch  1299] avg loss: 0.472679
[epoch 24, batch  1399] avg loss: 0.489404
[epoch 24, batch  1499] avg loss: 0.469611
[epoch 24, batch  1599] avg loss: 0.468464
[epoch 24, batch  1699] avg loss: 0.484171
[epoch 24, batch  1799] avg loss: 0.478714
[epoch 24, batch  1899] avg loss: 0.469964
[epoch 24, batch  1999] avg loss: 0.488337
[epoch 24, batch  2099] avg loss: 0.469727
[epoch 24, batch  2199] avg loss: 0.467328
[epoch 24, batch  2299] avg loss: 0.473851
[epoch 24, batch  2399] avg loss: 0.483453
[epoch 25, batch    99] avg loss: 0.479178
[epoch 25, batch   199] avg loss: 0.469088
[epoch 25, batch   299] avg loss: 0.484230
[epoch 25, batch   399] avg loss: 0.472647
[epoch 25, batch   499] avg loss: 0.486579
[epoch 25, batch   599] avg loss: 0.477352
[epoch 25, batch   699] avg loss: 0.475933
[epoch 25, batch   799] avg loss: 0.471330
[epoch 25, batch   899] avg loss: 0.462899
[epoch 25, batch   999] avg loss: 0.470177
[epoch 25, batch  1099] avg loss: 0.481273
[epoch 25, batch  1199] avg loss: 0.474829
[epoch 25, batch  1299] avg loss: 0.470955
[epoch 25, batch  1399] avg loss: 0.470770
[epoch 25, batch  1499] avg loss: 0.468066
[epoch 25, batch  1599] avg loss: 0.480353
[epoch 25, batch  1699] avg loss: 0.476517
[epoch 25, batch  1799] avg loss: 0.479804
[epoch 25, batch  1899] avg loss: 0.462399
[epoch 25, batch  1999] avg loss: 0.477817
[epoch 25, batch  2099] avg loss: 0.468180
[epoch 25, batch  2199] avg loss: 0.480286
[epoch 25, batch  2299] avg loss: 0.466778
[epoch 25, batch  2399] avg loss: 0.476487
[epoch 26, batch    99] avg loss: 0.467002
[epoch 26, batch   199] avg loss: 0.477506
[epoch 26, batch   299] avg loss: 0.471304
[epoch 26, batch   399] avg loss: 0.487589
[epoch 26, batch   499] avg loss: 0.463272
[epoch 26, batch   599] avg loss: 0.472022
[epoch 26, batch   699] avg loss: 0.471225
[epoch 26, batch   799] avg loss: 0.470508
[epoch 26, batch   899] avg loss: 0.468242
[epoch 26, batch   999] avg loss: 0.470448
[epoch 26, batch  1099] avg loss: 0.458953
[epoch 26, batch  1199] avg loss: 0.474193
[epoch 26, batch  1299] avg loss: 0.476085
[epoch 26, batch  1399] avg loss: 0.477536
[epoch 26, batch  1499] avg loss: 0.469923
[epoch 26, batch  1599] avg loss: 0.466792
[epoch 26, batch  1699] avg loss: 0.460314
[epoch 26, batch  1799] avg loss: 0.466046
[epoch 26, batch  1899] avg loss: 0.471121
[epoch 26, batch  1999] avg loss: 0.469807
[epoch 26, batch  2099] avg loss: 0.466346
[epoch 26, batch  2199] avg loss: 0.476933
[epoch 26, batch  2299] avg loss: 0.463500
[epoch 26, batch  2399] avg loss: 0.467963
[epoch 27, batch    99] avg loss: 0.465287
[epoch 27, batch   199] avg loss: 0.469488
[epoch 27, batch   299] avg loss: 0.471462
[epoch 27, batch   399] avg loss: 0.466065
[epoch 27, batch   499] avg loss: 0.468732
[epoch 27, batch   599] avg loss: 0.465284
[epoch 27, batch   699] avg loss: 0.488515
[epoch 27, batch   799] avg loss: 0.465826
[epoch 27, batch   899] avg loss: 0.466299
[epoch 27, batch   999] avg loss: 0.477840
[epoch 27, batch  1099] avg loss: 0.451431
[epoch 27, batch  1199] avg loss: 0.468669
[epoch 27, batch  1299] avg loss: 0.466001
[epoch 27, batch  1399] avg loss: 0.468097
[epoch 27, batch  1499] avg loss: 0.471687
[epoch 27, batch  1599] avg loss: 0.462675
[epoch 27, batch  1699] avg loss: 0.464069
[epoch 27, batch  1799] avg loss: 0.474897
[epoch 27, batch  1899] avg loss: 0.470145
[epoch 27, batch  1999] avg loss: 0.448764
[epoch 27, batch  2099] avg loss: 0.476366
[epoch 27, batch  2199] avg loss: 0.482409
[epoch 27, batch  2299] avg loss: 0.465210
[epoch 27, batch  2399] avg loss: 0.456735
[epoch 28, batch    99] avg loss: 0.471864
[epoch 28, batch   199] avg loss: 0.465970
[epoch 28, batch   299] avg loss: 0.465340
[epoch 28, batch   399] avg loss: 0.462789
[epoch 28, batch   499] avg loss: 0.462497
[epoch 28, batch   599] avg loss: 0.466066
[epoch 28, batch   699] avg loss: 0.466852
[epoch 28, batch   799] avg loss: 0.458697
[epoch 28, batch   899] avg loss: 0.458114
[epoch 28, batch   999] avg loss: 0.450951
[epoch 28, batch  1099] avg loss: 0.454909
[epoch 28, batch  1199] avg loss: 0.455004
[epoch 28, batch  1299] avg loss: 0.458836
[epoch 28, batch  1399] avg loss: 0.468060
[epoch 28, batch  1499] avg loss: 0.480067
[epoch 28, batch  1599] avg loss: 0.460625
[epoch 28, batch  1699] avg loss: 0.469946
[epoch 28, batch  1799] avg loss: 0.465750
[epoch 28, batch  1899] avg loss: 0.465892
[epoch 28, batch  1999] avg loss: 0.466339
[epoch 28, batch  2099] avg loss: 0.467502
[epoch 28, batch  2199] avg loss: 0.474929
[epoch 28, batch  2299] avg loss: 0.461251
[epoch 28, batch  2399] avg loss: 0.469357
[epoch 29, batch    99] avg loss: 0.464509
[epoch 29, batch   199] avg loss: 0.454102
[epoch 29, batch   299] avg loss: 0.467188
[epoch 29, batch   399] avg loss: 0.466299
[epoch 29, batch   499] avg loss: 0.447420
[epoch 29, batch   599] avg loss: 0.460120
[epoch 29, batch   699] avg loss: 0.470892
[epoch 29, batch   799] avg loss: 0.456723
[epoch 29, batch   899] avg loss: 0.448494
[epoch 29, batch   999] avg loss: 0.465894
[epoch 29, batch  1099] avg loss: 0.457282
[epoch 29, batch  1199] avg loss: 0.446768
[epoch 29, batch  1299] avg loss: 0.453938
[epoch 29, batch  1399] avg loss: 0.476036
[epoch 29, batch  1499] avg loss: 0.461971
[epoch 29, batch  1599] avg loss: 0.469865
[epoch 29, batch  1699] avg loss: 0.463038
[epoch 29, batch  1799] avg loss: 0.465308
[epoch 29, batch  1899] avg loss: 0.462833
[epoch 29, batch  1999] avg loss: 0.454927
[epoch 29, batch  2099] avg loss: 0.462296
[epoch 29, batch  2199] avg loss: 0.463084
[epoch 29, batch  2299] avg loss: 0.467170
[epoch 29, batch  2399] avg loss: 0.479446
[epoch 30, batch    99] avg loss: 0.451399
[epoch 30, batch   199] avg loss: 0.449816
[epoch 30, batch   299] avg loss: 0.470353
[epoch 30, batch   399] avg loss: 0.451804
[epoch 30, batch   499] avg loss: 0.455218
[epoch 30, batch   599] avg loss: 0.460162
[epoch 30, batch   699] avg loss: 0.455553
[epoch 30, batch   799] avg loss: 0.463841
[epoch 30, batch   899] avg loss: 0.464871
[epoch 30, batch   999] avg loss: 0.457580
[epoch 30, batch  1099] avg loss: 0.462266
[epoch 30, batch  1199] avg loss: 0.455323
[epoch 30, batch  1299] avg loss: 0.461314
[epoch 30, batch  1399] avg loss: 0.461977
[epoch 30, batch  1499] avg loss: 0.455668
[epoch 30, batch  1599] avg loss: 0.455827
[epoch 30, batch  1699] avg loss: 0.454476
[epoch 30, batch  1799] avg loss: 0.453542
[epoch 30, batch  1899] avg loss: 0.454125
[epoch 30, batch  1999] avg loss: 0.457622
[epoch 30, batch  2099] avg loss: 0.454749
[epoch 30, batch  2199] avg loss: 0.465626
[epoch 30, batch  2299] avg loss: 0.461887
[epoch 30, batch  2399] avg loss: 0.458558
[epoch 31, batch    99] avg loss: 0.467987
[epoch 31, batch   199] avg loss: 0.452400
[epoch 31, batch   299] avg loss: 0.455763
[epoch 31, batch   399] avg loss: 0.461894
[epoch 31, batch   499] avg loss: 0.451985
[epoch 31, batch   599] avg loss: 0.466410
[epoch 31, batch   699] avg loss: 0.451982
[epoch 31, batch   799] avg loss: 0.457866
[epoch 31, batch   899] avg loss: 0.457054
[epoch 31, batch   999] avg loss: 0.450233
[epoch 31, batch  1099] avg loss: 0.456769
[epoch 31, batch  1199] avg loss: 0.463635
[epoch 31, batch  1299] avg loss: 0.456633
[epoch 31, batch  1399] avg loss: 0.459063
[epoch 31, batch  1499] avg loss: 0.442874
[epoch 31, batch  1599] avg loss: 0.457990
[epoch 31, batch  1699] avg loss: 0.454874
[epoch 31, batch  1799] avg loss: 0.450161
[epoch 31, batch  1899] avg loss: 0.461709
[epoch 31, batch  1999] avg loss: 0.452622
[epoch 31, batch  2099] avg loss: 0.459944
[epoch 31, batch  2199] avg loss: 0.464541
[epoch 31, batch  2299] avg loss: 0.446936
[epoch 31, batch  2399] avg loss: 0.447083
[epoch 32, batch    99] avg loss: 0.449174
[epoch 32, batch   199] avg loss: 0.444918
[epoch 32, batch   299] avg loss: 0.454306
[epoch 32, batch   399] avg loss: 0.444921
[epoch 32, batch   499] avg loss: 0.463618
[epoch 32, batch   599] avg loss: 0.462977
[epoch 32, batch   699] avg loss: 0.455287
[epoch 32, batch   799] avg loss: 0.453078
[epoch 32, batch   899] avg loss: 0.452898
[epoch 32, batch   999] avg loss: 0.435514
[epoch 32, batch  1099] avg loss: 0.450400
[epoch 32, batch  1199] avg loss: 0.440311
[epoch 32, batch  1299] avg loss: 0.459004
[epoch 32, batch  1399] avg loss: 0.455796
[epoch 32, batch  1499] avg loss: 0.442261
[epoch 32, batch  1599] avg loss: 0.446742
[epoch 32, batch  1699] avg loss: 0.466065
[epoch 32, batch  1799] avg loss: 0.433407
[epoch 32, batch  1899] avg loss: 0.469414
[epoch 32, batch  1999] avg loss: 0.467867
[epoch 32, batch  2099] avg loss: 0.456442
[epoch 32, batch  2199] avg loss: 0.454197
[epoch 32, batch  2299] avg loss: 0.445158
[epoch 32, batch  2399] avg loss: 0.451080
[epoch 33, batch    99] avg loss: 0.461606
[epoch 33, batch   199] avg loss: 0.460162
[epoch 33, batch   299] avg loss: 0.455722
[epoch 33, batch   399] avg loss: 0.448568
[epoch 33, batch   499] avg loss: 0.451007
[epoch 33, batch   599] avg loss: 0.442842
[epoch 33, batch   699] avg loss: 0.439685
[epoch 33, batch   799] avg loss: 0.445870
[epoch 33, batch   899] avg loss: 0.456542
[epoch 33, batch   999] avg loss: 0.451998
[epoch 33, batch  1099] avg loss: 0.461649
[epoch 33, batch  1199] avg loss: 0.449613
[epoch 33, batch  1299] avg loss: 0.448885
[epoch 33, batch  1399] avg loss: 0.455536
[epoch 33, batch  1499] avg loss: 0.444581
[epoch 33, batch  1599] avg loss: 0.449914
[epoch 33, batch  1699] avg loss: 0.445576
[epoch 33, batch  1799] avg loss: 0.459709
[epoch 33, batch  1899] avg loss: 0.456294
[epoch 33, batch  1999] avg loss: 0.440460
[epoch 33, batch  2099] avg loss: 0.443583
[epoch 33, batch  2199] avg loss: 0.448635
[epoch 33, batch  2299] avg loss: 0.443234
[epoch 33, batch  2399] avg loss: 0.445353
[epoch 34, batch    99] avg loss: 0.438625
[epoch 34, batch   199] avg loss: 0.453397
[epoch 34, batch   299] avg loss: 0.452848
[epoch 34, batch   399] avg loss: 0.456272
[epoch 34, batch   499] avg loss: 0.451953
[epoch 34, batch   599] avg loss: 0.443227
[epoch 34, batch   699] avg loss: 0.455399
[epoch 34, batch   799] avg loss: 0.440352
[epoch 34, batch   899] avg loss: 0.441853
[epoch 34, batch   999] avg loss: 0.460241
[epoch 34, batch  1099] avg loss: 0.432184
[epoch 34, batch  1199] avg loss: 0.451380
[epoch 34, batch  1299] avg loss: 0.456543
[epoch 34, batch  1399] avg loss: 0.436076
[epoch 34, batch  1499] avg loss: 0.451924
[epoch 34, batch  1599] avg loss: 0.467720
[epoch 34, batch  1699] avg loss: 0.438560
[epoch 34, batch  1799] avg loss: 0.441040
[epoch 34, batch  1899] avg loss: 0.454156
[epoch 34, batch  1999] avg loss: 0.444993
[epoch 34, batch  2099] avg loss: 0.438502
[epoch 34, batch  2199] avg loss: 0.461197
[epoch 34, batch  2299] avg loss: 0.445926
[epoch 34, batch  2399] avg loss: 0.440923
[epoch 35, batch    99] avg loss: 0.449630
[epoch 35, batch   199] avg loss: 0.448572
[epoch 35, batch   299] avg loss: 0.448272
[epoch 35, batch   399] avg loss: 0.442807
[epoch 35, batch   499] avg loss: 0.456904
[epoch 35, batch   599] avg loss: 0.443412
[epoch 35, batch   699] avg loss: 0.437799
[epoch 35, batch   799] avg loss: 0.442242
[epoch 35, batch   899] avg loss: 0.445023
[epoch 35, batch   999] avg loss: 0.455931
[epoch 35, batch  1099] avg loss: 0.444956
[epoch 35, batch  1199] avg loss: 0.442176
[epoch 35, batch  1299] avg loss: 0.450954
[epoch 35, batch  1399] avg loss: 0.439503
[epoch 35, batch  1499] avg loss: 0.432109
[epoch 35, batch  1599] avg loss: 0.447186
[epoch 35, batch  1699] avg loss: 0.445311
[epoch 35, batch  1799] avg loss: 0.437068
[epoch 35, batch  1899] avg loss: 0.437564
[epoch 35, batch  1999] avg loss: 0.448209
[epoch 35, batch  2099] avg loss: 0.453033
[epoch 35, batch  2199] avg loss: 0.435312
[epoch 35, batch  2299] avg loss: 0.450384
[epoch 35, batch  2399] avg loss: 0.447536
[epoch 36, batch    99] avg loss: 0.445700
[epoch 36, batch   199] avg loss: 0.454604
[epoch 36, batch   299] avg loss: 0.445982
[epoch 36, batch   399] avg loss: 0.446304
[epoch 36, batch   499] avg loss: 0.445950
[epoch 36, batch   599] avg loss: 0.450579
[epoch 36, batch   699] avg loss: 0.454799
[epoch 36, batch   799] avg loss: 0.439426
[epoch 36, batch   899] avg loss: 0.437820
[epoch 36, batch   999] avg loss: 0.451164
[epoch 36, batch  1099] avg loss: 0.443910
[epoch 36, batch  1199] avg loss: 0.439328
[epoch 36, batch  1299] avg loss: 0.437081
[epoch 36, batch  1399] avg loss: 0.454576
[epoch 36, batch  1499] avg loss: 0.442239
[epoch 36, batch  1599] avg loss: 0.445265
[epoch 36, batch  1699] avg loss: 0.431302
[epoch 36, batch  1799] avg loss: 0.445327
[epoch 36, batch  1899] avg loss: 0.443940
[epoch 36, batch  1999] avg loss: 0.443100
[epoch 36, batch  2099] avg loss: 0.441000
[epoch 36, batch  2199] avg loss: 0.450503
[epoch 36, batch  2299] avg loss: 0.450044
[epoch 36, batch  2399] avg loss: 0.442574
[epoch 37, batch    99] avg loss: 0.443274
[epoch 37, batch   199] avg loss: 0.432777
[epoch 37, batch   299] avg loss: 0.447468
[epoch 37, batch   399] avg loss: 0.442465
[epoch 37, batch   499] avg loss: 0.463316
[epoch 37, batch   599] avg loss: 0.450098
[epoch 37, batch   699] avg loss: 0.432195
[epoch 37, batch   799] avg loss: 0.442526
[epoch 37, batch   899] avg loss: 0.440497
[epoch 37, batch   999] avg loss: 0.445024
[epoch 37, batch  1099] avg loss: 0.432248
[epoch 37, batch  1199] avg loss: 0.447704
[epoch 37, batch  1299] avg loss: 0.435412
[epoch 37, batch  1399] avg loss: 0.440164
[epoch 37, batch  1499] avg loss: 0.440687
[epoch 37, batch  1599] avg loss: 0.444181
[epoch 37, batch  1699] avg loss: 0.436031
[epoch 37, batch  1799] avg loss: 0.438129
[epoch 37, batch  1899] avg loss: 0.441809
[epoch 37, batch  1999] avg loss: 0.445530
[epoch 37, batch  2099] avg loss: 0.431521
[epoch 37, batch  2199] avg loss: 0.442730
[epoch 37, batch  2299] avg loss: 0.447461
[epoch 37, batch  2399] avg loss: 0.433009
[epoch 38, batch    99] avg loss: 0.434804
[epoch 38, batch   199] avg loss: 0.420003
[epoch 38, batch   299] avg loss: 0.435543
[epoch 38, batch   399] avg loss: 0.443281
[epoch 38, batch   499] avg loss: 0.433939
[epoch 38, batch   599] avg loss: 0.442140
[epoch 38, batch   699] avg loss: 0.434460
[epoch 38, batch   799] avg loss: 0.445166
[epoch 38, batch   899] avg loss: 0.445720
[epoch 38, batch   999] avg loss: 0.427116
[epoch 38, batch  1099] avg loss: 0.442096
[epoch 38, batch  1199] avg loss: 0.440193
[epoch 38, batch  1299] avg loss: 0.440892
[epoch 38, batch  1399] avg loss: 0.450301
[epoch 38, batch  1499] avg loss: 0.450117
[epoch 38, batch  1599] avg loss: 0.437603
[epoch 38, batch  1699] avg loss: 0.441793
[epoch 38, batch  1799] avg loss: 0.447054
[epoch 38, batch  1899] avg loss: 0.441620
[epoch 38, batch  1999] avg loss: 0.448650
[epoch 38, batch  2099] avg loss: 0.436339
[epoch 38, batch  2199] avg loss: 0.441521
[epoch 38, batch  2299] avg loss: 0.433766
[epoch 38, batch  2399] avg loss: 0.441586
[epoch 39, batch    99] avg loss: 0.439717
[epoch 39, batch   199] avg loss: 0.444212
[epoch 39, batch   299] avg loss: 0.435009
[epoch 39, batch   399] avg loss: 0.436659
[epoch 39, batch   499] avg loss: 0.432031
[epoch 39, batch   599] avg loss: 0.441057
[epoch 39, batch   699] avg loss: 0.440722
[epoch 39, batch   799] avg loss: 0.430292
[epoch 39, batch   899] avg loss: 0.441006
[epoch 39, batch   999] avg loss: 0.441592
[epoch 39, batch  1099] avg loss: 0.450279
[epoch 39, batch  1199] avg loss: 0.424397
[epoch 39, batch  1299] avg loss: 0.433423
[epoch 39, batch  1399] avg loss: 0.433788
[epoch 39, batch  1499] avg loss: 0.431558
[epoch 39, batch  1599] avg loss: 0.433414
[epoch 39, batch  1699] avg loss: 0.429272
[epoch 39, batch  1799] avg loss: 0.442982
[epoch 39, batch  1899] avg loss: 0.425240
[epoch 39, batch  1999] avg loss: 0.450432
[epoch 39, batch  2099] avg loss: 0.441602
[epoch 39, batch  2199] avg loss: 0.440976
[epoch 39, batch  2299] avg loss: 0.434982
[epoch 39, batch  2399] avg loss: 0.423131
Model saved to model/20200503-022146.pth.
accuracy/TriangPrismIsosc : 0.75
n_examples/TriangPrismIsosc : 500.0
accuracy/parallelepiped : 0.282
n_examples/parallelepiped : 500.0
accuracy/sphere : 1.0
n_examples/sphere : 102.0
accuracy/wire : 0.985
n_examples/wire : 200.0
accuracy/avg_geom : 0.6259600614439325
loss/validation_geom : 0.8582086379993163
accuracy/Au : 0.0
n_examples/Au : 0.0
accuracy/SiN : 0.0
n_examples/SiN : 0.0
accuracy/SiO2 : 0.0007680491551459293
n_examples/SiO2 : 1302.0
accuracy/avg_mat : 0.0007680491551459293
loss/validation_mat : 1.9806198260751189
MSE/ShortestDim : 3.3631389899187925
MAE/ShortestDim : 1.3755030068018104
MSE/MiddleDim : 12.215937402024979
MAE/MiddleDim : 2.8498312368554086
MSE/LongDim : 131.5496089249712
MAE/LongDim : 6.956952729349679
MSE/log Area/Vol : 11.744599935645882
MAE/log Area/Vol : 3.203718006702429
loss/validation_dim : 158.87328525256086
loss/validation : 161.7121137166353
Metrics saved to model/20200503-022146_metrics.csv.
